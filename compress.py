import sys
import os
import re
import io
import math
import shutil
import subprocess
import multiprocessing
import zlib  # Added for Flate compression
from pathlib import Path

# æ·»åŠ å½“å‰ç›®å½•åˆ°è·¯å¾„ï¼Œç¡®ä¿èƒ½å¯¼å…¥æœ¬åœ°æ¨¡å—
SCRIPT_DIR = Path(__file__).parent.resolve()
sys.path.insert(0, str(SCRIPT_DIR))

import pikepdf
import fitz  # PyMuPDF
import numpy as np
import imagecodecs  # GIL-free JPEG2000 encoding (replaces PIL JPEG2000)
import cv2  # OpenCV for image enhancement
from PIL import Image, ImageFilter, ImageEnhance
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed

# === ML Pipeline å¯ç”¨æ€§æ£€æŸ¥ (ä¸é¢„åŠ è½½æ¨¡å‹) ===
_ml_pipeline_available = None  # None=æœªæ£€æŸ¥, True=å¯ç”¨, False=ä¸å¯ç”¨

def check_ml_pipeline_available():
    """æ£€æŸ¥MLç®¡çº¿æ˜¯å¦å¯ç”¨ï¼ˆä»…æ£€æŸ¥æ¨¡å‹æ–‡ä»¶å’Œä¾èµ–ï¼Œä¸åŠ è½½æ¨¡å‹ï¼‰"""
    global _ml_pipeline_available
    if _ml_pipeline_available is None:
        try:
            models_dir = SCRIPT_DIR / "models"
            required = [
                "slbr-watermark-detector-fp16.onnx",
                "real-esrgan-x4plus-128-fp16.onnx",
                "NAF-DPM_denoiser-fp16.onnx",
                "NAF-DPM_init_predictor-fp16.onnx",
            ]
            missing = [m for m in required if not (models_dir / m).exists()]
            if missing:
                safe_print(f"      [WARN] MLæ¨¡å‹ç¼ºå¤±: {', '.join(missing)}")
                _ml_pipeline_available = False
            else:
                import ml_pipeline  # noqa: F401 â€” verify importable
                _ml_pipeline_available = True
        except Exception as e:
            safe_print(f"      [WARN] MLç®¡çº¿ä¸å¯ç”¨: {e}")
            _ml_pipeline_available = False
    return _ml_pipeline_available

# å…¨å±€é…ç½®
Image.MAX_IMAGE_PIXELS = None  # ç¦ç”¨DecompressionBombæ£€æŸ¥

# === å…¼å®¹æ€§ä¿®å¤ ===
try:
    # type: ignore
    PDF_REDACT_IMAGE_REMOVE = fitz.PDF_REDACT_IMAGE_REMOVE
except AttributeError:
    PDF_REDACT_IMAGE_REMOVE = 2

# === æ ¸å¿ƒé…ç½® (Integrated) ===
# åŸºç¡€é…ç½®
SIZE_THRESHOLD_MB = 100
MIN_IMAGE_SIZE = 2048
CHUNK_SIZE = 100
CURVE_SIMPLIFY_THRESHOLD = 0.5
CPU_CORES = max(1, multiprocessing.cpu_count() - 1)

# JPEG2000 ç¼–ç å¹¶è¡Œç­–ç•¥ (imagecodecs + OpenJPEG)
# numthreads: OpenJPEG å†…éƒ¨ç¼–ç çº¿ç¨‹æ•° (é‡Šæ”¾ GIL)
# JP2K_WORKERS: ThreadPoolExecutor å¹¶å‘ç¼–ç ä»»åŠ¡æ•°
# æœ€ä¼˜ç»„åˆ: workers * numthreads â‰ˆ CPU_CORES, é¿å…è¿‡åº¦è®¢é˜…
JP2K_THREADS = 4  # æ¯å¼ å›¾ç¼–ç ä½¿ç”¨çš„ OpenJPEG å†…éƒ¨çº¿ç¨‹æ•°
JP2K_WORKERS = max(2, CPU_CORES // JP2K_THREADS)  # å¹¶å‘ç¼–ç ä»»åŠ¡æ•°

# åˆ‡ç‰‡/ç°åº¦æ£€æµ‹é…ç½®
GRID_SIZE = 20
GLOBAL_FORCE_MONO_THRESHOLD = 0.98
BLOCK_GRAY_PIXEL_THRESHOLD = 0.15
GRAY_LOWER_BOUND = 50
GRAY_UPPER_BOUND = 220
BINARIZE_THRESHOLD = 180
TILE_GRID_ROWS = 5
TILE_GRID_COLS = 5
TILE_CHECK_GRID = 4
COLOR_STD_THRESHOLD = 5.0  # åˆ¤å®šæ˜¯å¦ä¸ºå½©è‰²çš„é˜ˆå€¼ (è¶Šå°è¶Šå®¹æ˜“è¢«åˆ¤å®šä¸ºå½©è‰²)

# å…¨å±€å˜é‡
lossy_report_list = []

# === å®‰å…¨æ‰“å°å‡½æ•° (é¿å… Windows GBK ç¼–ç é—®é¢˜) ===
def safe_print(msg):
    """å®‰å…¨æ‰“å°ï¼Œå¤„ç† Windows æ§åˆ¶å°çš„ç¼–ç é—®é¢˜"""
    try:
        print(msg)
    except UnicodeEncodeError:
        # ç§»é™¤æ— æ³•ç¼–ç çš„å­—ç¬¦ (ä¸»è¦æ˜¯ emoji)
        safe_msg = msg.encode('gbk', errors='replace').decode('gbk')
        print(safe_msg)


# ============================
# è¾…åŠ©å‡½æ•°
# ============================
def safe_remove(path):
    if path and os.path.exists(path):
        try:
            os.remove(path)
        except:
            pass


def get_file_mb(path):
    if not os.path.exists(path):
        return 0
    return os.path.getsize(path) / (1024 * 1024)


def is_valid_pdf(path):
    if not os.path.exists(path):
        return False
    if os.path.getsize(path) < 1024:
        return False
    return True


# ============================
# å•è‰²è£…é¥°æ¨¡å¼æ£€æµ‹ (é€é¡µåˆ¤å®š)
# ============================
def analyze_page_color_profile(page, gray_tolerance=10, gray_threshold=85, color_threshold=15):
    """
    åˆ†æå•é¡µçš„é¢œè‰²ç‰¹å¾
    è¿”å›: (is_mono_decorative, stats_dict)
    - is_mono_decorative: è¯¥é¡µæ˜¯å¦ä¸ºå•è‰²è£…é¥°é¡µé¢ï¼ˆé€‚åˆç°åº¦åŒ–ï¼‰
    """
    # ä½åˆ†è¾¨ç‡é‡‡æ ·
    pix = page.get_pixmap(matrix=fitz.Matrix(0.25, 0.25))
    img = Image.frombytes('RGB', [pix.width, pix.height], pix.samples)
    
    # NumPyå‘é‡åŒ–åˆ†æï¼ˆæ›¿ä»£é€åƒç´ Pythonå¾ªç¯ï¼Œæé€Ÿ50-100xï¼‰
    arr = np.array(img)  # (H, W, 3) uint8
    total = arr.shape[0] * arr.shape[1]
    
    if total == 0:
        return False, {}
    
    r, g, b = arr[:,:,0].ravel(), arr[:,:,1].ravel(), arr[:,:,2].ravel()
    
    # ç°åº¦åˆ¤å®š: max(|R-G|, |G-B|, |R-B|) <= gray_tolerance
    diff_rg = np.abs(r.astype(np.int16) - g.astype(np.int16))
    diff_gb = np.abs(g.astype(np.int16) - b.astype(np.int16))
    diff_rb = np.abs(r.astype(np.int16) - b.astype(np.int16))
    max_diff = np.maximum(np.maximum(diff_rg, diff_gb), diff_rb)
    
    gray_count = int(np.count_nonzero(max_diff <= gray_tolerance))
    
    gray_ratio = gray_count / total * 100
    color_ratio = 100 - gray_ratio
    
    # å½©è‰²åƒç´ è‰²è°ƒç»Ÿè®¡
    color_mask = max_diff > gray_tolerance
    color_count = total - gray_count
    
    if color_count > 0:
        cr, cg, cb = r[color_mask], g[color_mask], b[color_mask]
        hue_counts = {
            'blue': int(np.count_nonzero((cb > cr) & (cb > cg))),
            'pink/red': int(np.count_nonzero((cr > cg) & (cr > cb))),
            'green': int(np.count_nonzero((cg > cr) & (cg > cb))),
        }
        hue_counts['other'] = color_count - sum(hue_counts.values())
        dominant_hue = max(hue_counts.items(), key=lambda x: x[1])
        hue_concentration = dominant_hue[1] / color_count * 100
    else:
        dominant_hue = ('none', 0)
        hue_concentration = 100
    
    stats = {
        'gray_ratio': gray_ratio,
        'color_ratio': color_ratio,
        'dominant_hue': dominant_hue[0],
        'hue_concentration': hue_concentration
    }
    
    # å•è‰²è£…é¥°åˆ¤å®šï¼š
    # 1. ç°åº¦å æ¯” > 85%
    # 2. å½©è‰²å æ¯” < 15%
    # 3. å½©è‰²éƒ¨åˆ†çš„ä¸»è‰²è°ƒé›†ä¸­åº¦ > 60% (è¯´æ˜é¢œè‰²å•ä¸€)
    is_mono_decorative = (
        gray_ratio > gray_threshold and 
        color_ratio < color_threshold and 
        hue_concentration > 60
    )
    
    return is_mono_decorative, stats


def detect_mono_decorative_pages(pdf_path, sample_ratio=0.1, min_samples=15, max_samples=50):
    """
    æ£€æµ‹PDFä¸­çš„å•è‰²è£…é¥°é¡µé¢
    
    è¿”å›: (has_mono_pages, pages_to_convert, stats)
    - has_mono_pages: æ˜¯å¦å­˜åœ¨å¤§é‡å•è‰²è£…é¥°é¡µé¢ (>50%)
    - pages_to_convert: éœ€è¦ç°åº¦åŒ–çš„é¡µé¢ç´¢å¼•åˆ—è¡¨
    - stats: ç»Ÿè®¡ä¿¡æ¯
    """
    from collections import Counter
    import random
    
    try:
        doc = fitz.open(pdf_path)
        total_pages = len(doc)
        
        if total_pages == 0:
            doc.close()
            return False, [], {}
        
        # ç¡®å®šé‡‡æ ·æ•°é‡
        sample_count = max(min_samples, min(max_samples, int(total_pages * sample_ratio)))
        sample_count = min(sample_count, total_pages)
        
        # éšæœºé‡‡æ ·é¡µé¢ç´¢å¼•
        if sample_count >= total_pages:
            sample_indices = list(range(total_pages))
        else:
            sample_indices = sorted(random.sample(range(total_pages), sample_count))
        
        mono_pages_in_sample = []
        hue_counter = Counter()
        
        for page_num in sample_indices:
            page = doc[page_num]
            is_mono, stats = analyze_page_color_profile(page)
            
            if is_mono:
                mono_pages_in_sample.append(page_num)
            
            if stats.get('dominant_hue'):
                hue_counter[stats['dominant_hue']] += 1
        
        # è®¡ç®—å•è‰²è£…é¥°é¡µé¢æ¯”ä¾‹
        mono_ratio = len(mono_pages_in_sample) / len(sample_indices) * 100
        
        # å¦‚æœé‡‡æ ·ä¸­è¶…è¿‡50%æ˜¯å•è‰²è£…é¥°ï¼Œåˆ™å…¨é‡æ‰«æ
        pages_to_convert = []
        
        if mono_ratio > 50:
            # å…¨é‡æ‰«ææ‰€æœ‰é¡µé¢
            safe_print(f"      [SCAN] æ£€æµ‹åˆ°å•è‰²è£…é¥°æ¨¡å¼ (é‡‡æ ·ä¸­ {mono_ratio:.1f}% ç¬¦åˆ)ï¼Œæ­£åœ¨å…¨é‡æ‰«æ...")
            
            for page_num in tqdm(range(total_pages), desc="      Analyzing", leave=False):
                page = doc[page_num]
                is_mono, _ = analyze_page_color_profile(page)
                if is_mono:
                    pages_to_convert.append(page_num)
        
        doc.close()
        
        dominant_hue = hue_counter.most_common(1)[0][0] if hue_counter else 'none'
        
        summary_stats = {
            'total_pages': total_pages,
            'sample_count': len(sample_indices),
            'mono_in_sample': len(mono_pages_in_sample),
            'mono_ratio_sample': mono_ratio,
            'pages_to_convert': len(pages_to_convert),
            'dominant_hue': dominant_hue
        }
        
        has_significant_mono = len(pages_to_convert) > total_pages * 0.3  # è¶…è¿‡30%é¡µé¢éœ€è¦è½¬æ¢
        
        return has_significant_mono, pages_to_convert, summary_stats
        
    except Exception as e:
        return False, [], {'error': str(e)}


def enhance_document_image(img_array, mode='standard'):
    """
    æ–‡æ¡£å›¾åƒå¢å¼º - ä¿å®ˆä¸”å®‰å…¨çš„å¢å¼ºæ–¹å¼
    
    é’ˆå¯¹é—®é¢˜:
    1. æ•´ä½“æ¨¡ç³Š - é€‚åº¦é”åŒ–
    2. èƒŒæ™¯ç°è„ - æ¸©å’Œçš„å¯¹æ¯”åº¦è°ƒæ•´
    3. æ–‡å­—è¾¹ç¼˜æ¨¡ç³Š - è‡ªé€‚åº”é”åŒ–
    
    å¤„ç†æµç¨‹:
    1. è½»åº¦é™å™ª - ä¿ç•™ç»†èŠ‚
    2. å¯¹æ¯”åº¦æ‹‰ä¼¸ - æ‰©å±•åŠ¨æ€èŒƒå›´
    3. è‡ªé€‚åº”é”åŒ– - å¢å¼ºæ–‡å­—è¾¹ç¼˜
    4. äº®åº¦å¾®è°ƒ - è®©èƒŒæ™¯ç¨å¾®å˜ç™½
    
    Args:
        img_array: numpy array (ç°åº¦å›¾åƒ, uint8)
        mode: 'standard' (æ ‡å‡†), 'strong' (å¼ºåŠ›), 'mild' (æ¸©å’Œ)
    
    Returns:
        å¢å¼ºåçš„ numpy array
    """
    result = img_array.copy()
    
    # æ ¹æ®æ¨¡å¼è°ƒæ•´å‚æ•°
    if mode == 'strong':
        denoise_h = 5
        sharpen_amount = 0.4
        contrast_alpha = 1.15
        brightness_target = 245
    elif mode == 'mild':
        denoise_h = 3
        sharpen_amount = 0.2
        contrast_alpha = 1.05
        brightness_target = 240
    else:  # standard
        denoise_h = 4
        sharpen_amount = 0.3
        contrast_alpha = 1.1
        brightness_target = 242
    
    # ========================================
    # 1. è½»åº¦é™å™ª (ä¿ç•™è¾¹ç¼˜)
    # ========================================
    # ä½¿ç”¨è¾ƒå°çš„ h å€¼ï¼Œåªå»é™¤è½»å¾®å™ªç‚¹
    result = cv2.fastNlMeansDenoising(result, None, h=denoise_h, templateWindowSize=7, searchWindowSize=21)
    
    # ========================================
    # 2. å¯¹æ¯”åº¦æ‹‰ä¼¸ (Contrast Stretching)
    # ========================================
    # å°†åƒç´ å€¼æ‹‰ä¼¸åˆ°æ›´å®½çš„èŒƒå›´ï¼Œä½†ä¸è¦å¤ªæ¿€è¿›
    min_val = np.percentile(result, 2)   # 2% åˆ†ä½æ•°
    max_val = np.percentile(result, 98)  # 98% åˆ†ä½æ•°
    
    if max_val > min_val + 20:  # ç¡®ä¿æœ‰è¶³å¤Ÿçš„åŠ¨æ€èŒƒå›´
        # çº¿æ€§æ‹‰ä¼¸åˆ° 5-250 èŒƒå›´ï¼ˆç•™ä¸€ç‚¹ä½™é‡ï¼‰
        result = np.clip((result - min_val) * 245.0 / (max_val - min_val) + 5, 0, 255).astype(np.uint8)
    
    # ========================================
    # 3. æ¸©å’Œçš„å¯¹æ¯”åº¦å¢å¼º
    # ========================================
    # ä½¿ç”¨ CLAHEï¼Œä½†å‚æ•°æ›´ä¿å®ˆ
    clahe = cv2.createCLAHE(clipLimit=1.5, tileGridSize=(16, 16))
    result = clahe.apply(result)
    
    # ========================================
    # 4. è‡ªé€‚åº”é”åŒ– (Unsharp Masking)
    # ========================================
    # å…¬å¼ï¼šsharpened = original + amount * (original - blurred)
    # ä½¿ç”¨è¾ƒå°çš„ sigma ä¿ç•™æ›´å¤šç»†èŠ‚
    blurred = cv2.GaussianBlur(result, (0, 0), 1.5)
    result = cv2.addWeighted(result, 1 + sharpen_amount, blurred, -sharpen_amount, 0)
    
    # ç¬¬äºŒæ¬¡é”åŒ– - ä½¿ç”¨ PIL çš„ UnsharpMaskï¼Œæ›´æ¸©å’Œ
    pil_img = Image.fromarray(result)
    pil_img = pil_img.filter(ImageFilter.UnsharpMask(radius=1, percent=80, threshold=3))
    result = np.array(pil_img)
    
    # ========================================
    # 5. äº®åº¦å¾®è°ƒ - è®©èƒŒæ™¯ç¨å¾®å˜ç™½
    # ========================================
    # è®¡ç®—å½“å‰èƒŒæ™¯äº®åº¦ï¼ˆå–è¾ƒäº®åŒºåŸŸçš„å‡å€¼ï¼‰
    bright_pixels = result[result > np.percentile(result, 70)]
    if len(bright_pixels) > 0:
        current_bg = np.mean(bright_pixels)
        
        # å¦‚æœèƒŒæ™¯ä¸å¤Ÿç™½ï¼Œé€‚å½“æäº®
        if current_bg < brightness_target:
            # ä½¿ç”¨ gamma æ ¡æ­£æäº®èƒŒæ™¯
            gamma = 0.95  # è½»å¾®æäº®
            inv_gamma = 1.0 / gamma
            table = np.array([((i / 255.0) ** inv_gamma) * 255 for i in range(256)]).astype(np.uint8)
            result = cv2.LUT(result, table)
    
    # ========================================
    # 6. æœ€ç»ˆå¯¹æ¯”åº¦å¾®è°ƒ
    # ========================================
    result = cv2.convertScaleAbs(result, alpha=contrast_alpha, beta=0)
    
    # ç¡®ä¿è¾“å‡ºåœ¨æœ‰æ•ˆèŒƒå›´
    result = np.clip(result, 0, 255).astype(np.uint8)
    
    return result


def convert_pages_to_grayscale(input_path, output_path, page_indices, dpi=150, enhance=True, use_ml=False):
    """
    å°†æŒ‡å®šé¡µé¢æ•´é¡µæ …æ ¼åŒ–ä¸ºç°åº¦å›¾ç‰‡å¹¶æ›¿æ¢åŸé¡µé¢
    è¿™ä¼šç§»é™¤é¡µé¢ä¸Šçš„æ‰€æœ‰æ–‡å­—ã€çŸ¢é‡å›¾å½¢ï¼Œåªä¿ç•™å•å¼ ç°åº¦å›¾ç‰‡
    ç›®çš„æ˜¯è®©è¿™äº›é¡µé¢èƒ½è¿›å…¥åç»­çš„äºŒå€¼åŒ–å¤„ç†æµç¨‹
    
    Args:
        input_path: è¾“å…¥PDFè·¯å¾„
        output_path: è¾“å‡ºPDFè·¯å¾„
        page_indices: éœ€è¦ç°åº¦åŒ–çš„é¡µé¢ç´¢å¼•åˆ—è¡¨ (0-based)
        dpi: æ …æ ¼åŒ–åˆ†è¾¨ç‡ (é»˜è®¤150ï¼Œå¹³è¡¡è´¨é‡å’Œæ–‡ä»¶å¤§å°)
        enhance: æ˜¯å¦åº”ç”¨å›¾åƒå¢å¼º
        use_ml: æ˜¯å¦ä½¿ç”¨MLå¢å¼º (True=MLå¢å¼º, False=ä¼ ç»Ÿå¢å¼º)
    """
    if not page_indices:
        return False
    
    try:
        # ä½¿ç”¨ fitz (PyMuPDF) è¿›è¡Œé¡µé¢æ …æ ¼åŒ–
        doc = fitz.open(input_path)
        converted_count = 0
        total_pages = len(page_indices)
        
        page_set = set(page_indices)
        zoom = dpi / 72.0
        mat = fitz.Matrix(zoom, zoom)
        
        # === ML Pipeline Path ===
        # ä½¿ç”¨5é˜¶æ®µæµæ°´çº¿: Render -> SLBR(CPU) -> ESRGAN(GPU) -> NAF-DPM(GPU) -> DoxaPy(CPU)
        if enhance and use_ml:
            try:
                from ml_pipeline import PipelinedMLProcessor
                
                # ä¸ä¼  models_dirï¼Œä½¿ç”¨é»˜è®¤å€¼ (SCRIPT_DIR / "models")
                # è¿™æ ·åœ¨ EXE ä¸­ä¼šè‡ªåŠ¨æ‰¾åˆ° EXE åŒçº§çš„ models ç›®å½•
                processor = PipelinedMLProcessor(
                    nafdpm_batch_size=32,
                    esrgan_overlap=8
                )
                
                # è‡ªé€‚åº”æµ‹è¯•: è·å–æœ€ä¼˜ batch_size (é¡µæ•° > 20 æ—¶)
                if len(page_indices) > 20:
                    try:
                        from adaptive_config import get_optimal_config_for_pdf
                        batch_size, esrgan_overlap = get_optimal_config_for_pdf(
                            str(input_path), processor, force_benchmark=True)
                        processor.nafdpm_batch_size = batch_size
                        processor.esrgan_overlap = esrgan_overlap
                        print(f"[è‡ªé€‚åº”] æœ€ä¼˜ batch_size={batch_size}, overlap={esrgan_overlap}")
                    except Exception as e:
                        print(f"[è‡ªé€‚åº”] æµ‹è¯•å¤±è´¥: {e}")
                
                results = processor.process_document(
                    str(input_path), page_indices, dpi=dpi)
                
                # å°†ç°åº¦ç»“æœå†™å…¥PDF
                for page_num in page_indices:
                    if page_num not in results:
                        continue
                    img_array = results[page_num]
                    page = doc[page_num]
                    h_img, w_img = img_array.shape[:2]
                    pgm_header = f"P5\n{w_img} {h_img}\n255\n".encode('ascii')
                    img_data = pgm_header + np.ascontiguousarray(img_array).tobytes()
                    page.clean_contents()
                    page_rect = page.rect
                    page.add_redact_annot(page_rect)
                    page.apply_redactions(images=fitz.PDF_REDACT_IMAGE_REMOVE)
                    page.insert_image(page_rect, stream=img_data)
                    converted_count += 1
                del results
                
            except Exception as e:
                import traceback
                safe_print(f"      [WARN] MLç®¡çº¿å¤±è´¥: {type(e).__name__}: {e}")
                safe_print(traceback.format_exc())
                safe_print("      [WARN] å›é€€åˆ°ä¼ ç»Ÿå¢å¼º")
                use_ml = False  # å›é€€æ ‡è®°ï¼Œè¿›å…¥ä¸‹æ–¹ä¼ ç»Ÿè·¯å¾„
        
        # === ä¼ ç»Ÿå¢å¼º / æ— å¢å¼ºè·¯å¾„ (ä¹Ÿç”¨äºMLå›é€€) ===
        if not (enhance and use_ml):
            desc = "      ä¼ ç»Ÿå¢å¼º" if enhance else "      æ …æ ¼åŒ–"
            pbar = tqdm(page_indices, desc=desc,
                        bar_format='{desc}: {percentage:3.0f}%|{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}] {postfix}')
            for page_num in pbar:
                if page_num >= len(doc):
                    continue
                page = doc[page_num]
                pix = page.get_pixmap(matrix=mat, colorspace=fitz.csGRAY)
                img_array = np.frombuffer(pix.samples, dtype=np.uint8).reshape(pix.height, pix.width)
                if enhance:
                    img_array = enhance_document_image(img_array)
                h_img, w_img = img_array.shape[:2]
                pgm_header = f"P5\n{w_img} {h_img}\n255\n".encode('ascii')
                img_data = pgm_header + np.ascontiguousarray(img_array).tobytes()
                page.clean_contents()
                page_rect = page.rect
                page.add_redact_annot(page_rect)
                page.apply_redactions(images=fitz.PDF_REDACT_IMAGE_REMOVE)
                page.insert_image(page_rect, stream=img_data)
                converted_count += 1
            pbar.close()
        
        if converted_count > 0:
            doc.save(output_path, garbage=4, deflate=True)
            doc.close()
            return is_valid_pdf(output_path)
        else:
            doc.close()
            return False
        
    except Exception as e:
        try:
            print(f"      [WARN] ç°åº¦è½¬æ¢å¤±è´¥: {e}")
        except UnicodeEncodeError:
            print(f"      [WARN] Grayscale conversion failed: {e}")
        return False


# ============================
# ç°åº¦/è‰²å½© ä¸¥æ ¼æ£€æµ‹é€»è¾‘
# ============================
def is_block_gray(block):
    if block.size == 0:
        return False
    mid_mask = (block > GRAY_LOWER_BOUND) & (block < GRAY_UPPER_BOUND)
    return (np.count_nonzero(mid_mask) / block.size) > BLOCK_GRAY_PIXEL_THRESHOLD


def detect_strict_color(pil_img, grid_size=20, threshold=5.0):
    """
    ä¸¥æ ¼è‰²å½©æ£€æµ‹ï¼šå°†å›¾ç‰‡åˆ‡åˆ†ä¸º grid_size x grid_size ç½‘æ ¼
    åªè¦æœ‰ä¸€ä¸ªç½‘æ ¼çš„ RGB é€šé“æ ‡å‡†å·®å‡å€¼è¶…è¿‡é˜ˆå€¼ï¼Œå³åˆ¤å®šä¸ºå½©è‰²
    """
    if pil_img.mode not in ["RGB", "CMYK"]:
        return False  # å·²ç»æ˜¯ç°åº¦æˆ–äºŒå€¼

    # è½¬æ¢ä¸ºRGB numpyæ•°ç»„
    arr = np.array(pil_img.convert("RGB"))
    h, w, _ = arr.shape

    # 1. å…¨å±€å¿«é€Ÿæ£€æµ‹ (é¿å…æ˜æ˜¾å½©è‰²å›¾æµªè´¹æ—¶é—´åˆ‡ç½‘æ ¼)
    # è®¡ç®—å›¾åƒä¸­å¿ƒçš„ 100x100 åŒºåŸŸ
    cy, cx = h // 2, w // 2
    center_sample = arr[
        max(0, cy - 50) : min(h, cy + 50), max(0, cx - 50) : min(w, cx + 50)
    ]
    if center_sample.size > 0:
        if np.mean(np.std(center_sample, axis=2)) > (threshold * 2):
            return True

    # 2. ç½‘æ ¼æ£€æµ‹
    h_step = max(h // grid_size, 1)
    w_step = max(w // grid_size, 1)

    for y in range(0, h, h_step):
        for x in range(0, w, w_step):
            # æå– Block
            block = arr[y : y + h_step, x : x + w_step]
            if block.size == 0:
                continue

            # è®¡ç®—è¯¥ Block çš„è‰²å½©é¥±å’Œåº¦ (RGBé€šé“é—´çš„æ ‡å‡†å·®)
            # å¯¹äºçº¯ç°åº¦ï¼ŒR=G=Bï¼Œstd=0
            # å…è®¸å°‘é‡å™ªç‚¹ (threshold)
            block_sat = np.mean(np.std(block, axis=2))

            if block_sat > threshold:
                return True  # å‘ç°å½©è‰²åŒºå—ï¼Œåˆ¤å®šä¸ºå½©è‰²

    return False  # æœªå‘ç°å½©è‰²åŒºå—ï¼Œåˆ¤å®šä¸ºç°åº¦


def detect_mono_or_hybrid(pil_img):
    gray = pil_img.convert("L")
    arr = np.array(gray)
    h, w = arr.shape

    mid_mask_global = (arr > 30) & (arr < 225)
    if arr.size == 0:
        return "MONO", arr
    gray_ratio = np.count_nonzero(mid_mask_global) / arr.size

    if gray_ratio < (1.0 - GLOBAL_FORCE_MONO_THRESHOLD):
        return "MONO", arr

    h_step, w_step = max(h // GRID_SIZE, 1), max(w // GRID_SIZE, 1)
    has_gray = False
    for y in range(0, h, h_step):
        for x in range(0, w, w_step):
            if is_block_gray(arr[y : y + h_step, x : x + w_step]):
                has_gray = True
                break
        if has_gray:
            break
    return ("HYBRID" if has_gray else "MONO"), arr


def detect_tile_mode(pil_img, grid_size=4):
    gray = pil_img.convert("L")
    arr = np.array(gray)
    h, w = arr.shape

    mid_mask_global = (arr > 30) & (arr < 225)
    if arr.size == 0:
        return "MONO", arr
    gray_ratio = np.count_nonzero(mid_mask_global) / arr.size

    if gray_ratio < (1.0 - GLOBAL_FORCE_MONO_THRESHOLD):
        return "MONO", arr

    h_step, w_step = max(h // grid_size, 1), max(w // grid_size, 1)
    has_gray = False
    for y in range(0, h, h_step):
        for x in range(0, w, w_step):
            if is_block_gray(arr[y : y + h_step, x : x + w_step]):
                has_gray = True
                break
        if has_gray:
            break
    return ("HYBRID" if has_gray else "MONO"), arr


def is_tile_color(pil_img):
    """æ£€æµ‹åˆ‡ç‰‡æ˜¯å¦å½©è‰² (å¤ç”¨ strict color é€»è¾‘ï¼Œä½†é’ˆå¯¹å°å›¾ä¼˜åŒ–)"""
    return detect_strict_color(pil_img, grid_size=2, threshold=5.0)


def extract_raw_jpx(data):
    """ä» JP2 å®¹å™¨ä¸­æå–åŸå§‹ Codestream (å¦‚æœå­˜åœ¨)"""
    # JP2 Signature: 00 00 00 0C 6A 50 20 20 0D 0A 87 0A
    if data.startswith(b"\x00\x00\x00\x0cjP  \r\n\x87\n"):
        # It's a JP2 file, iterate boxes to find 'jp2c'
        pos = 0
        while pos < len(data) - 8:
            box_len = int.from_bytes(data[pos : pos + 4], "big")
            box_type = data[pos + 4 : pos + 8]

            if box_len == 0:
                # 0 means box extends to end of file
                if box_type == b"jp2c":
                    # éªŒè¯æ˜¯å¦ä¸º Codestream Start (SOC): FF 4F
                    if data[pos + 8 : pos + 10] == b"\xff\x4f":
                        return data[pos + 8 :]
                break

            if box_len == 1:
                # Large box (64-bit length), skip for simplicity or handle if needed
                # JP2 usually doesn't use this for small images
                pass

            if box_type == b"jp2c":
                if data[pos + 8 : pos + 10] == b"\xff\x4f":
                    return data[pos + 8 : pos + box_len]

            pos += box_len

    # å¦‚æœä¸æ˜¯ JP2 å®¹å™¨ï¼Œæˆ–è€…æ²¡æ‰¾åˆ° jp2cï¼Œæˆ–è€…å·²ç»æ˜¯ Raw (FF 4F)
    if data.startswith(b"\xff\x4f"):
        return data

    # å¦‚æœæ— æ³•æå–ï¼Œè¿”å›åŸå§‹æ•°æ® (MuPDF æœ‰æ—¶ä¹Ÿèƒ½å®¹å¿å®Œæ•´çš„ JP2 æ•°æ®)
    return data


# ============================
# Tiling Processor Class
# ============================
class TileProcessor:
    def __init__(self, page, xref, pil_img, page_num=0, quality_db=50):
        self.page = page
        self.xref = xref
        self.pil_img = pil_img
        self.page_num = page_num
        self.quality_db = quality_db  # æ¥æ”¶è´¨é‡å‚æ•°

    def process_and_replace(self):
        stats = {"mono": 0, "hybrid": 0}
        w, h = self.pil_img.size
        h_step = h / TILE_GRID_ROWS
        w_step = w / TILE_GRID_COLS

        rects = self.page.get_image_rects(self.xref)
        if not rects:
            return False, stats
        target_rect = rects[0]

        new_tiles = []
        total_new_size = 0

        # 1. Grid Analysis
        grid_map = []
        for r in range(TILE_GRID_ROWS):
            row_data = []
            for c in range(TILE_GRID_COLS):
                y0, x0 = int(r * h_step), int(c * w_step)
                y1 = int((r + 1) * h_step) if r < TILE_GRID_ROWS - 1 else h
                x1 = int((c + 1) * w_step) if c < TILE_GRID_COLS - 1 else w

                if x1 <= x0 or y1 <= y0:
                    row_data.append(None)
                    continue
                tile_img = self.pil_img.crop((x0, y0, x1, y1))
                tile_mode, _ = detect_tile_mode(tile_img)
                row_data.append(
                    {"mode": tile_mode, "img": tile_img, "rect": (x0, y0, x1, y1)}
                )
            grid_map.append(row_data)

        # 2. Merge Logic
        visited = [
            [False for _ in range(TILE_GRID_COLS)] for _ in range(TILE_GRID_ROWS)
        ]

        for r in range(TILE_GRID_ROWS):
            for c in range(TILE_GRID_COLS):
                if visited[r][c] or not grid_map[r][c]:
                    continue

                start_node = grid_map[r][c]
                current_mode = start_node["mode"]

                # Expand Right
                max_w = 0
                for k in range(c, TILE_GRID_COLS):
                    node = grid_map[r][k]
                    if not visited[r][k] and node and node["mode"] == current_mode:
                        max_w += 1
                    else:
                        break

                # Expand Down
                max_h = 1
                for k_r in range(r + 1, TILE_GRID_ROWS):
                    row_match = True
                    for k_c in range(c, c + max_w):
                        node = grid_map[k_r][k_c]
                        if (
                            visited[k_r][k_c]
                            or not node
                            or node["mode"] != current_mode
                        ):
                            row_match = False
                            break
                    if row_match:
                        max_h += 1
                    else:
                        break

                # Mark Visited
                for i in range(max_h):
                    for j in range(max_w):
                        visited[r + i][c + j] = True

                # Create Tile
                first_tile = grid_map[r][c]
                last_tile = grid_map[r + max_h - 1][c + max_w - 1]
                x0_big, y0_big = first_tile["rect"][0], first_tile["rect"][1]
                x1_big, y1_big = last_tile["rect"][2], last_tile["rect"][3]

                merged_img = self.pil_img.crop((x0_big, y0_big, x1_big, y1_big))

                stream_data = None
                if current_mode == "MONO":
                    stats["mono"] += 1
                    # type: ignore
                    img_bin = merged_img.convert("L").point(
                        lambda x: 0 if x < BINARIZE_THRESHOLD else 255, "1"
                    )
                    buf = io.BytesIO()
                    img_bin.save(buf, format="PNG", icc_profile=None)
                    stream_data = buf.getvalue()
                else:
                    stats["hybrid"] += 1
                    # imagecodecs: GIL-free JPEG2000 ç¼–ç ï¼Œç›´æ¥è¾“å‡º J2K codestream
                    if is_tile_color(merged_img):
                        _tile_arr = np.asarray(merged_img.convert("RGB"))
                    else:
                        _tile_arr = np.asarray(merged_img.convert("L"))
                    stream_data = imagecodecs.jpeg2k_encode(
                        _tile_arr, level=self.quality_db, codecformat='j2k',
                        numthreads=JP2K_THREADS,
                    )

                total_new_size += len(stream_data)

                # Calculate PDF Coords
                scale_x = target_rect.width / w
                scale_y = target_rect.height / h
                off_x = x0_big * scale_x
                off_y = y0_big * scale_y
                tile_w_pdf = (x1_big - x0_big) * scale_x
                tile_h_pdf = (y1_big - y0_big) * scale_y

                tile_rect = fitz.Rect(
                    target_rect.x0 + off_x,
                    target_rect.y0 + off_y,
                    target_rect.x0 + off_x + tile_w_pdf,
                    target_rect.y0 + off_y + tile_h_pdf,
                )
                new_tiles.append((tile_rect, stream_data))

        # Check Size Inflation
        orig_len = len(self.page.parent.xref_stream(self.xref))
        if total_new_size >= orig_len:
            return False, stats

        # Apply
        self.page.clean_contents()
        self.page.add_redact_annot(target_rect)
        self.page.apply_redactions(images=PDF_REDACT_IMAGE_REMOVE)
        for t_rect, t_data in new_tiles:
            self.page.insert_image(t_rect, stream=t_data, overlay=True)

        return True, stats


# ============================
# Phase 1/2: Core Functions
# ============================
def surgical_clean(input_path, output_path):
    print("      ğŸ“‹ æ­£åœ¨æ‰§è¡Œæ¸…ç†...")
    try:
        with pikepdf.open(input_path) as pdf:
            if "/PieceInfo" in pdf.Root:
                del pdf.Root["/PieceInfo"]  # type: ignore
            if "/Metadata" in pdf.Root:
                del pdf.Root["/Metadata"]  # type: ignore
            for page in pdf.pages:
                if "/PieceInfo" in page:
                    del page["/PieceInfo"]  # type: ignore
                if "/Thumb" in page:
                    del page["/Thumb"]  # type: ignore
            pdf.remove_unreferenced_resources()
            pdf.save(
                output_path,
                compress_streams=True,
                object_stream_mode=pikepdf.ObjectStreamMode.generate,
            )
        return is_valid_pdf(output_path)
    except:
        return False


def get_gs_path():
    base_dir = os.path.dirname(os.path.abspath(__file__))
    local_gs = os.path.join(base_dir, "gs", "bin", "gswin64c.exe")
    if os.path.exists(local_gs):
        return local_gs
    if shutil.which("gswin64c"):
        return "gswin64c"
    if shutil.which("gs"):
        return "gs"
    return None


def run_gs_level0(input_path, output_path, total_pages=0):
    gs_exe = get_gs_path()
    if not gs_exe:
        return False
    cmd = [
        gs_exe,
        "-sDEVICE=pdfwrite",
        "-dCompatibilityLevel=1.4",
        "-dNOPAUSE",
        "-dBATCH",
        "-dSAFER",
        f"-sOutputFile={output_path}",
        "-dPDFSETTINGS=/prepress",
        "-dDownsampleColorImages=false",
        "-dDownsampleGrayImages=false",
        "-dDownsampleMonoImages=false",
        "-dAutoFilterColorImages=false",
        "-dAutoFilterGrayImages=false",
        "-dColorImageFilter=/FlateEncode",
        "-dGrayImageFilter=/FlateEncode",
        input_path,
    ]
    try:
        process = subprocess.Popen(
            cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True
        )
        pbar = None  # å»¶è¿Ÿåˆå§‹åŒ–è¿›åº¦æ¡ï¼Œä»¥ä¾¿æ˜¾ç¤ºåŸå§‹è¾“å‡º

        if process.stdout:
            for line in process.stdout:
                # æ£€æŸ¥æ˜¯å¦å¼€å§‹å¤„ç†é¡µé¢ (GSè¾“å‡ºé€šå¸¸åŒ…å« "Page X")
                if "Page" in line:
                    if pbar is None and total_pages > 0:
                        # é¦–æ¬¡æ£€æµ‹åˆ°é¡µé¢å¤„ç†ï¼Œåˆå§‹åŒ–è¿›åº¦æ¡
                        pbar = tqdm(
                            total=total_pages,
                            desc="      ğŸ›¡ï¸ GS é‡æ„",
                            unit="page",
                        )

                    if pbar:
                        try:
                            pbar.update(1)
                        except:
                            pass
                else:
                    # åœ¨è¿›åº¦æ¡å‡ºç°å‰ï¼Œæ˜¾ç¤º GS çš„åŸå§‹è¾“å‡º (å¦‚ Loading font...)
                    if pbar is None:
                        # ä½¿ç”¨ \r è¦†ç›–å½“å‰è¡Œï¼Œé¿å…åˆ·å±
                        print(f"      [GS] {line.strip()}", end="\r")

        process.wait()
        if pbar:
            pbar.close()
        return process.returncode == 0 and is_valid_pdf(output_path)
    except:
        return False


# Regex Utils
NUM_RE = rb"[-+]?(?:\d*\.\d+|\d+)"
L_PATTERN = re.compile(rb"(" + NUM_RE + rb")(\s+)(" + NUM_RE + rb")(\s+)(l)(?=\s|$)")
C_PATTERN = re.compile(
    rb"("
    + NUM_RE
    + rb")(\s+)("
    + NUM_RE
    + rb")(\s+)"
    + rb"("
    + NUM_RE
    + rb")(\s+)("
    + NUM_RE
    + rb")(\s+)"
    + rb"("
    + NUM_RE
    + rb")(\s+)("
    + NUM_RE
    + rb")(\s+)(c)(?=\s|$)"
)
VY_PATTERN = re.compile(
    rb"("
    + NUM_RE
    + rb")(\s+)("
    + NUM_RE
    + rb")(\s+)"
    + rb"("
    + NUM_RE
    + rb")(\s+)("
    + NUM_RE
    + rb")(\s+)([vy])(?=\s|$)"
)
M_PATTERN = re.compile(rb"(" + NUM_RE + rb")(\s+)(" + NUM_RE + rb")(\s+)(m)(?=\s|$)")
W_PATTERN = re.compile(rb"(" + NUM_RE + rb")(\s+)(w)(?=\s|$)")


def format_number(bytes_val, sig_figs):
    try:
        # å®‰å…¨æ£€æŸ¥ï¼šè¿‡çŸ­çš„æ•°å­—ä¸å¤„ç†
        if len(bytes_val) < 3:
            return bytes_val
        val = float(bytes_val)
        new_str = (
            "{:.0f}".format(val)
            if val.is_integer()
            else "{:.{p}g}".format(val, p=sig_figs)
        )
        if new_str.startswith("0."):
            new_str = new_str[1:]
        elif new_str.startswith("-0."):
            new_str = "-" + new_str[2:]
        new_bytes = new_str.encode("ascii")
        if len(new_bytes) < len(bytes_val):
            return new_bytes
        return bytes_val
    except:
        return bytes_val


def replace_2args(m, sig_figs):
    """å¤„ç† l, m ç­‰ 2 å‚æ•°å‘½ä»¤"""
    return (
        format_number(m.group(1), sig_figs)
        + m.group(2)
        + format_number(m.group(3), sig_figs)
        + m.group(4)
        + m.group(5)
    )


def replace_w(m, sig_figs):
    """å¤„ç† w (çº¿å®½) å‘½ä»¤"""
    return format_number(m.group(1), sig_figs) + m.group(2) + m.group(3)


def replace_vy(m, sig_figs):
    """å¤„ç† v, y æ›²çº¿å‘½ä»¤ (4 å‚æ•°)"""
    n1 = format_number(m.group(1), sig_figs)
    n2 = format_number(m.group(3), sig_figs)
    n3 = format_number(m.group(5), sig_figs)
    n4 = format_number(m.group(7), sig_figs)
    op = m.group(9)
    return (
        n1
        + m.group(2)
        + n2
        + m.group(4)
        + n3
        + m.group(6)
        + n4
        + m.group(8)
        + op
    )


def replace_c_v4(m, sig_figs):
    """å¤„ç† c æ›²çº¿å‘½ä»¤ (6 å‚æ•°)"""
    nums = [format_number(m.group(i), sig_figs) for i in range(1, 12, 2)]
    spaces = [m.group(i) for i in range(2, 13, 2)]
    return b"".join(n + s for n, s in zip(nums, spaces)) + m.group(13)


def replace_c_smart(m, sig_figs):
    """æ™ºèƒ½æ›²çº¿ç®€åŒ–ï¼šå¦‚æœæ›²çº¿è¶³å¤Ÿå°ï¼Œå¯ä»¥æ›¿æ¢ä¸ºç›´çº¿"""
    try:
        x1, y1 = float(m.group(1)), float(m.group(3))
        x2, y2 = float(m.group(5)), float(m.group(7))
        x3, y3 = float(m.group(9)), float(m.group(11))
        span_x = max(x1, x2, x3) - min(x1, x2, x3)
        span_y = max(y1, y2, y3) - min(y1, y2, y3)
        if span_x < CURVE_SIMPLIFY_THRESHOLD and span_y < CURVE_SIMPLIFY_THRESHOLD:
            n3 = format_number(m.group(9), sig_figs)
            n4 = format_number(m.group(11), sig_figs)
            return n3 + b" " + n4 + b" l"
    except:
        pass
    return replace_c_v4(m, sig_figs)


def process_regex_chunk(file_path, xref_list, sig_figs, enable_smart_c):
    results = {}
    inline_img_re = re.compile(rb"(^|\s)BI(\s|$)")
    try:
        with pikepdf.open(file_path) as pdf:
            for xref in xref_list:
                try:
                    stream_obj = pdf.objects[xref]
                    if not isinstance(stream_obj, pikepdf.Stream):
                        continue
                    subtype = str(stream_obj.get("/Subtype") or "")
                    if "/Image" in subtype or "/Font" in subtype:
                        continue
                    raw_data = stream_obj.read_bytes()
                    if inline_img_re.search(raw_data):
                        continue

                    d = L_PATTERN.sub(lambda m: replace_2args(m, sig_figs), raw_data)
                    d = VY_PATTERN.sub(lambda m: replace_vy(m, sig_figs), d)
                    d = M_PATTERN.sub(lambda m: replace_2args(m, sig_figs), d)
                    d = W_PATTERN.sub(lambda m: replace_w(m, sig_figs), d)
                    if enable_smart_c:
                        d = C_PATTERN.sub(lambda m: replace_c_smart(m, sig_figs), d)
                    else:
                        d = C_PATTERN.sub(lambda m: replace_c_v4(m, sig_figs), d)

                    if len(d) < len(raw_data):
                        results[xref] = d
                except:
                    continue
        return results
    except:
        return {}


def run_regex_pass(input_path, output_path, sig_figs, enable_smart_c, desc):
    try:
        candidate_xrefs = []
        with pikepdf.open(input_path) as pdf:
            for i, obj in enumerate(pdf.objects):
                if isinstance(obj, pikepdf.Stream):
                    subtype = str(obj.get("/Subtype") or "")
                    if "/Image" not in subtype and "/Font" not in subtype:
                        candidate_xrefs.append(i)

        chunks = [
            candidate_xrefs[i : i + CHUNK_SIZE]
            for i in range(0, len(candidate_xrefs), CHUNK_SIZE)
        ]
        all_results = {}
        with ThreadPoolExecutor() as executor:
            futures = {
                executor.submit(
                    process_regex_chunk, input_path, chunk, sig_figs, enable_smart_c
                ): chunk
                for chunk in chunks
            }
            for f in tqdm(
                as_completed(futures),
                total=len(chunks),
                desc=f"      ğŸ”¥ {desc}",
                unit="chunk",
            ):
                if res := f.result():
                    all_results.update(res)

        if not all_results:
            return False
        with pikepdf.open(input_path) as pdf:
            for xref, data in all_results.items():
                pdf.objects[xref].write(data)
            pdf.remove_unreferenced_resources()
            pdf.save(
                output_path,
                compress_streams=True,
                object_stream_mode=pikepdf.ObjectStreamMode.generate,
            )
        return is_valid_pdf(output_path)
    except:
        return False


# ============================
# Phase 2: Safe Image Pass (Modified)
# ============================
def _compress_single_xref(pdf, xref, target_quality, mixed_page_xrefs):
    """å•ä¸ªxrefçš„å‹ç¼©é€»è¾‘ï¼ˆæ¥å—å·²æ‰“å¼€çš„pdfå¯¹è±¡ï¼Œé¿å…é‡å¤æ‰“å¼€PDFï¼‰"""
    image_obj = pdf.objects[xref]
    # Safety Checks
    if "/SMask" in image_obj or "/Mask" in image_obj:
        return None

    try:
        pdfimage = pikepdf.PdfImage(image_obj)
        pil_image = pdfimage.as_pil_image()

        # å®‰å…¨æ£€æŸ¥ï¼šå¦‚æœå›¾ç‰‡è¿‡å¤§ï¼Œå¯èƒ½å¯¼è‡´ MemoryErrorï¼Œç›´æ¥è·³è¿‡
        # 100M åƒç´ ä»¥ä¸Š (e.g. 10000x10000)
        if pil_image.width * pil_image.height > 100_000_000:
            return None

    except:
        # jbig2dec missing or other extraction error -> skip image
        return None

    # 1. ä¸¥æ ¼ç°åº¦æ£€æµ‹
    is_color = detect_strict_color(
        pil_image, grid_size=GRID_SIZE, threshold=COLOR_STD_THRESHOLD
    )

    # é€»è¾‘ä¿®æ­£ï¼š
    # å®‰å…¨å›¾ç‰‡å‹ç¼©é˜¶æ®µåº”å¤„ç†ï¼š
    # 1. æ‰€æœ‰å½©è‰²å›¾ç‰‡ (is_color = True)
    # 2. ä½äº "æ··åˆé¡µé¢" (æœ‰æ–‡å­—/çŸ¢é‡) ä¸Šçš„ç°åº¦å›¾ç‰‡ (is_color = False and xref in mixed_page_xrefs)
    #
    # å®‰å…¨å›¾ç‰‡å‹ç¼©é˜¶æ®µåº”è·³è¿‡ï¼š
    # 1. ä½äº "çº¯å›¾ç‰‡é¡µé¢" ä¸Šçš„ç°åº¦å›¾ç‰‡ (äº¤ç»™åˆ‡ç‰‡/äºŒå€¼åŒ–é˜¶æ®µ)

    if not is_color:
        if xref not in mixed_page_xrefs:
            # è¿™æ˜¯ä¸€ä¸ªç°åº¦å›¾ï¼Œä¸”ä¸åœ¨æ··åˆé¡µé¢ä¸Š -> è·³è¿‡ï¼Œç•™ç»™åˆ‡ç‰‡/äºŒå€¼åŒ–é˜¶æ®µ
            return None

        # å¦åˆ™ï¼šè™½ç„¶æ˜¯ç°åº¦ï¼Œä½†åœ¨æ··åˆé¡µé¢ä¸Šï¼Œè¿›è¡Œå®‰å…¨å‹ç¼©
        # === æ–°å¢é€»è¾‘ï¼šæ··åˆé¡µé¢ä¸­çš„å°å›¾ï¼Œå¦‚æœæ»¡è¶³äºŒå€¼åŒ–æ¡ä»¶ï¼Œç›´æ¥äºŒå€¼åŒ– ===
        mode, _ = detect_mono_or_hybrid(pil_image)
        if mode == "MONO" and target_quality is not None:
            # äºŒå€¼åŒ–å¤„ç†
            # type: ignore
            img_bin = pil_image.convert("L").point(
                lambda x: 0 if x < BINARIZE_THRESHOLD else 255, "1"
            )

            # ä½¿ç”¨ Flate (Zlib) å‹ç¼©åŸå§‹ 1-bit æ•°æ®
            # PIL tobytes("raw") å¯¹äº mode "1" è¿”å› packed bits (æ¯è¡Œ byte å¯¹é½)ï¼Œç¬¦åˆ PDF è¦æ±‚
            raw_bits = img_bin.tobytes()
            new_data = zlib.compress(raw_bits, level=9)

            if len(new_data) >= len(image_obj.read_raw_bytes()):
                return None

            return {
                "xref": xref,
                "data": new_data,
                "width": img_bin.width,
                "height": img_bin.height,
                "mode": "1",  # æ ‡è®°ä¸ºäºŒå€¼å›¾
            }
        # å¦‚æœæ˜¯ Hybrid ç°åº¦å›¾ï¼Œç»§ç»­ä¸‹æ–¹çš„ JP2 æµç¨‹

    # æ­£å¸¸å¤„ç† (Color æˆ– Mixed-Hybrid-Gray)
    if pil_image.mode not in ["RGB", "L"]:
        pil_image = pil_image.convert("RGB")

    try:
        img_arr = np.asarray(pil_image)
        if target_quality is not None:
            new_data = imagecodecs.jpeg2k_encode(img_arr, level=target_quality, numthreads=JP2K_THREADS)
        else:
            new_data = imagecodecs.jpeg2k_encode(img_arr, level=0, numthreads=JP2K_THREADS)
    except Exception:
        # Fallback for PIL/OpenJPEG encoding errors (broken data stream)
        # Try saving as PNG then return original (skip compression) or retry logic
        return None

    if len(new_data) >= len(image_obj.read_raw_bytes()):
        return None

    return {
        "xref": xref,
        "data": new_data,
        "width": pil_image.width,
        "height": pil_image.height,
        "mode": pil_image.mode,
    }


def compress_image_worker(xref, target_quality, pdf_path, mixed_page_xrefs):
    """å•å›¾å…¼å®¹æ¥å£ï¼šæ¯æ¬¡è°ƒç”¨æ‰“å¼€PDFï¼ˆä¿ç•™å‘åå…¼å®¹ï¼‰"""
    with pikepdf.open(pdf_path) as pdf:
        return _compress_single_xref(pdf, xref, target_quality, mixed_page_xrefs)


def compress_image_chunk_worker(xref_chunk, target_quality, pdf_path, mixed_page_xrefs, progress_callback=None):
    """æ‰¹é‡å¤„ç†ä¸€ç»„xrefï¼Œåªæ‰“å¼€ä¸€æ¬¡PDFï¼ˆå‡å°‘96%çš„PDFè§£æå¼€é”€ï¼‰"""
    results = []
    try:
        with pikepdf.open(pdf_path) as pdf:
            for xref in xref_chunk:
                res = None
                try:
                    res = _compress_single_xref(pdf, xref, target_quality, mixed_page_xrefs)
                    if res is not None:
                        results.append(res)
                except Exception:
                    pass
                if progress_callback:
                    progress_callback(1, res is not None)
    except Exception:
        pass
    return results


def _encode_single_image(xref, pil_image, orig_raw_size, target_quality, mixed_page_xrefs):
    """Phase B ç¼–ç å‡½æ•°ï¼šä½¿ç”¨ imagecodecs å®ç° GIL-free JPEG2000 ç¼–ç ï¼Œå¯å®‰å…¨å¤šçº¿ç¨‹å¹¶å‘ã€‚
    ä¸ _compress_single_xref é€»è¾‘ä¸€è‡´ï¼Œä½†ä¸ä¾èµ– pikepdf å¯¹è±¡ï¼ˆå›¾ç‰‡å·²åœ¨ Phase A æå–ï¼‰ã€‚
    """
    # 1. ä¸¥æ ¼ç°åº¦æ£€æµ‹
    is_color = detect_strict_color(
        pil_image, grid_size=GRID_SIZE, threshold=COLOR_STD_THRESHOLD
    )

    if not is_color:
        if xref not in mixed_page_xrefs:
            # ç°åº¦å›¾ä¸åœ¨æ··åˆé¡µé¢ -> ç•™ç»™åˆ‡ç‰‡/äºŒå€¼åŒ–é˜¶æ®µ
            return None

        # æ··åˆé¡µé¢ä¸­çš„ç°åº¦å›¾ï¼Œæ£€æŸ¥æ˜¯å¦å¯äºŒå€¼åŒ–
        mode, _ = detect_mono_or_hybrid(pil_image)
        if mode == "MONO" and target_quality is not None:
            # type: ignore
            img_bin = pil_image.convert("L").point(
                lambda x: 0 if x < BINARIZE_THRESHOLD else 255, "1"
            )
            raw_bits = img_bin.tobytes()
            new_data = zlib.compress(raw_bits, level=9)
            if len(new_data) >= orig_raw_size:
                return None
            return {
                "xref": xref,
                "data": new_data,
                "width": img_bin.width,
                "height": img_bin.height,
                "mode": "1",
            }

    # æ­£å¸¸å¤„ç† (Color æˆ– Mixed-Hybrid-Gray)
    if pil_image.mode not in ["RGB", "L"]:
        pil_image = pil_image.convert("RGB")

    try:
        img_arr = np.asarray(pil_image)
        if target_quality is not None:
            new_data = imagecodecs.jpeg2k_encode(img_arr, level=target_quality, numthreads=JP2K_THREADS)
        else:
            new_data = imagecodecs.jpeg2k_encode(img_arr, level=0, numthreads=JP2K_THREADS)
    except Exception:
        return None

    if len(new_data) >= orig_raw_size:
        return None

    return {
        "xref": xref,
        "data": new_data,
        "width": pil_image.width,
        "height": pil_image.height,
        "mode": pil_image.mode,
    }


def run_image_pass_safe(input_path, output_path, quality_db, desc):
    """å®‰å…¨å›¾ç‰‡å‹ç¼© Pass"""

    # 1. é¢„æ‰«æï¼šè¯†åˆ«å“ªäº› XREF å±äº "æ··åˆé¡µé¢" (æœ‰æ–‡å­—/çŸ¢é‡)
    # è¿™äº›é¡µé¢ä¸Šçš„ç°åº¦å›¾å¿…é¡»åœ¨å®‰å…¨å›¾ç‰‡å‹ç¼©é˜¶æ®µå¤„ç†ï¼Œä¸èƒ½ç•™ç»™ç ´åæ€§åˆ‡ç‰‡é˜¶æ®µ
    mixed_page_xrefs = set()
    try:
        doc = fitz.open(input_path)
        for page in doc:
            has_text = len(page.get_text("text").strip()) > 0
            # get_drawings() å¼€é”€è¾ƒå¤§ï¼›æœ‰æ–‡å­—æ—¶å·²å¯åˆ¤å®šä¸ºæ··åˆé¡µï¼Œç›´æ¥çŸ­è·¯ã€‚
            has_drawings = False
            if not has_text:
                has_drawings = len(page.get_drawings()) > 0

            if has_text or has_drawings:
                # è¿™æ˜¯ä¸€ä¸ªæ··åˆé¡µé¢ï¼Œè®°å½•å…¶æ‰€æœ‰å›¾ç‰‡ XREF
                img_list = page.get_images(full=True)
                for img in img_list:
                    mixed_page_xrefs.add(img[0])  # xref is index 0
        doc.close()
    except:
        pass  # å¦‚æœæ‰«æå¤±è´¥ï¼Œmixed_page_xrefs ä¸ºç©ºï¼Œç°åº¦å›¾å°†å…¨éƒ¨è·³è¿‡ç»™åˆ‡ç‰‡é˜¶æ®µ (é£é™©è¾ƒå°)

    # === Phase A: å•çº¿ç¨‹æå– (GIL-bound pikepdf) ===
    # åªæ‰“å¼€PDFä¸€æ¬¡ï¼Œæå–æ‰€æœ‰å›¾ç‰‡ä¸ºPILå¯¹è±¡ + åŸå§‹å¤§å°
    extracted = []  # list of (xref, pil_image, orig_raw_size)
    try:
        with pikepdf.open(input_path) as pdf:
            for i, obj in enumerate(pdf.objects):
                if isinstance(obj, pikepdf.Stream) and obj.get("/Subtype") == "/Image":
                    raw_size = len(obj.read_raw_bytes())
                    if raw_size < MIN_IMAGE_SIZE:
                        continue
                    # è·³è¿‡å¸¦maskçš„å›¾ç‰‡
                    if "/SMask" in obj or "/Mask" in obj:
                        continue
                    try:
                        pdfimage = pikepdf.PdfImage(obj)
                        pil_image = pdfimage.as_pil_image()
                        if pil_image.width * pil_image.height > 100_000_000:
                            continue
                        extracted.append((i, pil_image, raw_size))
                    except Exception:
                        continue
    except Exception:
        return False
    if not extracted:
        return False

    # === Phase B: å¤šçº¿ç¨‹ç¼–ç  (GIL-free numpy/PIL/zlib) ===
    results = []
    opt_count = [0]
    pbar = tqdm(
        total=len(extracted),
        desc=f"      ğŸ¨ {desc} (æ™ºèƒ½æ··åˆ)",
        unit="img",
    )

    def on_encode_done(future):
        """ç¼–ç å®Œæˆå›è°ƒ (GIL-freeæ“ä½œå®Œæˆå)"""
        try:
            res = future.result()
        except Exception:
            res = None
        pbar.update(1)
        if res is not None:
            opt_count[0] += 1
            pbar.set_postfix(opt=opt_count[0])

    with ThreadPoolExecutor(max_workers=JP2K_WORKERS) as executor:
        futures = []
        for xref, pil_image, orig_raw_size in extracted:
            f = executor.submit(
                _encode_single_image, xref, pil_image, orig_raw_size,
                quality_db, mixed_page_xrefs
            )
            f.add_done_callback(on_encode_done)
            futures.append(f)
        # ç­‰å¾…æ‰€æœ‰ç¼–ç å®Œæˆ
        for f in futures:
            try:
                res = f.result()
            except Exception:
                res = None
            if res is not None:
                results.append(res)
    pbar.close()

    if results:
        try:
            with pikepdf.open(input_path, allow_overwriting_input=True) as pdf:
                for res in results:
                    obj = pdf.objects[res["xref"]]

                    if res.get("mode") == "1":
                        # äºŒå€¼åŒ–å›¾ç‰‡ (FlateDecode)
                        obj.write(res["data"], filter=pikepdf.Name("/FlateDecode"))
                        obj.Width = res["width"]
                        obj.Height = res["height"]
                        obj.ColorSpace = pikepdf.Name("/DeviceGray")
                        obj.BitsPerComponent = 1
                    else:
                        # æ™®é€šå½©è‰²/ç°åº¦å›¾ç‰‡ (JPXDecode)
                        obj.write(res["data"], filter=pikepdf.Name("/JPXDecode"))
                        obj.Width = res["width"]
                        obj.Height = res["height"]
                        obj.ColorSpace = (
                            pikepdf.Name("/DeviceRGB")
                            if res["mode"] == "RGB"
                            else pikepdf.Name("/DeviceGray")
                        )
                        obj.BitsPerComponent = 8
                        if "/Filter" in obj:
                            obj.Filter = pikepdf.Name("/JPXDecode")

                    # æ¸…ç†é€šç”¨å±æ€§
                    if "/DecodeParms" in obj:
                        del obj["/DecodeParms"]  # type: ignore
                    if "/ICCProfile" in obj:
                        del obj["/ICCProfile"]  # type: ignore
                pdf.remove_unreferenced_resources()
                pdf.save(output_path, compress_streams=True)
            return is_valid_pdf(output_path)
        except:
            return False
    return False


# ============================
# Phase 3: Tiling Pass
# ============================
def process_page_chunk_tiling(input_path, start_page, end_page, chunk_id, target_quality, progress_callback=None):
    # ä½¿ç”¨ç¨³å¥çš„ä¸´æ—¶æ–‡ä»¶å (é¿å…åŒ…å« .tmp ç­‰å¹²æ‰°)
    base, _ = os.path.splitext(input_path)
    # å– base çš„ hash æˆ– ç®€å•æ¸…ç†ï¼Œé˜²æ­¢è¿‡é•¿
    # ç”Ÿæˆï¼šcurrent_dir/temp_chunk_{id}.pdf
    chunk_output = os.path.join(
        os.path.dirname(input_path), f"temp_chunk_{chunk_id}.pdf"
    )

    chunk_stats = {
        "mono_tiles": 0,
        "hybrid_tiles": 0,
        "pages_modified": 0,
        "pages_skipped": 0,
    }

    doc = fitz.open(input_path)
    new_doc = fitz.open()
    new_doc.insert_pdf(doc, from_page=start_page, to_page=end_page)
    doc.close()

    modified_count = 0
    for i in range(len(new_doc)):
        page = new_doc[i]
        img_list = page.get_images(full=True)
        if not img_list:
            chunk_stats["pages_skipped"] += 1
            if progress_callback:
                progress_callback(1, False)
            continue

        # ä¸€ï¼‰å‰ç½®æ¡ä»¶ï¼šåªæœ‰ "çº¯å›¾ç‰‡é¡µé¢" (æ— æ–‡å­—ã€æ— çŸ¢é‡) æ‰è¿›å…¥ç ´åæ€§åˆ‡ç‰‡å¤„ç†
        # å¦‚æœåŒ…å«æ–‡å­—æˆ–çŸ¢é‡å›¾ï¼Œç›´æ¥è·³è¿‡ (ä¸è¿›è¡ŒåŸä½å‹ç¼©ï¼Œå› ä¸ºå®‰å…¨é˜¶æ®µå¯èƒ½å·²ç»å¤„ç†è¿‡ï¼Œæˆ–è€…ç”¨æˆ·å¸Œæœ›ä¿ç•™åŸæ ·)
        has_text = len(page.get_text("text").strip()) > 0
        has_drawings = len(page.get_drawings()) > 0
        if has_text or has_drawings:
            chunk_stats["pages_skipped"] += 1
            if progress_callback:
                progress_callback(1, False)
            continue

        page_modified = False
        for img_info in list(img_list):
            xref = img_info[0]
            rects = page.get_image_rects(xref)
            if not rects:
                continue

            # è®¡ç®—å›¾ç‰‡å æ¯”ï¼Œç”¨äº Hybrid æ¨¡å¼ä¸‹çš„å†³ç­–
            is_large = (sum(r.get_area() for r in rects) / page.rect.get_area()) > 0.5

            pix = fitz.Pixmap(new_doc, xref)
            if pix.n >= 4:
                pix = fitz.Pixmap(fitz.csRGB, pix)
            if pix.n == 1:
                pil_img = Image.frombytes('L', [pix.width, pix.height], pix.samples)
                pil_img = pil_img.convert('RGB')
            elif pix.n == 2:
                # ç°åº¦+alpha â†’ å»alphaè½¬RGB
                pix_no_alpha = fitz.Pixmap(fitz.csGRAY, pix)
                pil_img = Image.frombytes('L', [pix_no_alpha.width, pix_no_alpha.height], pix_no_alpha.samples)
                pil_img = pil_img.convert('RGB')
            else:
                pil_img = Image.frombytes('RGB', [pix.width, pix.height], pix.samples)

            # 1. ä¸¥æ ¼ç°åº¦æ£€æµ‹
            is_color = detect_strict_color(
                pil_img, grid_size=GRID_SIZE, threshold=COLOR_STD_THRESHOLD
            )
            if is_color:
                continue  # å½©è‰²å›¾è·³è¿‡

            mode = detect_mono_or_hybrid(pil_img)[0]
            # åŸºäºå½“å‰ xref çš„è¦†ç›–ç‡ï¼Œé¿å…å°é¢ç§¯æ‰¿è½½å±‚è§¦å‘æ•´é¡µé‡å†™
            coverage_ratio = (sum(r.get_area() for r in rects) / page.rect.get_area()) if rects else 0.0

            # 2. ç°åº¦å›¾å¤„ç†ç­–ç•¥
            # æ­¤æ—¶å·²ç¡®å®šé¡µé¢æ— å¹²æ‰°å…ƒç´ ï¼Œå¯ä»¥å®‰å…¨åœ°é‡æ„é¡µé¢
            if mode == "MONO":
                # é˜²æŠ¤1ï¼šä»…å¤„ç†è¦†ç›–ç‡è¾ƒé«˜çš„ MONO å›¾å±‚ï¼Œé¿å…å°é¢ç§¯å›¾å±‚è¯¯è§¦å‘æ•´é¡µ clean_contents
                if coverage_ratio < 0.5:
                    continue

                # é˜²æŠ¤2ï¼šè·³è¿‡â€œçº¯é»‘æ‰¿è½½å±‚â€ï¼ˆå¸¸è§äºå¸¦ alpha/å å±‚çš„èµ„æºé¡µï¼‰ï¼Œé¿å…è¾“å‡ºæ•´é¡µé»‘å±
                gray_arr = np.asarray(pil_img.convert("L"), dtype=np.uint8)
                if int(gray_arr.max()) == 0:
                    continue

                # å…¨å•è‰² -> ç›´æ¥äºŒå€¼åŒ–é‡æ„ (ä¸åŒºåˆ†å¤§å°ï¼Œåªè¦æ˜¯çº¯å›¾é¡µé¢çš„ Mono å‡å¤„ç†)
                # type: ignore
                img_bin = pil_img.convert("L").point(
                    lambda x: 0 if x < BINARIZE_THRESHOLD else 255, "1"
                )
                buf = io.BytesIO()
                img_bin.save(buf, format="PNG", icc_profile=None)

                # é‡æ„é¡µé¢å†…å®¹
                page.clean_contents()
                page.add_redact_annot(rects[0])
                page.apply_redactions(images=PDF_REDACT_IMAGE_REMOVE)
                for rect in rects:
                    page.insert_image(rect, stream=buf.getvalue(), overlay=True)
                page_modified = True
                chunk_stats["mono_tiles"] += 1

            elif mode == "HYBRID":
                # äºŒï¼‰is_large é€»è¾‘ï¼š
                # åœ¨è¿›å…¥åˆ‡åˆ†åˆ†æ”¯åï¼Œå¦‚æœä¸æ˜¯çº¯äºŒå€¼åŒ–æƒ…å†µ (å³ Hybrid)ï¼Œ
                # åªæœ‰ "å¤§å›¾" æ‰è¿›è¡Œåˆ‡ç‰‡å¤„ç†ï¼›å°å›¾ç›´æ¥è·³è¿‡ã€‚
                if is_large:
                    processor = TileProcessor(
                        page,
                        xref,
                        pil_img,
                        page_num=(start_page + i),
                        quality_db=target_quality,
                    )
                    success, stats = processor.process_and_replace()
                    if success:
                        page_modified = True
                        chunk_stats["mono_tiles"] += stats["mono"]
                        chunk_stats["hybrid_tiles"] += stats["hybrid"]
                else:
                    # Hybrid å°å›¾ -> è·³è¿‡ (User requested: skip instead of in-place re-compress)
                    pass

        if page_modified:
            modified_count += 1
            chunk_stats["pages_modified"] += 1
        else:
            chunk_stats["pages_skipped"] += 1
        if progress_callback:
            progress_callback(1, page_modified)

    # é™ä½åƒåœ¾å›æ”¶çº§åˆ«ï¼Œé¿å…åœ¨ Chunk é˜¶æ®µç ´åå¼•ç”¨
    new_doc.save(chunk_output, garbage=0, deflate=True)
    new_doc.close()
    return chunk_output, chunk_stats


def run_tiling_pass(input_path, output_path, target_quality, desc):
    """æ‰§è¡Œç‰©ç†åˆ‡ç‰‡ Pass (é’ˆå¯¹ç°åº¦å›¾)"""
    file_mb = get_file_mb(input_path)
    safe_print(f"      [TILE] {desc} (ç°åº¦åˆ‡ç‰‡/ä¼˜åŒ– {target_quality}dB)...")
    # æŠ‘åˆ¶ MuPDF çš„ C çº§è¾“å‡º
    try:
        fitz.tools.set_stderr_file(os.devnull)  # type: ignore
    except:
        pass

    doc = fitz.open(input_path)
    total_pages = len(doc)
    doc.close()

    chunk_size = (total_pages + JP2K_WORKERS - 1) // JP2K_WORKERS
    futures = []
    modified_count = [0]  # ç”¨liståŒ…è£…ä»¥ä¾¿åœ¨é—­åŒ…ä¸­ä¿®æ”¹

    pbar = tqdm(
        total=total_pages,
        desc=f"      ğŸ§© {desc} (åˆ‡ç‰‡)",
        unit="page",
    )

    def on_page_done(n, was_modified):
        """æ¯å¤„ç†å®Œä¸€é¡µç”±workerçº¿ç¨‹å›è°ƒï¼ˆtqdm.updateçº¿ç¨‹å®‰å…¨ï¼‰"""
        pbar.update(n)
        if was_modified:
            modified_count[0] += 1
            pbar.set_postfix(mod=modified_count[0])

    with ThreadPoolExecutor(max_workers=JP2K_WORKERS) as executor:
        for i in range(JP2K_WORKERS):
            start = i * chunk_size
            end = min((i + 1) * chunk_size - 1, total_pages - 1)
            if start > end:
                break
            futures.append(
                executor.submit(
                    process_page_chunk_tiling, input_path, start, end, i, target_quality,
                    progress_callback=on_page_done
                )
            )

    chunk_files = []
    total_stats = {
        "mono_tiles": 0,
        "hybrid_tiles": 0,
        "pages_modified": 0,
        "pages_skipped": 0,
    }

    for f in futures:
        res, stats = f.result()
        if res:
            chunk_files.append(res)
            for k in total_stats:
                total_stats[k] += stats.get(k, 0)
    pbar.close()

    # åˆå¹¶ (Robust Sort)
    # æ–‡ä»¶åæ ¼å¼: temp_chunk_{id}.pdf
    def get_chunk_id(fname):
        # æ‰¾åˆ°æœ€åä¸€ä¸ª _ å’Œ .pdf ä¹‹é—´çš„æ•°å­—
        return int(fname.split("_")[-1].replace(".pdf", ""))

    chunk_files.sort(key=get_chunk_id)

    if not chunk_files:
        return False

    final_doc = fitz.open()
    for cf in chunk_files:
        c_doc = fitz.open(cf)
        final_doc.insert_pdf(c_doc)
        c_doc.close()

    final_doc.save(output_path, garbage=4, deflate=True)
    final_doc.close()

    for cf in chunk_files:
        os.remove(cf)

    safe_print(
        f"      [STAT] åˆ‡ç‰‡ç»Ÿè®¡: ä¿®æ”¹={total_stats['pages_modified']} | åˆ‡ç‰‡: äºŒå€¼={total_stats['mono_tiles']} æ··åˆ={total_stats['hybrid_tiles']}"
    )
    return is_valid_pdf(output_path)


# ============================
# ä¸»æµç¨‹
# ============================
def process_file(input_path, idx, total):
    file_mb = get_file_mb(input_path)
    initially_under_threshold = file_mb < SIZE_THRESHOLD_MB
    base_name = os.path.basename(input_path)
    safe_print(f"\n[{idx}/{total}] Processing: {base_name} ({file_mb:.2f} MB)")

    current_file = input_path

    # Temp files
    tmp_clean = input_path + ".tmp_clean"
    tmp_gs = input_path + ".tmp_gs"
    tmp_img0 = input_path + ".tmp_img0"
    tmp_gray = input_path + ".tmp_gray"

    # --- Phase 1: Safe Phase ---
    safe_print("      [Phase 1] å®‰å…¨æ¨¡å¼ (æ¸…ç† + GS + æ— æŸ)...")

    if initially_under_threshold:
        safe_print("      [OK] æºæ–‡ä»¶å·²è¾ƒå° (<100MB)ï¼Œä»…è¿è¡Œå®‰å…¨æ¨¡å¼ã€‚")

    if surgical_clean(input_path, tmp_clean):
        current_file = tmp_clean

    # è·å–é¡µæ•°ä»¥æ˜¾ç¤º GS è¿›åº¦æ¡
    total_pages = 0
    try:
        with pikepdf.open(current_file) as pdf:
            total_pages = len(pdf.pages)
    except:
        pass

    # GS å¯èƒ½ä¼šå¯¼è‡´ä¸­æ–‡å­—ä½“ä¹±ç æˆ–ç»“æ„æŸåï¼Œæš‚æ—¶ç¦ç”¨ GS é‡æ„æ­¥éª¤
    # === ä¿®å¤é€»è¾‘ï¼šå°è¯•è¿è¡Œ GSï¼Œä½†å¦‚æœç»“æœæŸåæˆ–æ–‡å­—ä¹±ç ï¼Œè‡ªåŠ¨å›é€€ ===
    gs_success = run_gs_level0(current_file, tmp_gs, total_pages)
    if gs_success:
        # éªŒè¯ GS ç»“æœæ˜¯å¦æŸå (MuPDF check)
        try:
            # ç»“æ„å®Œæ•´æ€§ + æ–‡å­—å®Œæ•´æ€§æ£€æŸ¥ (åˆå¹¶æ‰“å¼€ä¸¤ä¸ªPDFï¼Œå‡å°‘I/Oå¼€é”€)
            check_doc = fitz.open(tmp_gs)
            orig_doc = fitz.open(current_file)
            gs_text = check_doc[0].get_text("text")
            orig_text = orig_doc[0].get_text("text")
            check_doc.close()
            orig_doc.close()

            keep_gs = True
            if len(orig_text.strip()) > 10:
                # ç®€å•è®¡ç®—å­—ç¬¦é‡åˆåº¦
                set_orig = set(orig_text)
                set_gs = set(gs_text)
                common = set_orig.intersection(set_gs)
                # å¦‚æœé‡åˆå­—ç¬¦æ•°å°‘äºåŸå­—ç¬¦ç§ç±»çš„ 50%ï¼Œè®¤ä¸ºä¹±ç 
                if len(common) < len(set_orig) * 0.5:
                    safe_print("      [WARN] GS æ–‡å­—æŸå (ä¹±ç æ£€æµ‹)ï¼Œå›é€€ GS æ­¥éª¤ã€‚")
                    keep_gs = False
            
            if keep_gs:
                if current_file != input_path:
                    safe_remove(current_file)
                current_file = tmp_gs
            else:
                safe_remove(tmp_gs)

        except:
            safe_print("      [WARN] GS ç»“æœä¼¼ä¹æŸå (MuPDF æ£€æŸ¥å¤±è´¥)ï¼Œå›é€€ GS æ­¥éª¤ã€‚")
            safe_remove(tmp_gs)
            # ä¿æŒ current_file ä¸å˜ (å³è·³è¿‡ GS)
    else:
         safe_remove(tmp_gs)

    if run_image_pass_safe(current_file, tmp_img0, None, "æ— æŸ"):
        if current_file != input_path:
            safe_remove(current_file)
        current_file = tmp_img0

    safe_mb = get_file_mb(current_file)
    safe_print(f"      [DOWN] å®‰å…¨æ¨¡å¼ç»“æœ: {safe_mb:.2f} MB")

    # è§„åˆ™ï¼šå¦‚æœæºæ–‡ä»¶ä¸€å¼€å§‹å°±å°äºé˜ˆå€¼ï¼Œå®‰å…¨é˜¶æ®µç»“æŸåå¿…é¡»ç›´æ¥è¿”å›ï¼Œä¸è¿›å…¥åç»­æœ‰æŸæµç¨‹ã€‚
    if initially_under_threshold:
        safe_print("      [OK] åŸå§‹æ–‡ä»¶å·²ä½äºé˜ˆå€¼ï¼Œå®‰å…¨é˜¶æ®µåç›´æ¥ç»“æŸã€‚")
        if safe_mb < file_mb:
            try:
                shutil.move(current_file, input_path)
                safe_print(
                    f"      [SAVE] è¦†ç›–åŸæ–‡ä»¶: {os.path.basename(input_path)} ({safe_mb:.2f} MB)"
                )
            except:
                pass
        else:
            safe_print("      [SKIP] å®‰å…¨æ¨¡å¼æ— æ”¶ç›Šã€‚")
            if current_file != input_path:
                safe_remove(current_file)

        # Cleanup temps
        safe_remove(tmp_clean)
        safe_remove(tmp_gs)
        safe_remove(tmp_img0)
        safe_remove(tmp_gray)
        return

    if safe_mb < SIZE_THRESHOLD_MB:
        safe_print("      [OK] å®‰å…¨ä¼˜åŒ–å·²è¾¾ç›®æ ‡å¤§å°ï¼Œè·³è¿‡æœ‰æŸé˜¶æ®µã€‚")
        # Finalize and return early (using current_file as result)
        if safe_mb < file_mb:
            try:
                shutil.move(current_file, input_path)
                safe_print(
                    f"      [SAVE] è¦†ç›–åŸæ–‡ä»¶: {os.path.basename(input_path)} ({safe_mb:.2f} MB)"
                )
            except:
                pass
        else:
            safe_print("      [SKIP] å®‰å…¨æ¨¡å¼æ— æ”¶ç›Šã€‚")
            if current_file != input_path:
                safe_remove(current_file)

        # Cleanup temps
        safe_remove(tmp_clean)
        safe_remove(tmp_gs)
        safe_remove(tmp_img0)
        safe_remove(tmp_gray)
        return

    # --- Phase 2: Interleaved Optimization (User Requested) ---
    # å…ˆæ£€æµ‹å•è‰²è£…é¥°é¡µé¢
    safe_print("      [SCAN] åˆ†æé¢œè‰²ç‰¹å¾...")
    has_mono_pages, pages_to_convert, mono_stats = detect_mono_decorative_pages(current_file)
    
    if has_mono_pages and pages_to_convert:
        total_pages_mono = mono_stats.get('total_pages', 0)
        convert_count = len(pages_to_convert)
        dominant_hue = mono_stats.get('dominant_hue', 'unknown')
        
        # ä¼°ç®—æ—¶é—´ (æ ¹æ®GPU/CPUåŒºåˆ†)
        ml_available = check_ml_pipeline_available()
        gpu_name = 'CPU'
        try:
            from ml_enhance import get_gpu_providers
            _, gpu_name = get_gpu_providers()
        except Exception:
            pass
        # GPU (DirectML/CUDA) ~0.5-1åˆ†é’Ÿ/é¡µ, CPU ~3-5åˆ†é’Ÿ/é¡µ
        time_per_page_ml = 1 if gpu_name != 'CPU' else 5
        time_per_page_traditional = 0.1  # åˆ†é’Ÿ
        estimated_ml_time = convert_count * time_per_page_ml
        estimated_traditional_time = convert_count * time_per_page_traditional
        
        safe_print(f"")
        safe_print(f"      â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
        safe_print(f"      â•‘  æ£€æµ‹åˆ° {convert_count}/{total_pages_mono} é¡µå•è‰²è£…é¥°é¡µé¢ (ä¸»è‰²è°ƒ: {dominant_hue})")
        safe_print(f"      â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£")
        safe_print(f"      â•‘  è¿™äº›é¡µé¢å«æœ‰è£…é¥°è‰²ï¼ˆå¦‚è“è‰²è¾¹æ¡†ï¼‰ï¼Œä½†ä¸»ä½“ä¸ºç°åº¦å†…å®¹ã€‚")
        safe_print(f"      â•‘  æ …æ ¼åŒ–åå¯å¯ç”¨äºŒå€¼åŒ–å‹ç¼©ï¼Œæ˜¾è‘—å‡å°æ–‡ä»¶ä½“ç§¯ã€‚")
        safe_print(f"      â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£")
        safe_print(f"      â•‘  å¢å¼ºæ–¹å¼é€‰æ‹©:")
        safe_print(f"      â•‘")
        safe_print(f"      â•‘  [1] MLå¢å¼º (ESRGAN + NAF-DPM + DoxaPy)")
        safe_print(f"      â•‘      - æ•ˆæœæœ€ä½³ï¼šæ–‡å­—é”åˆ©ã€èƒŒæ™¯çº¯å‡€ã€æ¶ˆé™¤æ‰«æå™ªç‚¹")
        # æ˜¾ç¤ºGPUåŠ é€ŸçŠ¶æ€
        if gpu_name != 'CPU':
            safe_print(f"      â•‘      - GPUåŠ é€Ÿ: {gpu_name} (å¤§å¹…æå‡é€Ÿåº¦)")
        else:
            safe_print(f"      â•‘      - ä½¿ç”¨CPUæ¨ç† (è¾ƒæ…¢)")
        safe_print(f"      â•‘      - é¢„è®¡è€—æ—¶: ~{estimated_ml_time} åˆ†é’Ÿ ({convert_count}é¡µ Ã— ~{time_per_page_ml}åˆ†é’Ÿ/é¡µ)")
        safe_print(f"      â•‘")
        safe_print(f"      â•‘  [2] ä¼ ç»Ÿå¢å¼º (é™å™ª + CLAHE + é”åŒ–)")
        safe_print(f"      â•‘      - æ•ˆæœä¸€èˆ¬ï¼šåŸºç¡€å»å™ªå’Œå¯¹æ¯”åº¦è°ƒæ•´")
        safe_print(f"      â•‘      - é¢„è®¡è€—æ—¶: ~{estimated_traditional_time:.1f} åˆ†é’Ÿ")
        safe_print(f"      â•‘")
        safe_print(f"      â•‘  [3] è·³è¿‡å¢å¼º")
        safe_print(f"      â•‘      - ä»…æ …æ ¼åŒ–ä¸ºç°åº¦ï¼Œä¸åšä»»ä½•å¢å¼ºå¤„ç†")
        safe_print(f"      â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
        
        # ç”¨æˆ·äº¤äº’
        try:
            user_input = input("      --> è¯·é€‰æ‹© [1/2/3]ï¼Œç›´æ¥å›è½¦é»˜è®¤é€‰æ‹© 2 (ä¼ ç»Ÿå¢å¼º): ").strip()
            
            if user_input == '1':
                safe_print(f"      [ML] ä½¿ç”¨MLå¢å¼ºç®¡çº¿å¤„ç† {convert_count} é¡µ...")
                if convert_pages_to_grayscale(current_file, tmp_gray, pages_to_convert, enhance=True, use_ml=True):
                    if current_file != input_path:
                        safe_remove(current_file)
                    current_file = tmp_gray
                    safe_print(f"      [OK] MLå¢å¼ºå®Œæˆ: {get_file_mb(current_file):.2f} MB")
                else:
                    safe_print("      [WARN] MLå¢å¼ºè¿‡ç¨‹å‡ºé”™ã€‚")
            elif user_input == '3':
                safe_print(f"      [SKIP] è·³è¿‡å¢å¼ºï¼Œä»…æ …æ ¼åŒ–...")
                if convert_pages_to_grayscale(current_file, tmp_gray, pages_to_convert, enhance=False):
                    if current_file != input_path:
                        safe_remove(current_file)
                    current_file = tmp_gray
                    safe_print(f"      [OK] æ …æ ¼åŒ–å®Œæˆ: {get_file_mb(current_file):.2f} MB")
            else:  # é»˜è®¤é€‰æ‹© 2
                safe_print(f"      [ä¼ ç»Ÿ] ä½¿ç”¨ä¼ ç»Ÿå¢å¼ºå¤„ç† {convert_count} é¡µ...")
                if convert_pages_to_grayscale(current_file, tmp_gray, pages_to_convert, enhance=True, use_ml=False):
                    if current_file != input_path:
                        safe_remove(current_file)
                    current_file = tmp_gray
                    safe_print(f"      [OK] ä¼ ç»Ÿå¢å¼ºå®Œæˆ: {get_file_mb(current_file):.2f} MB")
                else:
                    safe_print("      [WARN] ä¼ ç»Ÿå¢å¼ºè¿‡ç¨‹å‡ºé”™ã€‚")
        except EOFError:
            # éäº¤äº’æ¨¡å¼ï¼Œé»˜è®¤ä½¿ç”¨ä¼ ç»Ÿå¢å¼º
            safe_print("      [AUTO] éäº¤äº’æ¨¡å¼ï¼Œä½¿ç”¨ä¼ ç»Ÿå¢å¼º...")
            if convert_pages_to_grayscale(current_file, tmp_gray, pages_to_convert, enhance=True, use_ml=False):
                if current_file != input_path:
                    safe_remove(current_file)
                current_file = tmp_gray
    else:
        safe_print("      [OK] æœªæ£€æµ‹åˆ°æ˜¾è‘—çš„å•è‰²è£…é¥°é¡µé¢ã€‚")

    # L1 -> I 50 -> T 50 -> L2 -> I 45 -> T 45 ...
    safe_print(
        "      [Phase 2] äº¤é”™å‹ç¼© (çŸ¢é‡ + å›¾ç‰‡ + ç°åº¦åˆ‡ç‰‡)..."
    )

    stages = [
        ("V", (4, False), "çŸ¢é‡ L1", ".tmp_v1"),
        ("I", (50,), "å›¾ç‰‡ 50dB", ".tmp_i50"),
        ("T", (50,), "åˆ‡ç‰‡ 50dB", ".tmp_t50"),
        ("V", (3, False), "çŸ¢é‡ L2", ".tmp_v2"),
        ("I", (45,), "å›¾ç‰‡ 45dB", ".tmp_i45"),
        ("T", (45,), "åˆ‡ç‰‡ 45dB", ".tmp_t45"),
        ("V", (2, False), "çŸ¢é‡ L3", ".tmp_v3"),
        ("I", (40,), "å›¾ç‰‡ 40dB", ".tmp_i40"),
        ("T", (40,), "åˆ‡ç‰‡ 40dB", ".tmp_t40"),
        ("V", (2, True), "çŸ¢é‡ L4", ".tmp_v4"),
        ("I", (35,), "å›¾ç‰‡ 35dB", ".tmp_i35"),
        ("T", (35,), "åˆ‡ç‰‡ 35dB", ".tmp_t35"),
    ]

    lossy_working_file = current_file

    for step_idx, (stype, args, desc, suffix) in enumerate(stages, 1):
        if get_file_mb(lossy_working_file) < SIZE_THRESHOLD_MB:
            break

        safe_print(f"      [STEP] æ­¥éª¤ {step_idx}: {desc} ...")
        next_file = input_path + suffix
        success = False

        if stype == "V":
            success = run_regex_pass(
                lossy_working_file, next_file, args[0], args[1], desc
            )
        elif stype == "I":
            success = run_image_pass_safe(lossy_working_file, next_file, args[0], desc)
        elif stype == "T":
            # Tiling pass
            success = run_tiling_pass(lossy_working_file, next_file, args[0], desc)

        if success:
            new_mb = get_file_mb(next_file)
            if new_mb > 0.01 and new_mb < get_file_mb(lossy_working_file):
                safe_print(f"      [DOWN] æˆåŠŸ: {new_mb:.2f} MB")
                if lossy_working_file != input_path:
                    safe_remove(lossy_working_file)
                lossy_working_file = next_file
            else:
                safe_print("      [SKIP] æ— æ”¶ç›Š")
                safe_remove(next_file)
        else:
            safe_print("      [SKIP] å·²è·³è¿‡")

    # Final Save
    final_mb = get_file_mb(lossy_working_file)
    if final_mb < file_mb:
        root, ext = os.path.splitext(input_path)
        optimized_path = f"{root}_opted{ext}"
        try:
            shutil.move(lossy_working_file, optimized_path)
            safe_print(
                f"      [DONE] å®Œæˆ: {os.path.basename(optimized_path)} ({final_mb:.2f} MB)"
            )
            # åªæœ‰æœ‰æŸå‹ç¼©æ‰æ·»åŠ åˆ°æŠ¥å‘Šåˆ—è¡¨
            lossy_report_list.append((base_name, os.path.basename(optimized_path)))
        except:
            pass
    else:
        safe_print("      [SKIP] æœªä¼˜åŒ–")
        if lossy_working_file != input_path:
            safe_remove(lossy_working_file)

    # Cleanup
    for s in stages:
        safe_remove(input_path + s[3])
    safe_remove(tmp_clean)
    safe_remove(tmp_gs)
    safe_remove(tmp_img0)
    safe_remove(tmp_gray)


def main():
    if sys.platform.startswith("win"):
        try:
            # type: ignore
            sys.stdout.reconfigure(encoding="utf-8")
        except:
            pass
    multiprocessing.freeze_support()
    safe_print("=== Fireworks Hybrid Engine (Interleaved Compression) ===")
    
    # GPU detection for ML acceleration
    try:
        from ml_enhance import get_gpu_providers
        providers, provider_name = get_gpu_providers()
        if provider_name == 'CPU':
            safe_print(f"  [GPU] æœªæ£€æµ‹åˆ°GPUåŠ é€Ÿï¼Œå°†ä½¿ç”¨CPUæ¨ç†")
        else:
            safe_print(f"  [GPU] MLåŠ é€Ÿ: {provider_name} ({providers[0]})")
    except Exception:
        safe_print("  [GPU] MLæ¨¡å—æœªåŠ è½½ï¼ŒGPUæ£€æµ‹è·³è¿‡")

    pdf_files = [
        os.path.join(r, f)
        for r, d, fs in os.walk(".")
        for f in fs
        if f.lower().endswith(".pdf") and "_opted" not in f and ".tmp" not in f
    ]

    for idx, f in enumerate(pdf_files, 1):
        process_file(f, idx, len(pdf_files))

    if lossy_report_list:
        print("\n" + "=" * 50)
        safe_print("[WARN] ä»¥ä¸‹æ–‡ä»¶è§¦å‘äº†æœ‰æŸå‹ç¼© (é˜¶æ®µ 2/3)ï¼Œè¯·åŠ¡å¿…æ£€æŸ¥å†…å®¹å®Œæ•´æ€§ï¼š")
        for orig, opt in lossy_report_list:
            print(f" - {orig} -> {opt}")
        print("=" * 50 + "\n")


if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        safe_print(f"\n[ERROR] è¿è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
    finally:
        input("\næŒ‰å›è½¦é”®é€€å‡º...")
