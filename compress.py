import sys
import os
import re
import io
import math
import shutil
import subprocess
import multiprocessing
import threading
import zlib  # Added for Flate compression
import importlib
from pathlib import Path

# æ·»åŠ å½“å‰ç›®å½•åˆ°è·¯å¾„ï¼Œç¡®ä¿èƒ½å¯¼å…¥æœ¬åœ°æ¨¡å—
SCRIPT_DIR = Path(__file__).parent.resolve()
sys.path.insert(0, str(SCRIPT_DIR))

import pikepdf
import fitz  # PyMuPDF
import numpy as np
import imagecodecs  # GIL-free JPEG2000 encoding (replaces PIL JPEG2000)
import cv2  # OpenCV for image enhancement
from PIL import Image, ImageFilter, ImageEnhance
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed

def _bootstrap_vector_engine():
    """åŠ è½½å¿…éœ€çš„ Cython çŸ¢é‡å¼•æ“ã€‚

    è§„åˆ™ï¼š
    - EXE/å†»ç»“ç¯å¢ƒï¼šç¼ºå¤±å³ç›´æ¥æŠ¥é”™ã€‚
    - Python è„šæœ¬ç¯å¢ƒï¼šè‹¥æœªæ‰¾åˆ°æ‰©å±•ï¼Œåˆ™å°è¯•è‡ªåŠ¨ç¼–è¯‘ä¸€æ¬¡ï¼›ä»å¤±è´¥åˆ™æŠ¥é”™å¹¶ç»™å‡ºæŒ‡å¼•ã€‚
    """
    module_name = "vector_hotspot_cython_nogil"
    try:
        return importlib.import_module(module_name)
    except Exception as first_exc:
        is_frozen = bool(getattr(sys, "frozen", False))
        build_script = SCRIPT_DIR / "build_cython_vector_hotspot.py"

        if (not is_frozen) and build_script.exists():
            try:
                print("[INIT] æœªæ£€æµ‹åˆ° Cython çŸ¢é‡å¼•æ“ï¼Œå°è¯•è‡ªåŠ¨ç¼–è¯‘...", flush=True)
            except Exception:
                pass
            try:
                subprocess.run(
                    [sys.executable, str(build_script), "build_ext", "--inplace"],
                    cwd=str(SCRIPT_DIR),
                    check=True,
                )
                importlib.invalidate_caches()
                return importlib.import_module(module_name)
            except Exception as build_exc:
                raise RuntimeError(
                    "æœªèƒ½è‡ªåŠ¨ç¼–è¯‘å¿…éœ€çš„ Cython çŸ¢é‡å¼•æ“ã€‚"
                    "è¯·å…ˆæ‰§è¡Œï¼šuv sync --frozen && uv run python build_cython_vector_hotspot.py build_ext --inplace"
                ) from build_exc

        raise RuntimeError(
            "æœªèƒ½åŠ è½½å¿…éœ€çš„çŸ¢é‡åŠ é€Ÿå¼•æ“ vector_hotspot_cython_nogilã€‚"
            "è¯·å…ˆç¼–è¯‘ Cython æ‰©å±•å¹¶ç¡®ä¿å…¶è¢«æ­£ç¡®æ‰“åŒ…åˆ°å¯æ‰§è¡Œæ–‡ä»¶ä¸­ã€‚"
        ) from first_exc


# å•ä¸€çŸ¢é‡çƒ­ç‚¹å¼•æ“ï¼ˆå¹¶å‘ä¼˜åŒ–ä¸“ç”¨ï¼‰
_vector_engine = _bootstrap_vector_engine()

# === ML Pipeline å¯ç”¨æ€§æ£€æŸ¥ (ä¸é¢„åŠ è½½æ¨¡å‹) ===
_ml_pipeline_available = None  # None=æœªæ£€æŸ¥, True=å¯ç”¨, False=ä¸å¯ç”¨

def check_ml_pipeline_available():
    """æ£€æŸ¥MLç®¡çº¿æ˜¯å¦å¯ç”¨ï¼ˆä»…æ£€æŸ¥æ¨¡å‹æ–‡ä»¶å’Œä¾èµ–ï¼Œä¸åŠ è½½æ¨¡å‹ï¼‰"""
    global _ml_pipeline_available
    if _ml_pipeline_available is None:
        try:
            models_dir = SCRIPT_DIR / "models"
            required = [
                "slbr-watermark-detector-fp16.onnx",
                "real-esrgan-x4plus-128-fp16.onnx",
                "NAF-DPM_denoiser-fp16.onnx",
                "NAF-DPM_init_predictor-fp16.onnx",
            ]
            missing = [m for m in required if not (models_dir / m).exists()]
            if missing:
                safe_print(f"      [WARN] MLæ¨¡å‹ç¼ºå¤±: {', '.join(missing)}")
                _ml_pipeline_available = False
            else:
                import ml_pipeline  # noqa: F401 â€” verify importable
                _ml_pipeline_available = True
        except Exception as e:
            safe_print(f"      [WARN] MLç®¡çº¿ä¸å¯ç”¨: {e}")
            _ml_pipeline_available = False
    return _ml_pipeline_available

# å…¨å±€é…ç½®
Image.MAX_IMAGE_PIXELS = None  # ç¦ç”¨DecompressionBombæ£€æŸ¥

# === å…¼å®¹æ€§ä¿®å¤ ===
try:
    # type: ignore
    PDF_REDACT_IMAGE_REMOVE = fitz.PDF_REDACT_IMAGE_REMOVE
except AttributeError:
    PDF_REDACT_IMAGE_REMOVE = 2

# === æ ¸å¿ƒé…ç½® (Integrated) ===
# åŸºç¡€é…ç½®
SIZE_THRESHOLD_MB = 100
MIN_IMAGE_SIZE = 2048
CHUNK_SIZE = 20
CURVE_SIMPLIFY_THRESHOLD = 0.20
CPU_CORES = max(1, multiprocessing.cpu_count() - 1)

# JPEG2000 ç¼–ç å¹¶è¡Œç­–ç•¥ (imagecodecs + OpenJPEG)
# numthreads: OpenJPEG å†…éƒ¨ç¼–ç çº¿ç¨‹æ•° (é‡Šæ”¾ GIL)
# JP2K_WORKERS: ThreadPoolExecutor å¹¶å‘ç¼–ç ä»»åŠ¡æ•°
# æœ€ä¼˜ç»„åˆ: workers * numthreads â‰ˆ CPU_CORES, é¿å…è¿‡åº¦è®¢é˜…
JP2K_THREADS = 4  # æ¯å¼ å›¾ç¼–ç ä½¿ç”¨çš„ OpenJPEG å†…éƒ¨çº¿ç¨‹æ•°
JP2K_WORKERS = max(2, CPU_CORES // JP2K_THREADS)  # å¹¶å‘ç¼–ç ä»»åŠ¡æ•°

# åˆ‡ç‰‡/ç°åº¦æ£€æµ‹é…ç½®
GRID_SIZE = 20
GLOBAL_FORCE_MONO_THRESHOLD = 0.98
BLOCK_GRAY_PIXEL_THRESHOLD = 0.15
GRAY_LOWER_BOUND = 50
GRAY_UPPER_BOUND = 220
BINARIZE_THRESHOLD = 180
TILE_GRID_ROWS = 5
TILE_GRID_COLS = 5
TILE_CHECK_GRID = 4
COLOR_STD_THRESHOLD = 5.0  # åˆ¤å®šæ˜¯å¦ä¸ºå½©è‰²çš„é˜ˆå€¼ (è¶Šå°è¶Šå®¹æ˜“è¢«åˆ¤å®šä¸ºå½©è‰²)

# å…¨å±€å˜é‡
lossy_report_list = []
large_file_report_list = []

# === å®‰å…¨æ‰“å°å‡½æ•° (é¿å… Windows GBK ç¼–ç é—®é¢˜) ===
def safe_print(msg):
    """å®‰å…¨æ‰“å°ï¼Œå¤„ç† Windows æ§åˆ¶å°çš„ç¼–ç é—®é¢˜"""
    try:
        print(msg)
    except UnicodeEncodeError:
        # ç§»é™¤æ— æ³•ç¼–ç çš„å­—ç¬¦ (ä¸»è¦æ˜¯ emoji)
        safe_msg = msg.encode('gbk', errors='replace').decode('gbk')
        print(safe_msg)


# ============================
# è¾…åŠ©å‡½æ•°
# ============================
def safe_remove(path):
    if path and os.path.exists(path):
        try:
            os.remove(path)
        except:
            pass


def get_file_mb(path):
    if not os.path.exists(path):
        return 0
    return os.path.getsize(path) / (1024 * 1024)


def _build_image_size_map(pdf_path):
    """æ„å»º xref -> å›¾åƒæµåŸå§‹å¤§å°æ˜ å°„ã€‚"""
    size_map = {}
    try:
        with pikepdf.open(pdf_path) as pdf:
            for i, obj in enumerate(pdf.objects):
                if isinstance(obj, pikepdf.Stream) and obj.get("/Subtype") == "/Image":
                    try:
                        size_map[i] = len(obj.read_raw_bytes())
                    except:
                        pass
    except:
        pass
    return size_map


def _estimate_page_image_payloads(pdf_path):
    """æŒ‰é¡µä¼°ç®—å›¾åƒè½½è·ï¼ˆæ¯é¡µæŒ‰å”¯ä¸€ xref è®¡ä¸€æ¬¡ï¼‰ã€‚"""
    payloads = []
    try:
        size_map = _build_image_size_map(pdf_path)
        doc = fitz.open(pdf_path)
        for i in range(len(doc)):
            xrefs = sorted(set(im[0] for im in doc[i].get_images(full=True)))
            payloads.append(sum(size_map.get(x, 0) for x in xrefs))
        doc.close()
    except:
        return []
    return payloads


def rollback_worse_pages_by_image_payload(prev_pdf, cand_pdf, out_pdf, tolerance_bytes=0, max_rounds=3):
    """å°†å€™é€‰æ–‡ä»¶ä¸­â€œå›¾åƒè½½è·å˜å¤§â€çš„é¡µé¢å›é€€ä¸ºä¸Šä¸€é˜¶æ®µé¡µé¢ï¼ˆè¿­ä»£æ”¶æ•›ç‰ˆï¼‰ã€‚

    è¯´æ˜ï¼š
    - åªä»¥ prev_pdf ä¸ºå”¯ä¸€åŸºå‡†ï¼Œä¸åšç›¸å¯¹åŸå§‹è¾“å…¥é¡µå›é€€ã€‚
    - å› é¡µé¢æ›¿æ¢ä¼šæ”¹å˜èµ„æºå¼•ç”¨å…³ç³»ï¼Œå•è½®å›é€€åå¯èƒ½ä»æœ‰æ®‹ç•™æ”¾å¤§é¡µï¼Œ
      è¿™é‡Œè¿­ä»£æœ€å¤š max_rounds è½®ï¼Œç›´åˆ°æ— æ”¾å¤§é¡µæˆ–è¾¾åˆ°ä¸Šé™ã€‚

    è¿”å›: (ok, total_rolled_back_pages)
    """
    try:
        prev_payloads = _estimate_page_image_payloads(prev_pdf)
        cand_payloads = _estimate_page_image_payloads(cand_pdf)
        if not prev_payloads or not cand_payloads:
            return False, 0
        if len(prev_payloads) != len(cand_payloads):
            return False, 0

        current_input = cand_pdf
        temp_outputs = []
        total_flagged = set()

        for round_idx in range(max(1, max_rounds)):
            cur_payloads = _estimate_page_image_payloads(current_input)
            if not cur_payloads or len(cur_payloads) != len(prev_payloads):
                break

            worse_pages = [
                idx for idx, (p0, p1) in enumerate(zip(prev_payloads, cur_payloads))
                if p1 > p0 + tolerance_bytes
            ]
            if not worse_pages:
                break

            for idx in worse_pages:
                total_flagged.add(idx)

            round_out = f"{out_pdf}.r{round_idx}"
            temp_outputs.append(round_out)

            cand_doc = fitz.open(current_input)
            prev_doc = fitz.open(prev_pdf)
            # å€’åºæ›¿æ¢ï¼Œé¿å…é¡µç æ¼‚ç§»
            for idx in sorted(worse_pages, reverse=True):
                cand_doc.delete_page(idx)
                cand_doc.insert_pdf(prev_doc, from_page=idx, to_page=idx, start_at=idx)
            cand_doc.save(round_out, garbage=4, deflate=True)
            cand_doc.close()
            prev_doc.close()

            current_input = round_out

        final_payloads = _estimate_page_image_payloads(current_input)
        if final_payloads and len(final_payloads) == len(prev_payloads):
            remain_worse = [
                idx for idx, (p0, p1) in enumerate(zip(prev_payloads, final_payloads))
                if p1 > p0 + tolerance_bytes
            ]
            for idx in remain_worse:
                total_flagged.add(idx)

        if not total_flagged:
            for t in temp_outputs:
                safe_remove(t)
            return False, 0

        if current_input != out_pdf:
            shutil.copy2(current_input, out_pdf)

        for t in temp_outputs:
            if t != out_pdf:
                safe_remove(t)

        return is_valid_pdf(out_pdf), len(total_flagged)
    except:
        return False, 0


def _get_page_content_stream_pairs(page_obj):
    """æå–é¡µé¢ /Contents ä¸­çš„æµå¯¹è±¡åºåˆ—ã€‚

    è¿”å›: [(slot, stream_obj), ...]
    - slot: "/Contents" (å•æµ) æˆ–æ•°ç»„ä¸‹æ ‡ (å¤šæµ)
    """
    pairs = []
    try:
        contents = page_obj.get("/Contents", None)
        if isinstance(contents, pikepdf.Stream):
            pairs.append(("/Contents", contents))
        elif isinstance(contents, pikepdf.Array):
            for idx, item in enumerate(contents):
                if isinstance(item, pikepdf.Stream):
                    pairs.append((idx, item))
    except:
        return []
    return pairs


_GS_REF_RE = re.compile(rb"/([^\s/<>\[\]\(\)%]+)\s+gs(?=\s|$)")
_XOBJ_REF_RE = re.compile(rb"/([^\s/<>\[\]\(\)%]+)\s+Do(?=\s|$)")
_CS_REF_RE = re.compile(rb"/([^\s/<>\[\]\(\)%]+)\s+(?:cs|CS)(?=\s|$)")

# èµ„æºç±»å‹ â†’ (regex, PDFèµ„æºå­å­—å…¸é”®å)
_RESOURCE_REF_PATTERNS = [
    (_GS_REF_RE, "/ExtGState"),
    (_XOBJ_REF_RE, "/XObject"),
    (_CS_REF_RE, "/ColorSpace"),
]


def _extract_gs_refs_from_stream(stream_obj):
    """æå–å†…å®¹æµä¸­ä½¿ç”¨çš„ gs èµ„æºåé›†åˆï¼ˆä¸å«å‰å¯¼ /ï¼‰ã€‚"""
    try:
        data = stream_obj.read_bytes()
        return set(m.decode("latin1", "ignore") for m in _GS_REF_RE.findall(data))
    except:
        return set()


def _extract_all_resource_refs(stream_obj):
    """æå–å†…å®¹æµä¸­æ‰€æœ‰èµ„æºå¼•ç”¨ï¼ŒæŒ‰ç±»å‹åˆ†ç»„ã€‚

    è¿”å›: {"/ExtGState": set, "/XObject": set, "/ColorSpace": set}
    """
    result = {}
    try:
        data = stream_obj.read_bytes()
        for regex, res_key in _RESOURCE_REF_PATTERNS:
            names = set(m.decode("latin1", "ignore") for m in regex.findall(data))
            if names:
                result[res_key] = names
    except:
        pass
    return result


def _get_page_extgstate_keys(page_obj):
    """æå–é¡µé¢ /Resources /ExtGState çš„é”®åé›†åˆï¼ˆä¸å«å‰å¯¼ /ï¼‰ã€‚"""
    keys = set()
    try:
        res = page_obj.get("/Resources", None)
        if isinstance(res, pikepdf.Dictionary):
            ext = res.get("/ExtGState", None)
            if isinstance(ext, pikepdf.Dictionary):
                for k in ext.keys():
                    keys.add(str(k).lstrip("/"))
    except:
        return set()
    return keys


def _get_page_resources_dict(page_obj):
    """è·å–é¡µé¢èµ„æºå­—å…¸ï¼Œæ”¯æŒæ²¿ /Parent é“¾ç»§æ‰¿æŸ¥æ‰¾ã€‚"""
    try:
        node = page_obj
        hops = 0
        while node is not None and hops < 32:
            res = node.get("/Resources", None)
            if isinstance(res, pikepdf.Dictionary):
                return res
            node = node.get("/Parent", None)
            hops += 1
    except:
        return None
    return None


def _get_page_extgstate_dict(page_obj):
    """è·å–é¡µé¢ ExtGState èµ„æºå­—å…¸ï¼Œä¸å­˜åœ¨åˆ™è¿”å› Noneã€‚"""
    try:
        res = _get_page_resources_dict(page_obj)
        if isinstance(res, pikepdf.Dictionary):
            ext = res.get("/ExtGState", None)
            if isinstance(ext, pikepdf.Dictionary):
                return ext
    except:
        return None
    return None


def _ensure_page_own_extgstate_dict(page_obj):
    """ç¡®ä¿é¡µé¢æ‹¥æœ‰å¯å†™çš„ /Resources /ExtGStateï¼ˆé¿å…æ”¹åˆ°ç»§æ‰¿å­—å…¸ï¼‰ã€‚"""
    try:
        inherited_res = _get_page_resources_dict(page_obj)

        # 1) èµ„æºå­—å…¸ä¸‹æ²‰åˆ°å½“å‰é¡µ
        if "/Resources" in page_obj and isinstance(page_obj.get("/Resources"), pikepdf.Dictionary):
            page_res = page_obj.get("/Resources")
        else:
            if isinstance(inherited_res, pikepdf.Dictionary):
                page_res = pikepdf.Dictionary(inherited_res)
            else:
                page_res = pikepdf.Dictionary()
            page_obj["/Resources"] = page_res

        # 2) ExtGState ä¸‹æ²‰åˆ°å½“å‰é¡µèµ„æº
        ext = page_res.get("/ExtGState", None)
        if isinstance(ext, pikepdf.Dictionary):
            own_ext = pikepdf.Dictionary(ext)
            page_res["/ExtGState"] = own_ext
            return own_ext

        own_ext = pikepdf.Dictionary()
        page_res["/ExtGState"] = own_ext
        return own_ext
    except:
        return None


def _inject_prev_extgstate_aliases(prev_page, cand_page, missing_refs, cand_pdf):
    """æŠŠ prev é¡µé¢ç¼ºå¤±çš„ ExtGState åç§°æ³¨å…¥åˆ° cand é¡µé¢èµ„æºï¼Œä½œä¸ºåç§°æ˜ å°„åˆ«åã€‚"""
    if not missing_refs:
        return True
    try:
        prev_ext = _get_page_extgstate_dict(prev_page)
        cand_ext = _ensure_page_own_extgstate_dict(cand_page)
        if not isinstance(prev_ext, pikepdf.Dictionary) or not isinstance(cand_ext, pikepdf.Dictionary):
            return False

        for r in sorted(missing_refs):
            key = pikepdf.Name("/" + r)
            prev_obj = prev_ext.get(key, None)
            if prev_obj is None:
                prev_obj = prev_ext.get("/" + r, None)
            if prev_obj is None:
                return False
            cand_ext[key] = cand_pdf.copy_foreign(prev_obj)

        return True
    except:
        return False


def _inject_missing_resources(prev_page, cand_page, refs_by_type, cand_pdf):
    """é€šç”¨èµ„æºæ³¨å…¥ï¼šå°† prev é¡µé¢ä¸­ç¼ºå¤±çš„èµ„æºæ¡ç›®æ³¨å…¥ cand é¡µé¢ã€‚

    refs_by_type: {"/XObject": {"Im0","Im1"}, "/ColorSpace": {"CS2"}, ...}
    ä»…æ³¨å…¥ cand é¡µé¢èµ„æºå­—å…¸ä¸­ç¡®å®ç¼ºå¤±çš„æ¡ç›®ã€‚
    è¿”å› True å¦‚æœæ‰€æœ‰ç¼ºå¤±æ¡ç›®å‡æˆåŠŸæ³¨å…¥ã€‚
    """
    if not refs_by_type:
        return True
    try:
        prev_res = _get_page_resources_dict(prev_page)
        if not isinstance(prev_res, pikepdf.Dictionary):
            return False

        # ç¡®ä¿ cand é¡µé¢æœ‰è‡ªå·±çš„å¯å†™èµ„æºå­—å…¸
        if "/Resources" in cand_page and isinstance(cand_page.get("/Resources"), pikepdf.Dictionary):
            cand_res = cand_page.get("/Resources")
        else:
            inherited = _get_page_resources_dict(cand_page)
            cand_res = pikepdf.Dictionary(inherited) if isinstance(inherited, pikepdf.Dictionary) else pikepdf.Dictionary()
            cand_page["/Resources"] = cand_res

        for res_key, names in refs_by_type.items():
            prev_sub = prev_res.get(res_key, None)
            if not isinstance(prev_sub, pikepdf.Dictionary):
                return False

            cand_sub = cand_res.get(res_key, None)
            if not isinstance(cand_sub, pikepdf.Dictionary):
                cand_sub = pikepdf.Dictionary()
                cand_res[res_key] = cand_sub

            for name in sorted(names):
                key = pikepdf.Name("/" + name)
                if key in cand_sub:
                    continue  # å·²å­˜åœ¨ï¼Œæ— éœ€æ³¨å…¥
                prev_obj = prev_sub.get(key, None)
                if prev_obj is None:
                    return False
                cand_sub[key] = cand_pdf.copy_foreign(prev_obj)

        return True
    except:
        return False


def rollback_worse_content_streams(
    prev_pdf,
    cand_pdf,
    out_pdf,
    tolerance_bytes=64,
    safe_resource_check=True,
):
    """æµçº§å†…å®¹å›é€€ï¼šä»…å›é€€ç›¸å¯¹ä¸Šä¸€é˜¶æ®µâ€œæ˜æ˜¾å˜å¤§â€çš„å†…å®¹æµã€‚

    è®¾è®¡è¦ç‚¹ï¼š
    - ä»…æ¯”è¾ƒ/å›é€€æ¯é¡µ /Contents ä¸­æŒ‰é¡ºåºå¯¹åº”çš„æµå¯¹è±¡ï¼Œä¸åšé¡µçº§æ›¿æ¢ã€‚
    - ä½¿ç”¨ tolerance_bytes å®¹å¿ç¼–ç æŠ–åŠ¨ï¼ˆä¾‹å¦‚ +1B è¿™ç±»æ— æ„ä¹‰å·®å¼‚ï¼‰ã€‚

        è¿”å›: (ok, rolled_stream_count, affected_page_count)

        safe_resource_check:
        - True: å›é€€å‰æ ¡éªŒ prev æµä¸­å¼•ç”¨çš„èµ„æºï¼ˆExtGState/XObject/ColorSpaceï¼‰
            åœ¨ cand é¡µé¢çš„ /Resources ä¸­å…¨éƒ¨å¯è§£æï¼›ç¼ºå¤±æ—¶ä» prev é¡µæ³¨å…¥ï¼›
            æ³¨å…¥å¤±è´¥åˆ™è·³è¿‡è¯¥æµã€‚
        - False: ä¸åšè¯¥æ ¡éªŒï¼ˆä¸å®‰å…¨æ¨¡å¼ï¼Œä»…ç”¨äºå¯¹ç…§å®éªŒï¼‰ã€‚
    """
    try:
        rolled_streams = 0
        affected_pages = set()

        with pikepdf.open(prev_pdf) as prev, pikepdf.open(cand_pdf) as cand:
            if len(prev.pages) != len(cand.pages):
                return False, 0, 0

            for page_idx in range(len(cand.pages)):
                prev_pairs = _get_page_content_stream_pairs(prev.pages[page_idx])
                cand_pairs = _get_page_content_stream_pairs(cand.pages[page_idx])
                if not prev_pairs or not cand_pairs:
                    continue

                # ä»…æŒ‰å…±åŒå¯æ˜ å°„çš„æµæ•°é‡æ¯”è¾ƒï¼Œé¿å…ç»“æ„å·®å¼‚å¯¼è‡´è¯¯æ›¿æ¢ã€‚
                compare_n = min(len(prev_pairs), len(cand_pairs))
                for k in range(compare_n):
                    _, prev_stream = prev_pairs[k]
                    cand_slot, cand_stream = cand_pairs[k]
                    try:
                        prev_raw_len = len(prev_stream.read_raw_bytes())
                        cand_raw_len = len(cand_stream.read_raw_bytes())
                    except:
                        continue

                    if cand_raw_len > prev_raw_len + tolerance_bytes:
                        if safe_resource_check:
                            try:
                                cand_page = cand.pages[page_idx]
                                all_refs = _extract_all_resource_refs(prev_stream)
                                if all_refs:
                                    # æ£€æŸ¥æ‰€æœ‰èµ„æºç±»å‹ï¼Œæ”¶é›†ç¼ºå¤±é¡¹
                                    cand_res = _get_page_resources_dict(cand_page)
                                    missing_by_type = {}
                                    for res_key, names in all_refs.items():
                                        cand_sub = cand_res.get(res_key, None) if isinstance(cand_res, pikepdf.Dictionary) else None
                                        existing = set()
                                        if isinstance(cand_sub, pikepdf.Dictionary):
                                            existing = set(str(k).lstrip("/") for k in cand_sub.keys())
                                        missing = names - existing
                                        if missing:
                                            missing_by_type[res_key] = missing

                                    if missing_by_type:
                                        ok = _inject_missing_resources(
                                            prev.pages[page_idx], cand_page, missing_by_type, cand
                                        )
                                        if not ok:
                                            continue

                                        # æ³¨å…¥åäºŒæ¬¡ç¡®è®¤æ‰€æœ‰å¼•ç”¨å¯è§£æ
                                        cand_res2 = _get_page_resources_dict(cand_page)
                                        for res_key, names in all_refs.items():
                                            cand_sub2 = cand_res2.get(res_key, None) if isinstance(cand_res2, pikepdf.Dictionary) else None
                                            existing2 = set(str(k).lstrip("/") for k in cand_sub2.keys()) if isinstance(cand_sub2, pikepdf.Dictionary) else set()
                                            if not names.issubset(existing2):
                                                ok = False
                                                break
                                        if not ok:
                                            continue
                            except:
                                continue
                        try:
                            replaced_stream = cand.copy_foreign(prev_stream)
                            cand_page = cand.pages[page_idx]
                            if cand_slot == "/Contents":
                                cand_page["/Contents"] = replaced_stream
                            else:
                                arr = cand_page.get("/Contents", None)
                                if isinstance(arr, pikepdf.Array) and 0 <= cand_slot < len(arr):
                                    arr[cand_slot] = replaced_stream
                                else:
                                    continue
                            rolled_streams += 1
                            affected_pages.add(page_idx)
                        except:
                            continue

            if rolled_streams <= 0:
                return False, 0, 0

            cand.save(
                out_pdf,
                compress_streams=True,
                object_stream_mode=pikepdf.ObjectStreamMode.generate,
            )

        return is_valid_pdf(out_pdf), rolled_streams, len(affected_pages)
    except:
        return False, 0, 0


def is_valid_pdf(path):
    if not os.path.exists(path):
        return False
    if os.path.getsize(path) < 1024:
        return False
    return True


# ============================
# å•è‰²è£…é¥°æ¨¡å¼æ£€æµ‹ (é€é¡µåˆ¤å®š)
# ============================
def analyze_page_color_profile(page, gray_tolerance=10, gray_threshold=85, color_threshold=15):
    """
    åˆ†æå•é¡µçš„é¢œè‰²ç‰¹å¾
    è¿”å›: (is_mono_decorative, stats_dict)
    - is_mono_decorative: è¯¥é¡µæ˜¯å¦ä¸ºå•è‰²è£…é¥°é¡µé¢ï¼ˆé€‚åˆç°åº¦åŒ–ï¼‰
    """
    # ä½åˆ†è¾¨ç‡é‡‡æ ·
    pix = page.get_pixmap(matrix=fitz.Matrix(0.25, 0.25))
    img = Image.frombytes('RGB', [pix.width, pix.height], pix.samples)
    
    # NumPyå‘é‡åŒ–åˆ†æï¼ˆæ›¿ä»£é€åƒç´ Pythonå¾ªç¯ï¼Œæé€Ÿ50-100xï¼‰
    arr = np.array(img)  # (H, W, 3) uint8
    total = arr.shape[0] * arr.shape[1]
    
    if total == 0:
        return False, {}
    
    r, g, b = arr[:,:,0].ravel(), arr[:,:,1].ravel(), arr[:,:,2].ravel()
    
    # ç°åº¦åˆ¤å®š: max(|R-G|, |G-B|, |R-B|) <= gray_tolerance
    diff_rg = np.abs(r.astype(np.int16) - g.astype(np.int16))
    diff_gb = np.abs(g.astype(np.int16) - b.astype(np.int16))
    diff_rb = np.abs(r.astype(np.int16) - b.astype(np.int16))
    max_diff = np.maximum(np.maximum(diff_rg, diff_gb), diff_rb)
    
    gray_count = int(np.count_nonzero(max_diff <= gray_tolerance))
    
    gray_ratio = gray_count / total * 100
    color_ratio = 100 - gray_ratio
    
    # å½©è‰²åƒç´ è‰²è°ƒç»Ÿè®¡
    color_mask = max_diff > gray_tolerance
    color_count = total - gray_count
    
    if color_count > 0:
        cr, cg, cb = r[color_mask], g[color_mask], b[color_mask]
        hue_counts = {
            'blue': int(np.count_nonzero((cb > cr) & (cb > cg))),
            'pink/red': int(np.count_nonzero((cr > cg) & (cr > cb))),
            'green': int(np.count_nonzero((cg > cr) & (cg > cb))),
        }
        hue_counts['other'] = color_count - sum(hue_counts.values())
        dominant_hue = max(hue_counts.items(), key=lambda x: x[1])
        hue_concentration = dominant_hue[1] / color_count * 100
    else:
        dominant_hue = ('none', 0)
        hue_concentration = 100
    
    stats = {
        'gray_ratio': gray_ratio,
        'color_ratio': color_ratio,
        'dominant_hue': dominant_hue[0],
        'hue_concentration': hue_concentration
    }
    
    # å•è‰²è£…é¥°åˆ¤å®šï¼š
    # 1. ç°åº¦å æ¯” > 85%
    # 2. å½©è‰²å æ¯” < 15%
    # 3. å½©è‰²éƒ¨åˆ†çš„ä¸»è‰²è°ƒé›†ä¸­åº¦ > 60% (è¯´æ˜é¢œè‰²å•ä¸€)
    is_mono_decorative = (
        gray_ratio > gray_threshold and 
        color_ratio < color_threshold and 
        hue_concentration > 60
    )
    
    return is_mono_decorative, stats


def detect_mono_decorative_pages(pdf_path, sample_ratio=0.1, min_samples=15, max_samples=50):
    """
    æ£€æµ‹PDFä¸­çš„å•è‰²è£…é¥°é¡µé¢
    
    è¿”å›: (has_mono_pages, pages_to_convert, stats)
    - has_mono_pages: æ˜¯å¦å­˜åœ¨å¤§é‡å•è‰²è£…é¥°é¡µé¢ (>50%)
    - pages_to_convert: éœ€è¦ç°åº¦åŒ–çš„é¡µé¢ç´¢å¼•åˆ—è¡¨
    - stats: ç»Ÿè®¡ä¿¡æ¯
    """
    from collections import Counter
    import random
    
    try:
        doc = fitz.open(pdf_path)
        total_pages = len(doc)
        
        if total_pages == 0:
            doc.close()
            return False, [], {}
        
        # ç¡®å®šé‡‡æ ·æ•°é‡
        sample_count = max(min_samples, min(max_samples, int(total_pages * sample_ratio)))
        sample_count = min(sample_count, total_pages)
        
        # éšæœºé‡‡æ ·é¡µé¢ç´¢å¼•
        if sample_count >= total_pages:
            sample_indices = list(range(total_pages))
        else:
            sample_indices = sorted(random.sample(range(total_pages), sample_count))
        
        mono_pages_in_sample = []
        hue_counter = Counter()
        
        for page_num in sample_indices:
            page = doc[page_num]
            is_mono, stats = analyze_page_color_profile(page)
            
            if is_mono:
                mono_pages_in_sample.append(page_num)
            
            if stats.get('dominant_hue'):
                hue_counter[stats['dominant_hue']] += 1
        
        # è®¡ç®—å•è‰²è£…é¥°é¡µé¢æ¯”ä¾‹
        mono_ratio = len(mono_pages_in_sample) / len(sample_indices) * 100
        
        # å¦‚æœé‡‡æ ·ä¸­è¶…è¿‡50%æ˜¯å•è‰²è£…é¥°ï¼Œåˆ™å…¨é‡æ‰«æ
        pages_to_convert = []
        
        if mono_ratio > 50:
            # å…¨é‡æ‰«ææ‰€æœ‰é¡µé¢
            safe_print(f"      [SCAN] æ£€æµ‹åˆ°å•è‰²è£…é¥°æ¨¡å¼ (é‡‡æ ·ä¸­ {mono_ratio:.1f}% ç¬¦åˆ)ï¼Œæ­£åœ¨å…¨é‡æ‰«æ...")
            
            for page_num in tqdm(range(total_pages), desc="      Analyzing", leave=False):
                page = doc[page_num]
                is_mono, _ = analyze_page_color_profile(page)
                if is_mono:
                    pages_to_convert.append(page_num)
        
        doc.close()
        
        dominant_hue = hue_counter.most_common(1)[0][0] if hue_counter else 'none'
        
        summary_stats = {
            'total_pages': total_pages,
            'sample_count': len(sample_indices),
            'mono_in_sample': len(mono_pages_in_sample),
            'mono_ratio_sample': mono_ratio,
            'pages_to_convert': len(pages_to_convert),
            'dominant_hue': dominant_hue
        }
        
        has_significant_mono = len(pages_to_convert) > total_pages * 0.3  # è¶…è¿‡30%é¡µé¢éœ€è¦è½¬æ¢
        
        return has_significant_mono, pages_to_convert, summary_stats
        
    except Exception as e:
        return False, [], {'error': str(e)}


def enhance_document_image(img_array, mode='standard'):
    """
    æ–‡æ¡£å›¾åƒå¢å¼º - ä¿å®ˆä¸”å®‰å…¨çš„å¢å¼ºæ–¹å¼
    
    é’ˆå¯¹é—®é¢˜:
    1. æ•´ä½“æ¨¡ç³Š - é€‚åº¦é”åŒ–
    2. èƒŒæ™¯ç°è„ - æ¸©å’Œçš„å¯¹æ¯”åº¦è°ƒæ•´
    3. æ–‡å­—è¾¹ç¼˜æ¨¡ç³Š - è‡ªé€‚åº”é”åŒ–
    
    å¤„ç†æµç¨‹:
    1. è½»åº¦é™å™ª - ä¿ç•™ç»†èŠ‚
    2. å¯¹æ¯”åº¦æ‹‰ä¼¸ - æ‰©å±•åŠ¨æ€èŒƒå›´
    3. è‡ªé€‚åº”é”åŒ– - å¢å¼ºæ–‡å­—è¾¹ç¼˜
    4. äº®åº¦å¾®è°ƒ - è®©èƒŒæ™¯ç¨å¾®å˜ç™½
    
    Args:
        img_array: numpy array (ç°åº¦å›¾åƒ, uint8)
        mode: 'standard' (æ ‡å‡†), 'strong' (å¼ºåŠ›), 'mild' (æ¸©å’Œ)
    
    Returns:
        å¢å¼ºåçš„ numpy array
    """
    result = img_array.copy()
    
    # æ ¹æ®æ¨¡å¼è°ƒæ•´å‚æ•°
    if mode == 'strong':
        denoise_h = 5
        sharpen_amount = 0.4
        contrast_alpha = 1.15
        brightness_target = 245
    elif mode == 'mild':
        denoise_h = 3
        sharpen_amount = 0.2
        contrast_alpha = 1.05
        brightness_target = 240
    else:  # standard
        denoise_h = 4
        sharpen_amount = 0.3
        contrast_alpha = 1.1
        brightness_target = 242
    
    # ========================================
    # 1. è½»åº¦é™å™ª (ä¿ç•™è¾¹ç¼˜)
    # ========================================
    # ä½¿ç”¨è¾ƒå°çš„ h å€¼ï¼Œåªå»é™¤è½»å¾®å™ªç‚¹
    result = cv2.fastNlMeansDenoising(result, None, h=denoise_h, templateWindowSize=7, searchWindowSize=21)
    
    # ========================================
    # 2. å¯¹æ¯”åº¦æ‹‰ä¼¸ (Contrast Stretching)
    # ========================================
    # å°†åƒç´ å€¼æ‹‰ä¼¸åˆ°æ›´å®½çš„èŒƒå›´ï¼Œä½†ä¸è¦å¤ªæ¿€è¿›
    min_val = np.percentile(result, 2)   # 2% åˆ†ä½æ•°
    max_val = np.percentile(result, 98)  # 98% åˆ†ä½æ•°
    
    if max_val > min_val + 20:  # ç¡®ä¿æœ‰è¶³å¤Ÿçš„åŠ¨æ€èŒƒå›´
        # çº¿æ€§æ‹‰ä¼¸åˆ° 5-250 èŒƒå›´ï¼ˆç•™ä¸€ç‚¹ä½™é‡ï¼‰
        result = np.clip((result - min_val) * 245.0 / (max_val - min_val) + 5, 0, 255).astype(np.uint8)
    
    # ========================================
    # 3. æ¸©å’Œçš„å¯¹æ¯”åº¦å¢å¼º
    # ========================================
    # ä½¿ç”¨ CLAHEï¼Œä½†å‚æ•°æ›´ä¿å®ˆ
    clahe = cv2.createCLAHE(clipLimit=1.5, tileGridSize=(16, 16))
    result = clahe.apply(result)
    
    # ========================================
    # 4. è‡ªé€‚åº”é”åŒ– (Unsharp Masking)
    # ========================================
    # å…¬å¼ï¼šsharpened = original + amount * (original - blurred)
    # ä½¿ç”¨è¾ƒå°çš„ sigma ä¿ç•™æ›´å¤šç»†èŠ‚
    blurred = cv2.GaussianBlur(result, (0, 0), 1.5)
    result = cv2.addWeighted(result, 1 + sharpen_amount, blurred, -sharpen_amount, 0)
    
    # ç¬¬äºŒæ¬¡é”åŒ– - ä½¿ç”¨ PIL çš„ UnsharpMaskï¼Œæ›´æ¸©å’Œ
    pil_img = Image.fromarray(result)
    pil_img = pil_img.filter(ImageFilter.UnsharpMask(radius=1, percent=80, threshold=3))
    result = np.array(pil_img)
    
    # ========================================
    # 5. äº®åº¦å¾®è°ƒ - è®©èƒŒæ™¯ç¨å¾®å˜ç™½
    # ========================================
    # è®¡ç®—å½“å‰èƒŒæ™¯äº®åº¦ï¼ˆå–è¾ƒäº®åŒºåŸŸçš„å‡å€¼ï¼‰
    bright_pixels = result[result > np.percentile(result, 70)]
    if len(bright_pixels) > 0:
        current_bg = np.mean(bright_pixels)
        
        # å¦‚æœèƒŒæ™¯ä¸å¤Ÿç™½ï¼Œé€‚å½“æäº®
        if current_bg < brightness_target:
            # ä½¿ç”¨ gamma æ ¡æ­£æäº®èƒŒæ™¯
            gamma = 0.95  # è½»å¾®æäº®
            inv_gamma = 1.0 / gamma
            table = np.array([((i / 255.0) ** inv_gamma) * 255 for i in range(256)]).astype(np.uint8)
            result = cv2.LUT(result, table)
    
    # ========================================
    # 6. æœ€ç»ˆå¯¹æ¯”åº¦å¾®è°ƒ
    # ========================================
    result = cv2.convertScaleAbs(result, alpha=contrast_alpha, beta=0)
    
    # ç¡®ä¿è¾“å‡ºåœ¨æœ‰æ•ˆèŒƒå›´
    result = np.clip(result, 0, 255).astype(np.uint8)
    
    return result


def convert_pages_to_grayscale(input_path, output_path, page_indices, dpi=150, enhance=True, use_ml=False):
    """
    å°†æŒ‡å®šé¡µé¢æ•´é¡µæ …æ ¼åŒ–ä¸ºç°åº¦å›¾ç‰‡å¹¶æ›¿æ¢åŸé¡µé¢
    è¿™ä¼šç§»é™¤é¡µé¢ä¸Šçš„æ‰€æœ‰æ–‡å­—ã€çŸ¢é‡å›¾å½¢ï¼Œåªä¿ç•™å•å¼ ç°åº¦å›¾ç‰‡
    ç›®çš„æ˜¯è®©è¿™äº›é¡µé¢èƒ½è¿›å…¥åç»­çš„äºŒå€¼åŒ–å¤„ç†æµç¨‹
    
    Args:
        input_path: è¾“å…¥PDFè·¯å¾„
        output_path: è¾“å‡ºPDFè·¯å¾„
        page_indices: éœ€è¦ç°åº¦åŒ–çš„é¡µé¢ç´¢å¼•åˆ—è¡¨ (0-based)
        dpi: æ …æ ¼åŒ–åˆ†è¾¨ç‡ (é»˜è®¤150ï¼Œå¹³è¡¡è´¨é‡å’Œæ–‡ä»¶å¤§å°)
        enhance: æ˜¯å¦åº”ç”¨å›¾åƒå¢å¼º
        use_ml: æ˜¯å¦ä½¿ç”¨MLå¢å¼º (True=MLå¢å¼º, False=ä¼ ç»Ÿå¢å¼º)
    """
    if not page_indices:
        return False
    
    try:
        # ä½¿ç”¨ fitz (PyMuPDF) è¿›è¡Œé¡µé¢æ …æ ¼åŒ–
        doc = fitz.open(input_path)
        converted_count = 0
        total_pages = len(page_indices)
        
        page_set = set(page_indices)
        zoom = dpi / 72.0
        mat = fitz.Matrix(zoom, zoom)
        
        # === ML Pipeline Path ===
        # ä½¿ç”¨5é˜¶æ®µæµæ°´çº¿: Render -> SLBR(CPU) -> ESRGAN(GPU) -> NAF-DPM(GPU) -> DoxaPy(CPU)
        if enhance and use_ml:
            try:
                from ml_pipeline import PipelinedMLProcessor
                
                # ä¸ä¼  models_dirï¼Œä½¿ç”¨é»˜è®¤å€¼ (SCRIPT_DIR / "models")
                # è¿™æ ·åœ¨ EXE ä¸­ä¼šè‡ªåŠ¨æ‰¾åˆ° EXE åŒçº§çš„ models ç›®å½•
                processor = PipelinedMLProcessor(
                    nafdpm_batch_size=32,
                    esrgan_overlap=8
                )
                
                # è‡ªé€‚åº”æµ‹è¯•: è·å–æœ€ä¼˜ batch_size (é¡µæ•° > 20 æ—¶)
                if len(page_indices) > 20:
                    try:
                        from adaptive_config import get_optimal_config_for_pdf
                        batch_size, esrgan_overlap = get_optimal_config_for_pdf(
                            str(input_path), processor, force_benchmark=True)
                        processor.nafdpm_batch_size = batch_size
                        processor.esrgan_overlap = esrgan_overlap
                        print(f"[è‡ªé€‚åº”] æœ€ä¼˜ batch_size={batch_size}, overlap={esrgan_overlap}")
                    except Exception as e:
                        print(f"[è‡ªé€‚åº”] æµ‹è¯•å¤±è´¥: {e}")
                
                results = processor.process_document(
                    str(input_path), page_indices, dpi=dpi)
                
                # å°†ç°åº¦ç»“æœå†™å…¥PDF
                for page_num in page_indices:
                    if page_num not in results:
                        continue
                    img_array = results[page_num]
                    page = doc[page_num]
                    h_img, w_img = img_array.shape[:2]
                    pgm_header = f"P5\n{w_img} {h_img}\n255\n".encode('ascii')
                    img_data = pgm_header + np.ascontiguousarray(img_array).tobytes()
                    page.clean_contents()
                    page_rect = page.rect
                    page.add_redact_annot(page_rect)
                    page.apply_redactions(images=fitz.PDF_REDACT_IMAGE_REMOVE)
                    page.insert_image(page_rect, stream=img_data)
                    converted_count += 1
                del results
                
            except Exception as e:
                import traceback
                safe_print(f"      [WARN] MLç®¡çº¿å¤±è´¥: {type(e).__name__}: {e}")
                safe_print(traceback.format_exc())
                safe_print("      [WARN] å›é€€åˆ°ä¼ ç»Ÿå¢å¼º")
                use_ml = False  # å›é€€æ ‡è®°ï¼Œè¿›å…¥ä¸‹æ–¹ä¼ ç»Ÿè·¯å¾„
        
        # === ä¼ ç»Ÿå¢å¼º / æ— å¢å¼ºè·¯å¾„ (ä¹Ÿç”¨äºMLå›é€€) ===
        if not (enhance and use_ml):
            desc = "      ä¼ ç»Ÿå¢å¼º" if enhance else "      æ …æ ¼åŒ–"
            pbar = tqdm(page_indices, desc=desc,
                        bar_format='{desc}: {percentage:3.0f}%|{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}] {postfix}')
            for page_num in pbar:
                if page_num >= len(doc):
                    continue
                page = doc[page_num]
                pix = page.get_pixmap(matrix=mat, colorspace=fitz.csGRAY)
                img_array = np.frombuffer(pix.samples, dtype=np.uint8).reshape(pix.height, pix.width)
                if enhance:
                    img_array = enhance_document_image(img_array)
                h_img, w_img = img_array.shape[:2]
                pgm_header = f"P5\n{w_img} {h_img}\n255\n".encode('ascii')
                img_data = pgm_header + np.ascontiguousarray(img_array).tobytes()
                page.clean_contents()
                page_rect = page.rect
                page.add_redact_annot(page_rect)
                page.apply_redactions(images=fitz.PDF_REDACT_IMAGE_REMOVE)
                page.insert_image(page_rect, stream=img_data)
                converted_count += 1
            pbar.close()
        
        if converted_count > 0:
            doc.save(output_path, garbage=4, deflate=True)
            doc.close()
            return is_valid_pdf(output_path)
        else:
            doc.close()
            return False
        
    except Exception as e:
        try:
            print(f"      [WARN] ç°åº¦è½¬æ¢å¤±è´¥: {e}")
        except UnicodeEncodeError:
            print(f"      [WARN] Grayscale conversion failed: {e}")
        return False


# ============================
# ç°åº¦/è‰²å½© ä¸¥æ ¼æ£€æµ‹é€»è¾‘
# ============================
def is_block_gray(block):
    if block.size == 0:
        return False
    mid_mask = (block > GRAY_LOWER_BOUND) & (block < GRAY_UPPER_BOUND)
    return (np.count_nonzero(mid_mask) / block.size) > BLOCK_GRAY_PIXEL_THRESHOLD


def detect_strict_color(pil_img, grid_size=20, threshold=5.0):
    """
    ä¸¥æ ¼è‰²å½©æ£€æµ‹ï¼šå°†å›¾ç‰‡åˆ‡åˆ†ä¸º grid_size x grid_size ç½‘æ ¼
    åªè¦æœ‰ä¸€ä¸ªç½‘æ ¼çš„ RGB é€šé“æ ‡å‡†å·®å‡å€¼è¶…è¿‡é˜ˆå€¼ï¼Œå³åˆ¤å®šä¸ºå½©è‰²
    """
    if pil_img.mode not in ["RGB", "CMYK"]:
        return False  # å·²ç»æ˜¯ç°åº¦æˆ–äºŒå€¼

    # è½¬æ¢ä¸ºRGB numpyæ•°ç»„
    arr = np.array(pil_img.convert("RGB"))
    h, w, _ = arr.shape

    # 1. å…¨å±€å¿«é€Ÿæ£€æµ‹ (é¿å…æ˜æ˜¾å½©è‰²å›¾æµªè´¹æ—¶é—´åˆ‡ç½‘æ ¼)
    # è®¡ç®—å›¾åƒä¸­å¿ƒçš„ 100x100 åŒºåŸŸ
    cy, cx = h // 2, w // 2
    center_sample = arr[
        max(0, cy - 50) : min(h, cy + 50), max(0, cx - 50) : min(w, cx + 50)
    ]
    if center_sample.size > 0:
        if np.mean(np.std(center_sample, axis=2)) > (threshold * 2):
            return True

    # 2. ç½‘æ ¼æ£€æµ‹
    h_step = max(h // grid_size, 1)
    w_step = max(w // grid_size, 1)

    for y in range(0, h, h_step):
        for x in range(0, w, w_step):
            # æå– Block
            block = arr[y : y + h_step, x : x + w_step]
            if block.size == 0:
                continue

            # è®¡ç®—è¯¥ Block çš„è‰²å½©é¥±å’Œåº¦ (RGBé€šé“é—´çš„æ ‡å‡†å·®)
            # å¯¹äºçº¯ç°åº¦ï¼ŒR=G=Bï¼Œstd=0
            # å…è®¸å°‘é‡å™ªç‚¹ (threshold)
            block_sat = np.mean(np.std(block, axis=2))

            if block_sat > threshold:
                return True  # å‘ç°å½©è‰²åŒºå—ï¼Œåˆ¤å®šä¸ºå½©è‰²

    return False  # æœªå‘ç°å½©è‰²åŒºå—ï¼Œåˆ¤å®šä¸ºç°åº¦


def detect_mono_or_hybrid(pil_img):
    gray = pil_img.convert("L")
    arr = np.array(gray)
    h, w = arr.shape

    mid_mask_global = (arr > 30) & (arr < 225)
    if arr.size == 0:
        return "MONO", arr
    gray_ratio = np.count_nonzero(mid_mask_global) / arr.size

    if gray_ratio < (1.0 - GLOBAL_FORCE_MONO_THRESHOLD):
        return "MONO", arr

    h_step, w_step = max(h // GRID_SIZE, 1), max(w // GRID_SIZE, 1)
    has_gray = False
    for y in range(0, h, h_step):
        for x in range(0, w, w_step):
            if is_block_gray(arr[y : y + h_step, x : x + w_step]):
                has_gray = True
                break
        if has_gray:
            break
    return ("HYBRID" if has_gray else "MONO"), arr


def detect_tile_mode(pil_img, grid_size=4):
    gray = pil_img.convert("L")
    arr = np.array(gray)
    h, w = arr.shape

    mid_mask_global = (arr > 30) & (arr < 225)
    if arr.size == 0:
        return "MONO", arr
    gray_ratio = np.count_nonzero(mid_mask_global) / arr.size

    if gray_ratio < (1.0 - GLOBAL_FORCE_MONO_THRESHOLD):
        return "MONO", arr

    h_step, w_step = max(h // grid_size, 1), max(w // grid_size, 1)
    has_gray = False
    for y in range(0, h, h_step):
        for x in range(0, w, w_step):
            if is_block_gray(arr[y : y + h_step, x : x + w_step]):
                has_gray = True
                break
        if has_gray:
            break
    return ("HYBRID" if has_gray else "MONO"), arr


def is_tile_color(pil_img):
    """æ£€æµ‹åˆ‡ç‰‡æ˜¯å¦å½©è‰² (å¤ç”¨ strict color é€»è¾‘ï¼Œä½†é’ˆå¯¹å°å›¾ä¼˜åŒ–)"""
    return detect_strict_color(pil_img, grid_size=2, threshold=5.0)


def extract_raw_jpx(data):
    """ä» JP2 å®¹å™¨ä¸­æå–åŸå§‹ Codestream (å¦‚æœå­˜åœ¨)"""
    # JP2 Signature: 00 00 00 0C 6A 50 20 20 0D 0A 87 0A
    if data.startswith(b"\x00\x00\x00\x0cjP  \r\n\x87\n"):
        # It's a JP2 file, iterate boxes to find 'jp2c'
        pos = 0
        while pos < len(data) - 8:
            box_len = int.from_bytes(data[pos : pos + 4], "big")
            box_type = data[pos + 4 : pos + 8]

            if box_len == 0:
                # 0 means box extends to end of file
                if box_type == b"jp2c":
                    # éªŒè¯æ˜¯å¦ä¸º Codestream Start (SOC): FF 4F
                    if data[pos + 8 : pos + 10] == b"\xff\x4f":
                        return data[pos + 8 :]
                break

            if box_len == 1:
                # Large box (64-bit length), skip for simplicity or handle if needed
                # JP2 usually doesn't use this for small images
                pass

            if box_type == b"jp2c":
                if data[pos + 8 : pos + 10] == b"\xff\x4f":
                    return data[pos + 8 : pos + box_len]

            pos += box_len

    # å¦‚æœä¸æ˜¯ JP2 å®¹å™¨ï¼Œæˆ–è€…æ²¡æ‰¾åˆ° jp2cï¼Œæˆ–è€…å·²ç»æ˜¯ Raw (FF 4F)
    if data.startswith(b"\xff\x4f"):
        return data

    # å¦‚æœæ— æ³•æå–ï¼Œè¿”å›åŸå§‹æ•°æ® (MuPDF æœ‰æ—¶ä¹Ÿèƒ½å®¹å¿å®Œæ•´çš„ JP2 æ•°æ®)
    return data


# ============================
# Tiling Processor Class
# ============================
class TileProcessor:
    def __init__(self, page, xref, pil_img, page_num=0, quality_db=50):
        self.page = page
        self.xref = xref
        self.pil_img = pil_img
        self.page_num = page_num
        self.quality_db = quality_db  # æ¥æ”¶è´¨é‡å‚æ•°

    def process_and_replace(self):
        stats = {"mono": 0, "hybrid": 0}
        w, h = self.pil_img.size
        h_step = h / TILE_GRID_ROWS
        w_step = w / TILE_GRID_COLS

        rects = self.page.get_image_rects(self.xref)
        if not rects:
            return False, stats
        target_rect = rects[0]

        new_tiles = []
        total_new_size = 0

        # 1. Grid Analysis
        grid_map = []
        for r in range(TILE_GRID_ROWS):
            row_data = []
            for c in range(TILE_GRID_COLS):
                y0, x0 = int(r * h_step), int(c * w_step)
                y1 = int((r + 1) * h_step) if r < TILE_GRID_ROWS - 1 else h
                x1 = int((c + 1) * w_step) if c < TILE_GRID_COLS - 1 else w

                if x1 <= x0 or y1 <= y0:
                    row_data.append(None)
                    continue
                tile_img = self.pil_img.crop((x0, y0, x1, y1))
                tile_mode, _ = detect_tile_mode(tile_img)
                row_data.append(
                    {"mode": tile_mode, "img": tile_img, "rect": (x0, y0, x1, y1)}
                )
            grid_map.append(row_data)

        # 2. Merge Logic
        visited = [
            [False for _ in range(TILE_GRID_COLS)] for _ in range(TILE_GRID_ROWS)
        ]

        for r in range(TILE_GRID_ROWS):
            for c in range(TILE_GRID_COLS):
                if visited[r][c] or not grid_map[r][c]:
                    continue

                start_node = grid_map[r][c]
                current_mode = start_node["mode"]

                # Expand Right
                max_w = 0
                for k in range(c, TILE_GRID_COLS):
                    node = grid_map[r][k]
                    if not visited[r][k] and node and node["mode"] == current_mode:
                        max_w += 1
                    else:
                        break

                # Expand Down
                max_h = 1
                for k_r in range(r + 1, TILE_GRID_ROWS):
                    row_match = True
                    for k_c in range(c, c + max_w):
                        node = grid_map[k_r][k_c]
                        if (
                            visited[k_r][k_c]
                            or not node
                            or node["mode"] != current_mode
                        ):
                            row_match = False
                            break
                    if row_match:
                        max_h += 1
                    else:
                        break

                # Mark Visited
                for i in range(max_h):
                    for j in range(max_w):
                        visited[r + i][c + j] = True

                # Create Tile
                first_tile = grid_map[r][c]
                last_tile = grid_map[r + max_h - 1][c + max_w - 1]
                x0_big, y0_big = first_tile["rect"][0], first_tile["rect"][1]
                x1_big, y1_big = last_tile["rect"][2], last_tile["rect"][3]

                merged_img = self.pil_img.crop((x0_big, y0_big, x1_big, y1_big))

                stream_data = None
                if current_mode == "MONO":
                    stats["mono"] += 1
                    # type: ignore
                    img_bin = merged_img.convert("L").point(
                        lambda x: 0 if x < BINARIZE_THRESHOLD else 255, "1"
                    )
                    buf = io.BytesIO()
                    img_bin.save(buf, format="PNG", icc_profile=None)
                    stream_data = buf.getvalue()
                else:
                    stats["hybrid"] += 1
                    # imagecodecs: GIL-free JPEG2000 ç¼–ç ï¼Œç›´æ¥è¾“å‡º J2K codestream
                    if is_tile_color(merged_img):
                        _tile_arr = np.asarray(merged_img.convert("RGB"))
                    else:
                        _tile_arr = np.asarray(merged_img.convert("L"))
                    stream_data = imagecodecs.jpeg2k_encode(
                        _tile_arr, level=self.quality_db, codecformat='j2k',
                        numthreads=JP2K_THREADS,
                    )

                total_new_size += len(stream_data)

                # Calculate PDF Coords
                scale_x = target_rect.width / w
                scale_y = target_rect.height / h
                off_x = x0_big * scale_x
                off_y = y0_big * scale_y
                tile_w_pdf = (x1_big - x0_big) * scale_x
                tile_h_pdf = (y1_big - y0_big) * scale_y

                tile_rect = fitz.Rect(
                    target_rect.x0 + off_x,
                    target_rect.y0 + off_y,
                    target_rect.x0 + off_x + tile_w_pdf,
                    target_rect.y0 + off_y + tile_h_pdf,
                )
                new_tiles.append((tile_rect, stream_data))

        # Check Size Inflation
        orig_len = len(self.page.parent.xref_stream(self.xref))
        if total_new_size >= orig_len:
            return False, stats

        # Apply
        self.page.clean_contents()
        self.page.add_redact_annot(target_rect)
        self.page.apply_redactions(images=PDF_REDACT_IMAGE_REMOVE)
        for t_rect, t_data in new_tiles:
            self.page.insert_image(t_rect, stream=t_data, overlay=True)

        return True, stats


# ============================
# Phase 1/2: Core Functions
# ============================
def surgical_clean(input_path, output_path):
    print("      ğŸ“‹ æ­£åœ¨æ‰§è¡Œæ¸…ç†...")
    try:
        with pikepdf.open(input_path) as pdf:
            if "/PieceInfo" in pdf.Root:
                del pdf.Root["/PieceInfo"]  # type: ignore
            if "/Metadata" in pdf.Root:
                del pdf.Root["/Metadata"]  # type: ignore
            for page in pdf.pages:
                if "/PieceInfo" in page:
                    del page["/PieceInfo"]  # type: ignore
                if "/Thumb" in page:
                    del page["/Thumb"]  # type: ignore
            pdf.remove_unreferenced_resources()
            pdf.save(
                output_path,
                compress_streams=True,
                object_stream_mode=pikepdf.ObjectStreamMode.generate,
            )
        return is_valid_pdf(output_path)
    except:
        return False


def get_gs_path():
    base_dir = os.path.dirname(os.path.abspath(__file__))
    local_gs = os.path.join(base_dir, "gs", "bin", "gswin64c.exe")
    if os.path.exists(local_gs):
        return local_gs
    if shutil.which("gswin64c"):
        return "gswin64c"
    if shutil.which("gs"):
        return "gs"
    return None


def run_gs_level0(input_path, output_path, total_pages=0):
    gs_exe = get_gs_path()
    if not gs_exe:
        return False
    cmd = [
        gs_exe,
        "-sDEVICE=pdfwrite",
        "-dCompatibilityLevel=1.4",
        "-dNOPAUSE",
        "-dBATCH",
        "-dSAFER",
        f"-sOutputFile={output_path}",
        "-dPDFSETTINGS=/prepress",
        "-dDownsampleColorImages=false",
        "-dDownsampleGrayImages=false",
        "-dDownsampleMonoImages=false",
        "-dAutoFilterColorImages=false",
        "-dAutoFilterGrayImages=false",
        "-dColorImageFilter=/FlateEncode",
        "-dGrayImageFilter=/FlateEncode",
        input_path,
    ]
    try:
        process = subprocess.Popen(
            cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True
        )
        pbar = None  # å»¶è¿Ÿåˆå§‹åŒ–è¿›åº¦æ¡ï¼Œä»¥ä¾¿æ˜¾ç¤ºåŸå§‹è¾“å‡º

        if process.stdout:
            for line in process.stdout:
                # æ£€æŸ¥æ˜¯å¦å¼€å§‹å¤„ç†é¡µé¢ (GSè¾“å‡ºé€šå¸¸åŒ…å« "Page X")
                if "Page" in line:
                    if pbar is None and total_pages > 0:
                        # é¦–æ¬¡æ£€æµ‹åˆ°é¡µé¢å¤„ç†ï¼Œåˆå§‹åŒ–è¿›åº¦æ¡
                        pbar = tqdm(
                            total=total_pages,
                            desc="      ğŸ›¡ï¸ GS é‡æ„",
                            unit="page",
                        )

                    if pbar:
                        try:
                            pbar.update(1)
                        except:
                            pass
                else:
                    # åœ¨è¿›åº¦æ¡å‡ºç°å‰ï¼Œæ˜¾ç¤º GS çš„åŸå§‹è¾“å‡º (å¦‚ Loading font...)
                    if pbar is None:
                        # ä½¿ç”¨ \r è¦†ç›–å½“å‰è¡Œï¼Œé¿å…åˆ·å±
                        print(f"      [GS] {line.strip()}", end="\r")

        process.wait()
        if pbar:
            pbar.close()
        return process.returncode == 0 and is_valid_pdf(output_path)
    except:
        return False


# Regex Utils
NUM_RE = rb"[-+]?(?:\d*\.\d+|\d+)"
L_PATTERN = re.compile(rb"(" + NUM_RE + rb")(\s+)(" + NUM_RE + rb")(\s+)(l)(?=\s|$)")
C_PATTERN = re.compile(
    rb"("
    + NUM_RE
    + rb")(\s+)("
    + NUM_RE
    + rb")(\s+)"
    + rb"("
    + NUM_RE
    + rb")(\s+)("
    + NUM_RE
    + rb")(\s+)"
    + rb"("
    + NUM_RE
    + rb")(\s+)("
    + NUM_RE
    + rb")(\s+)(c)(?=\s|$)"
)
VY_PATTERN = re.compile(
    rb"("
    + NUM_RE
    + rb")(\s+)("
    + NUM_RE
    + rb")(\s+)"
    + rb"("
    + NUM_RE
    + rb")(\s+)("
    + NUM_RE
    + rb")(\s+)([vy])(?=\s|$)"
)
M_PATTERN = re.compile(rb"(" + NUM_RE + rb")(\s+)(" + NUM_RE + rb")(\s+)(m)(?=\s|$)")
W_PATTERN = re.compile(rb"(" + NUM_RE + rb")(\s+)(w)(?=\s|$)")

# çŸ¢é‡æµåˆ†å—é˜ˆå€¼ä¸å—å¤§å°ï¼ˆåå‘â€œè¿›åº¦å¹³æ»‘â€ï¼‰
# - è¶…è¿‡ 1MB å³è¿›å…¥åˆ†å—å¤„ç†
# - æ¯å—çº¦ 256KBï¼Œæå‡è¿›åº¦æ¡åˆ·æ–°é¢‘ç‡
VECTOR_SPLIT_THRESHOLD_BYTES = 1 * 1024 * 1024
VECTOR_CHUNK_TARGET_BYTES = 256 * 1024
VECTOR_INNER_WORKERS = max(1, int(os.environ.get("VECTOR_INNER_WORKERS", "1")))
REGEX_STREAM_WORKERS = max(1, int(os.environ.get("REGEX_STREAM_WORKERS", "4")))


def format_number(bytes_val, sig_figs):
    try:
        # å®‰å…¨æ£€æŸ¥ï¼šè¿‡çŸ­çš„æ•°å­—ä¸å¤„ç†
        if len(bytes_val) < 3:
            return bytes_val
        val = float(bytes_val)
        # å…³é”®ä¿®å¤ï¼šPDF æ•°å€¼è¯­æ³•ä¸æ¥å—ç§‘å­¦è®¡æ•°æ³•ï¼ˆå¦‚ 2e+04ï¼‰ã€‚
        # è¿™é‡Œå¼ºåˆ¶ä½¿ç”¨åè¿›åˆ¶å®šç‚¹è¡¨ç¤ºï¼Œå†å»æ‰æœ«å°¾æ— æ•ˆ 0ã€‚
        if val.is_integer():
            new_str = "{:.0f}".format(val)
        else:
            # sig_figs ä½œä¸ºå°æ•°ä½ä¸Šé™ä½¿ç”¨ï¼Œé¿å…è¾“å‡ºæŒ‡æ•°å½¢å¼
            new_str = f"{val:.{sig_figs}f}".rstrip("0").rstrip(".")
            # å…œåº•ï¼šè‹¥è¢«æ ¼å¼åŒ–ä¸ºç©ºï¼Œå›é€€ä¸º 0
            if not new_str:
                new_str = "0"

        if new_str.startswith("0."):
            new_str = new_str[1:]
        elif new_str.startswith("-0."):
            new_str = "-" + new_str[2:]
        elif new_str == "-0":
            new_str = "0"
        new_bytes = new_str.encode("ascii")
        if len(new_bytes) < len(bytes_val):
            return new_bytes
        return bytes_val
    except:
        return bytes_val


def replace_2args(m, sig_figs):
    """å¤„ç† l, m ç­‰ 2 å‚æ•°å‘½ä»¤"""
    return (
        format_number(m.group(1), sig_figs)
        + m.group(2)
        + format_number(m.group(3), sig_figs)
        + m.group(4)
        + m.group(5)
    )


def replace_w(m, sig_figs):
    """å¤„ç† w (çº¿å®½) å‘½ä»¤"""
    return format_number(m.group(1), sig_figs) + m.group(2) + m.group(3)


def replace_vy(m, sig_figs):
    """å¤„ç† v, y æ›²çº¿å‘½ä»¤ (4 å‚æ•°)"""
    n1 = format_number(m.group(1), sig_figs)
    n2 = format_number(m.group(3), sig_figs)
    n3 = format_number(m.group(5), sig_figs)
    n4 = format_number(m.group(7), sig_figs)
    op = m.group(9)
    return (
        n1
        + m.group(2)
        + n2
        + m.group(4)
        + n3
        + m.group(6)
        + n4
        + m.group(8)
        + op
    )


def replace_c_v4(m, sig_figs):
    """å¤„ç† c æ›²çº¿å‘½ä»¤ (6 å‚æ•°)"""
    nums = [format_number(m.group(i), sig_figs) for i in range(1, 12, 2)]
    spaces = [m.group(i) for i in range(2, 13, 2)]
    return b"".join(n + s for n, s in zip(nums, spaces)) + m.group(13)


def replace_c_smart(m, sig_figs):
    """æ™ºèƒ½æ›²çº¿ç®€åŒ–ï¼šå¦‚æœæ›²çº¿è¶³å¤Ÿå°ï¼Œå¯ä»¥æ›¿æ¢ä¸ºç›´çº¿"""
    try:
        x1, y1 = float(m.group(1)), float(m.group(3))
        x2, y2 = float(m.group(5)), float(m.group(7))
        x3, y3 = float(m.group(9)), float(m.group(11))
        span_x = max(x1, x2, x3) - min(x1, x2, x3)
        span_y = max(y1, y2, y3) - min(y1, y2, y3)
        if span_x < CURVE_SIMPLIFY_THRESHOLD and span_y < CURVE_SIMPLIFY_THRESHOLD:
            n3 = format_number(m.group(9), sig_figs)
            n4 = format_number(m.group(11), sig_figs)
            return n3 + b" " + n4 + b" l"
    except:
        pass
    return replace_c_v4(m, sig_figs)


def _apply_vector_regex_passes(raw_data, sig_figs, enable_smart_c):
    """å¯¹ä¸€æ®µå†…å®¹æµæ‰§è¡Œæ—¢æœ‰çŸ¢é‡æ­£åˆ™ä¼˜åŒ–ã€‚"""
    d = L_PATTERN.sub(lambda m: replace_2args(m, sig_figs), raw_data)
    d = VY_PATTERN.sub(lambda m: replace_vy(m, sig_figs), d)
    d = M_PATTERN.sub(lambda m: replace_2args(m, sig_figs), d)
    d = W_PATTERN.sub(lambda m: replace_w(m, sig_figs), d)
    if enable_smart_c:
        d = C_PATTERN.sub(lambda m: replace_c_smart(m, sig_figs), d)
    else:
        d = C_PATTERN.sub(lambda m: replace_c_v4(m, sig_figs), d)
    return d


def _optimize_vector_stream(raw_data, sig_figs, enable_smart_c):
    """å•å¼•æ“æ‰§è¡Œï¼šCython å¼•æ“ä¸ºå¿…éœ€é¡¹ï¼Œä¸å…è®¸å›é€€ã€‚"""
    if not hasattr(_vector_engine, "optimize_stream_scan_nogil"):
        raise RuntimeError(
            "vector_hotspot_cython_nogil ç¼ºå°‘ optimize_stream_scan_nogilï¼Œ"
            "è¯·æ£€æŸ¥æ‰©å±•ç¼–è¯‘/æ‰“åŒ…æµç¨‹ã€‚"
        )
    return _vector_engine.optimize_stream_scan_nogil(
        raw_data, sig_figs, enable_smart_c, CURVE_SIMPLIFY_THRESHOLD
    )


def _split_and_optimize_large_stream(raw_data, sig_figs, enable_smart_c, progress_callback=None):
    """è¶…å¤§æµåˆ†å—ä¼˜åŒ–ï¼šæŒ‰è¡Œèšåˆæˆçº¦ 1MB å­å—ï¼Œé€å—å¤„ç†å¹¶æ‹¼å›ã€‚"""
    if len(raw_data) <= VECTOR_SPLIT_THRESHOLD_BYTES:
        out = _optimize_vector_stream(raw_data, sig_figs, enable_smart_c)
        if progress_callback:
            progress_callback(len(raw_data))
        return out

    lines = raw_data.splitlines(keepends=True)
    # è‹¥å‡ ä¹æ²¡æœ‰æ¢è¡Œï¼ŒæŒ‰ç©ºç™½è¾¹ç•Œåˆ‡å—ï¼ˆé¿å…é”™è¯¯åˆ‡æ–­ tokenï¼‰
    if len(lines) <= 1:
        chunks = []
        n = len(raw_data)
        pos = 0
        while pos < n:
            end = min(pos + VECTOR_CHUNK_TARGET_BYTES, n)
            if end < n:
                split = end
                while split > pos and raw_data[split - 1] not in b" \t\r\n":
                    split -= 1
                if split == pos:
                    split = end
                end = split
            chunks.append(raw_data[pos:end])
            pos = end

        out_parts = [b""] * len(chunks)
        max_workers = min(VECTOR_INNER_WORKERS, len(chunks))
        if max_workers <= 1:
            for idx, ch in enumerate(chunks):
                out_parts[idx] = _optimize_vector_stream(ch, sig_figs, enable_smart_c)
                if progress_callback:
                    progress_callback(len(chunks[idx]))
        else:
            with ThreadPoolExecutor(max_workers=max_workers) as ex:
                futures = {
                    ex.submit(_optimize_vector_stream, ch, sig_figs, enable_smart_c): idx
                    for idx, ch in enumerate(chunks)
                }
                for f in as_completed(futures):
                    idx = futures[f]
                    out_parts[idx] = f.result()
                    if progress_callback:
                        progress_callback(len(chunks[idx]))
        return b"".join(out_parts)

    chunks = []
    buf = []
    buf_size = 0
    for ln in lines:
        if buf_size + len(ln) > VECTOR_CHUNK_TARGET_BYTES and buf:
            chunks.append(b"".join(buf))
            buf = [ln]
            buf_size = len(ln)
        else:
            buf.append(ln)
            buf_size += len(ln)
    if buf:
        chunks.append(b"".join(buf))

    out_parts = [b""] * len(chunks)
    max_workers = min(VECTOR_INNER_WORKERS, len(chunks))
    if max_workers <= 1:
        for idx, ch in enumerate(chunks):
            out_parts[idx] = _optimize_vector_stream(ch, sig_figs, enable_smart_c)
            if progress_callback:
                progress_callback(len(chunks[idx]))
    else:
        with ThreadPoolExecutor(max_workers=max_workers) as ex:
            futures = {
                ex.submit(_optimize_vector_stream, ch, sig_figs, enable_smart_c): idx
                for idx, ch in enumerate(chunks)
            }
            for f in as_completed(futures):
                idx = futures[f]
                out_parts[idx] = f.result()
                if progress_callback:
                    progress_callback(len(chunks[idx]))

    return b"".join(out_parts)


def run_regex_pass(input_path, output_path, sig_figs, enable_smart_c, desc):
    try:
        inline_img_re = re.compile(rb"(^|\\s)BI(\\s|$)")

        # Phase 1: å•æ¬¡ openï¼Œè¯»å–å…¨éƒ¨å€™é€‰æµåˆ°å†…å­˜
        candidate_raw = {}  # xref -> raw_bytes
        with pikepdf.open(input_path) as pdf:
            # è‡ªé€‚åº” sig_figsï¼šè¶…å¤§é¡µé¢éœ€è¦æ›´é«˜ç²¾åº¦ä»¥ä¿æŠ¤ç»†å¾®çŸ¢é‡ç‰¹å¾
            _BASE_DIM = 612.0  # æ ‡å‡† A4 å®½åº¦ (pt)
            max_page_dim = 0.0
            for page in pdf.pages:
                mb = page.get("/MediaBox")
                if mb:
                    try:
                        w = abs(float(mb[2]) - float(mb[0]))
                        h = abs(float(mb[3]) - float(mb[1]))
                        max_page_dim = max(max_page_dim, w, h)
                    except Exception:
                        pass
            if max_page_dim > _BASE_DIM:
                import math as _m
                extra_sf = round(_m.log10(max_page_dim / _BASE_DIM))
                if extra_sf > 0:
                    sig_figs = sig_figs + extra_sf
                    safe_print(f"      [è‡ªé€‚åº”] é¡µé¢æœ€å¤§å°ºå¯¸ {max_page_dim:.0f}pt > æ ‡å‡† {_BASE_DIM:.0f}pt, sig_figs {sig_figs - extra_sf}â†’{sig_figs}")

            for i, obj in enumerate(pdf.objects):
                if isinstance(obj, pikepdf.Stream):
                    subtype = str(obj.get("/Subtype") or "")
                    if "/Image" not in subtype and "/Font" not in subtype:
                        try:
                            raw = obj.read_bytes()
                            if not inline_img_re.search(raw):
                                candidate_raw[i] = raw
                        except:
                            pass

        if not candidate_raw:
            return False

        total_bytes = sum(len(v) for v in candidate_raw.values())
        total_xrefs = len(candidate_raw)

        # Phase 2: å¤šçº¿ç¨‹å¹¶è¡Œ Cython å¤„ç†ï¼ˆçº¯å†…å­˜ï¼Œä¸å†æ‰“å¼€ PDFï¼‰
        all_results = {}
        done_bytes = [0]
        done_streams = [0]
        pbar_lock = threading.Lock()
        stream_workers = min(max(1, REGEX_STREAM_WORKERS), total_xrefs)

        pbar = tqdm(
            total=max(total_bytes, 1),
            desc=f"      ğŸ”¥ {desc}",
            unit="B",
            unit_scale=True,
            unit_divisor=1024,
        )

        def on_bytes_done(nbytes):
            with pbar_lock:
                remaining = max(total_bytes - done_bytes[0], 0)
                inc = min(max(int(nbytes), 0), remaining)
                if inc > 0:
                    pbar.update(inc)
                    done_bytes[0] += inc

        def _process_one(xref_raw):
            xref, raw_data = xref_raw
            d = _split_and_optimize_large_stream(
                raw_data, sig_figs, enable_smart_c, progress_callback=on_bytes_done
            )
            if len(d) < len(raw_data):
                return (xref, d)
            return None

        with ThreadPoolExecutor(max_workers=stream_workers) as executor:
            futures = {
                executor.submit(_process_one, item): item[0]
                for item in candidate_raw.items()
            }
            for f in as_completed(futures):
                res = f.result()
                done_streams[0] += 1
                if res:
                    all_results[res[0]] = res[1]
                pbar.set_postfix(streams=f"{done_streams[0]}/{total_xrefs}")

        if done_bytes[0] < total_bytes:
            pbar.update(total_bytes - done_bytes[0])
        pbar.close()

        # é‡Šæ”¾åŸå§‹æ•°æ®å†…å­˜
        candidate_raw.clear()

        if not all_results:
            return False

        # Phase 3: å•æ¬¡ openï¼Œå†™å›ä¼˜åŒ–ç»“æœ
        with pikepdf.open(input_path) as pdf:
            for xref, data in all_results.items():
                pdf.objects[xref].write(data)
            pdf.remove_unreferenced_resources()
            pdf.save(
                output_path,
                compress_streams=True,
                object_stream_mode=pikepdf.ObjectStreamMode.generate,
            )
        return is_valid_pdf(output_path)
    except:
        return False



# ============================
# Phase 2: Safe Image Pass (Modified)
# ============================
def _compress_single_xref(pdf, xref, target_quality, mixed_page_xrefs):
    """å•ä¸ªxrefçš„å‹ç¼©é€»è¾‘ï¼ˆæ¥å—å·²æ‰“å¼€çš„pdfå¯¹è±¡ï¼Œé¿å…é‡å¤æ‰“å¼€PDFï¼‰"""
    image_obj = pdf.objects[xref]
    # Safety Checks - ç¡¬ Mask ä»ç„¶è·³è¿‡
    if "/Mask" in image_obj:
        return None
    # SMask: æ£€æµ‹å…¨ 255ï¼ˆå®Œå…¨ä¸é€æ˜ï¼‰çš„ trivial SMaskï¼Œå¯å®‰å…¨å¿½ç•¥å¹¶ç»§ç»­å¤„ç†
    if "/SMask" in image_obj:
        try:
            smask_obj = image_obj["/SMask"]
            smask_data = bytes(smask_obj.read_bytes())
            if smask_data != b'\xff' * len(smask_data):
                return None  # é trivial SMaskï¼Œè·³è¿‡
        except Exception:
            return None

    try:
        pdfimage = pikepdf.PdfImage(image_obj)
        pil_image = pdfimage.as_pil_image()

        # å®‰å…¨æ£€æŸ¥ï¼šå¦‚æœå›¾ç‰‡è¿‡å¤§ï¼Œå¯èƒ½å¯¼è‡´ MemoryErrorï¼Œç›´æ¥è·³è¿‡
        # 100M åƒç´ ä»¥ä¸Š (e.g. 10000x10000)
        if pil_image.width * pil_image.height > 100_000_000:
            return None

    except:
        # jbig2dec missing or other extraction error -> skip image
        return None

    # 1. ä¸¥æ ¼ç°åº¦æ£€æµ‹
    is_color = detect_strict_color(
        pil_image, grid_size=GRID_SIZE, threshold=COLOR_STD_THRESHOLD
    )

    # é€»è¾‘ä¿®æ­£ï¼š
    # å®‰å…¨å›¾ç‰‡å‹ç¼©é˜¶æ®µåº”å¤„ç†ï¼š
    # 1. æ‰€æœ‰å½©è‰²å›¾ç‰‡ (is_color = True)
    # 2. ä½äº "æ··åˆé¡µé¢" (æœ‰æ–‡å­—/çŸ¢é‡) ä¸Šçš„ç°åº¦å›¾ç‰‡ (is_color = False and xref in mixed_page_xrefs)
    #
    # å®‰å…¨å›¾ç‰‡å‹ç¼©é˜¶æ®µåº”è·³è¿‡ï¼š
    # 1. ä½äº "çº¯å›¾ç‰‡é¡µé¢" ä¸Šçš„ç°åº¦å›¾ç‰‡ (äº¤ç»™åˆ‡ç‰‡/äºŒå€¼åŒ–é˜¶æ®µ)

    if not is_color:
        if xref not in mixed_page_xrefs:
            # è¿™æ˜¯ä¸€ä¸ªç°åº¦å›¾ï¼Œä¸”ä¸åœ¨æ··åˆé¡µé¢ä¸Š -> è·³è¿‡ï¼Œç•™ç»™åˆ‡ç‰‡/äºŒå€¼åŒ–é˜¶æ®µ
            return None

        # å¦åˆ™ï¼šè™½ç„¶æ˜¯ç°åº¦ï¼Œä½†åœ¨æ··åˆé¡µé¢ä¸Šï¼Œè¿›è¡Œå®‰å…¨å‹ç¼©
        # === æ–°å¢é€»è¾‘ï¼šæ··åˆé¡µé¢ä¸­çš„å°å›¾ï¼Œå¦‚æœæ»¡è¶³äºŒå€¼åŒ–æ¡ä»¶ï¼Œç›´æ¥äºŒå€¼åŒ– ===
        mode, _ = detect_mono_or_hybrid(pil_image)
        if mode == "MONO" and target_quality is not None:
            # äºŒå€¼åŒ–å¤„ç†
            # type: ignore
            img_bin = pil_image.convert("L").point(
                lambda x: 0 if x < BINARIZE_THRESHOLD else 255, "1"
            )

            # ä½¿ç”¨ Flate (Zlib) å‹ç¼©åŸå§‹ 1-bit æ•°æ®
            # PIL tobytes("raw") å¯¹äº mode "1" è¿”å› packed bits (æ¯è¡Œ byte å¯¹é½)ï¼Œç¬¦åˆ PDF è¦æ±‚
            raw_bits = img_bin.tobytes()
            new_data = zlib.compress(raw_bits, level=9)

            if len(new_data) >= len(image_obj.read_raw_bytes()):
                return None

            return {
                "xref": xref,
                "data": new_data,
                "width": img_bin.width,
                "height": img_bin.height,
                "mode": "1",  # æ ‡è®°ä¸ºäºŒå€¼å›¾
            }
        # å¦‚æœæ˜¯ Hybrid ç°åº¦å›¾ï¼Œç»§ç»­ä¸‹æ–¹çš„ JP2 æµç¨‹

    # æ­£å¸¸å¤„ç† (Color æˆ– Mixed-Hybrid-Gray)
    if pil_image.mode not in ["RGB", "L"]:
        pil_image = pil_image.convert("RGB")

    try:
        img_arr = np.asarray(pil_image)
        if target_quality is not None:
            new_data = imagecodecs.jpeg2k_encode(img_arr, level=target_quality, numthreads=JP2K_THREADS)
        else:
            new_data = imagecodecs.jpeg2k_encode(img_arr, level=0, numthreads=JP2K_THREADS)
    except Exception:
        # Fallback for PIL/OpenJPEG encoding errors (broken data stream)
        # Try saving as PNG then return original (skip compression) or retry logic
        return None

    if len(new_data) >= len(image_obj.read_raw_bytes()):
        return None

    return {
        "xref": xref,
        "data": new_data,
        "width": pil_image.width,
        "height": pil_image.height,
        "mode": pil_image.mode,
    }


def compress_image_worker(xref, target_quality, pdf_path, mixed_page_xrefs):
    """å•å›¾å…¼å®¹æ¥å£ï¼šæ¯æ¬¡è°ƒç”¨æ‰“å¼€PDFï¼ˆä¿ç•™å‘åå…¼å®¹ï¼‰"""
    with pikepdf.open(pdf_path) as pdf:
        return _compress_single_xref(pdf, xref, target_quality, mixed_page_xrefs)


def compress_image_chunk_worker(xref_chunk, target_quality, pdf_path, mixed_page_xrefs, progress_callback=None):
    """æ‰¹é‡å¤„ç†ä¸€ç»„xrefï¼Œåªæ‰“å¼€ä¸€æ¬¡PDFï¼ˆå‡å°‘96%çš„PDFè§£æå¼€é”€ï¼‰"""
    results = []
    try:
        with pikepdf.open(pdf_path) as pdf:
            for xref in xref_chunk:
                res = None
                try:
                    res = _compress_single_xref(pdf, xref, target_quality, mixed_page_xrefs)
                    if res is not None:
                        results.append(res)
                except Exception:
                    pass
                if progress_callback:
                    progress_callback(1, res is not None)
    except Exception:
        pass
    return results


def _encode_single_image(xref, pil_image, orig_raw_size, target_quality, mixed_page_xrefs, is_dct_cmyk=False):
    """Phase B ç¼–ç å‡½æ•°ï¼šä½¿ç”¨ imagecodecs å®ç° GIL-free JPEG2000 ç¼–ç ï¼Œå¯å®‰å…¨å¤šçº¿ç¨‹å¹¶å‘ã€‚
    ä¸ _compress_single_xref é€»è¾‘ä¸€è‡´ï¼Œä½†ä¸ä¾èµ– pikepdf å¯¹è±¡ï¼ˆå›¾ç‰‡å·²åœ¨ Phase A æå–ï¼‰ã€‚
    """
    # CMYK å›¾ç‰‡ï¼šè·³è¿‡ç°åº¦æ£€æµ‹å’ŒäºŒå€¼åŒ–ï¼Œç›´æ¥èµ° JP2K ç¼–ç è·¯å¾„ã€‚
    # detect_strict_color ä¼šå°† CMYK è½¬ RGB æ£€æµ‹ï¼Œä½é¥±å’Œåº¦ CMYK è¢«è¯¯åˆ¤ä¸ºç°åº¦å
    # åœ¨æ··åˆé¡µé¢è§¦å‘äºŒå€¼åŒ–(1-bit DeviceGray)ï¼Œå®Œå…¨ç ´å CMYK è‰²å½©ä¿¡æ¯ã€‚
    if pil_image.mode == "CMYK":
        try:
            img_arr = np.asarray(pil_image)
            # DCTDecode CMYK ä½¿ç”¨åè½¬é€šé“çº¦å®š(0=æ»¡å¢¨, 255=æ— å¢¨)ï¼Œ
            # éœ€åè½¬ä¸ºæ ‡å‡† PDF DeviceCMYK çº¦å®š(0=æ— å¢¨, 255=æ»¡å¢¨)ï¼Œ
            # å¦åˆ™ JPXDecode æ¸²æŸ“å™¨æŒ‰æ ‡å‡†çº¦å®šè§£è¯»ä¼šäº§ç”Ÿè‰²å½©åè½¬ã€‚
            if is_dct_cmyk:
                img_arr = 255 - img_arr
            # CMYK å¿…é¡»å…³é—­ MCTï¼ˆå¤šåˆ†é‡å˜æ¢ä»…é€‚ç”¨äº RGB/YCbCrï¼‰
            if target_quality is not None:
                new_data = imagecodecs.jpeg2k_encode(
                    img_arr, level=target_quality, mct=False, numthreads=JP2K_THREADS
                )
            else:
                new_data = imagecodecs.jpeg2k_encode(
                    img_arr, level=0, mct=False, numthreads=JP2K_THREADS
                )
        except Exception:
            return None
        if len(new_data) >= orig_raw_size:
            return None
        return {
            "xref": xref,
            "data": new_data,
            "width": pil_image.width,
            "height": pil_image.height,
            "mode": "CMYK",
        }

    # 1. ä¸¥æ ¼ç°åº¦æ£€æµ‹
    is_color = detect_strict_color(
        pil_image, grid_size=GRID_SIZE, threshold=COLOR_STD_THRESHOLD
    )

    if not is_color:
        if xref not in mixed_page_xrefs:
            # ç°åº¦å›¾ä¸åœ¨æ··åˆé¡µé¢ -> ç•™ç»™åˆ‡ç‰‡/äºŒå€¼åŒ–é˜¶æ®µ
            return None

        # æ··åˆé¡µé¢ä¸­çš„ç°åº¦å›¾ï¼Œæ£€æŸ¥æ˜¯å¦å¯äºŒå€¼åŒ–
        mode, _ = detect_mono_or_hybrid(pil_image)
        if mode == "MONO" and target_quality is not None:
            # type: ignore
            img_bin = pil_image.convert("L").point(
                lambda x: 0 if x < BINARIZE_THRESHOLD else 255, "1"
            )
            raw_bits = img_bin.tobytes()
            new_data = zlib.compress(raw_bits, level=9)
            if len(new_data) >= orig_raw_size:
                return None
            return {
                "xref": xref,
                "data": new_data,
                "width": img_bin.width,
                "height": img_bin.height,
                "mode": "1",
            }

    # æ­£å¸¸å¤„ç† (Color æˆ– Mixed-Hybrid-Gray)
    if pil_image.mode not in ["RGB", "L"]:
        pil_image = pil_image.convert("RGB")

    try:
        img_arr = np.asarray(pil_image)
        if target_quality is not None:
            new_data = imagecodecs.jpeg2k_encode(img_arr, level=target_quality, numthreads=JP2K_THREADS)
        else:
            new_data = imagecodecs.jpeg2k_encode(img_arr, level=0, numthreads=JP2K_THREADS)
    except Exception:
        return None

    if len(new_data) >= orig_raw_size:
        return None

    return {
        "xref": xref,
        "data": new_data,
        "width": pil_image.width,
        "height": pil_image.height,
        "mode": pil_image.mode,
    }


def run_image_pass_safe(input_path, output_path, quality_db, desc):
    """å®‰å…¨å›¾ç‰‡å‹ç¼© Pass"""

    # 1. é¢„æ‰«æï¼šè¯†åˆ«å“ªäº› XREF å±äº "æ··åˆé¡µé¢" (æœ‰æ–‡å­—/çŸ¢é‡)
    # è¿™äº›é¡µé¢ä¸Šçš„ç°åº¦å›¾å¿…é¡»åœ¨å®‰å…¨å›¾ç‰‡å‹ç¼©é˜¶æ®µå¤„ç†ï¼Œä¸èƒ½ç•™ç»™ç ´åæ€§åˆ‡ç‰‡é˜¶æ®µ
    mixed_page_xrefs = set()
    try:
        doc = fitz.open(input_path)
        for page in doc:
            has_text = len(page.get_text("text").strip()) > 0
            # get_drawings() å¼€é”€è¾ƒå¤§ï¼›æœ‰æ–‡å­—æ—¶å·²å¯åˆ¤å®šä¸ºæ··åˆé¡µï¼Œç›´æ¥çŸ­è·¯ã€‚
            has_drawings = False
            if not has_text:
                has_drawings = len(page.get_drawings()) > 0

            if has_text or has_drawings:
                # è¿™æ˜¯ä¸€ä¸ªæ··åˆé¡µé¢ï¼Œè®°å½•å…¶æ‰€æœ‰å›¾ç‰‡ XREF
                img_list = page.get_images(full=True)
                for img in img_list:
                    mixed_page_xrefs.add(img[0])  # xref is index 0
        doc.close()
    except:
        pass  # å¦‚æœæ‰«æå¤±è´¥ï¼Œmixed_page_xrefs ä¸ºç©ºï¼Œç°åº¦å›¾å°†å…¨éƒ¨è·³è¿‡ç»™åˆ‡ç‰‡é˜¶æ®µ (é£é™©è¾ƒå°)

    # === Phase A: å•çº¿ç¨‹æå– (GIL-bound pikepdf) ===
    # åªæ‰“å¼€PDFä¸€æ¬¡ï¼Œæå–æ‰€æœ‰å›¾ç‰‡ä¸ºPILå¯¹è±¡ + åŸå§‹å¤§å°
    extracted = []  # list of (xref, pil_image, orig_raw_size, is_dct_cmyk)
    trivial_smask_xrefs = set()  # è®°å½•éœ€è¦ç§»é™¤çš„å…¨é€æ˜ SMask å›¾ç‰‡ xref
    try:
        with pikepdf.open(input_path) as pdf:
            for i, obj in enumerate(pdf.objects):
                if isinstance(obj, pikepdf.Stream) and obj.get("/Subtype") == "/Image":
                    raw_size = len(obj.read_raw_bytes())
                    if raw_size < MIN_IMAGE_SIZE:
                        continue
                    # ç¡¬ Maskï¼ˆé SMaskï¼‰ä»ç„¶è·³è¿‡
                    if "/Mask" in obj:
                        continue
                    # SMask æ£€æµ‹ï¼šå…¨ 255 çš„ SMaskï¼ˆå®Œå…¨ä¸é€æ˜ï¼‰å¯å®‰å…¨ç§»é™¤
                    has_smask = "/SMask" in obj
                    if has_smask:
                        try:
                            smask_obj = obj["/SMask"]
                            smask_data = bytes(smask_obj.read_bytes())
                            # æ£€æŸ¥æ˜¯å¦å…¨éƒ¨ä¸º 255ï¼ˆå®Œå…¨ä¸é€æ˜ï¼‰
                            if smask_data == b'\xff' * len(smask_data):
                                trivial_smask_xrefs.add(i)
                                raw_size += len(smask_obj.read_raw_bytes())
                            else:
                                continue  # é trivial SMaskï¼Œè·³è¿‡
                        except Exception:
                            continue
                    try:
                        pdfimage = pikepdf.PdfImage(obj)
                        pil_image = pdfimage.as_pil_image()
                        if pil_image.width * pil_image.height > 100_000_000:
                            continue
                        # æ ‡è®° DCTDecode CMYKï¼šJPEG CMYK ä½¿ç”¨åè½¬é€šé“å€¼çº¦å®š(0=æ»¡å¢¨)
                        # è€Œ FlateDecode/Indexed CMYK ä½¿ç”¨æ ‡å‡†çº¦å®š(0=æ— å¢¨)
                        is_dct_cmyk = (
                            pil_image.mode == "CMYK"
                            and str(obj.get("/Filter")) == "/DCTDecode"
                        )
                        extracted.append((i, pil_image, raw_size, is_dct_cmyk))
                    except Exception:
                        continue
    except Exception:
        return False
    if not extracted:
        return False

    # === Phase B: å¤šçº¿ç¨‹ç¼–ç  (GIL-free numpy/PIL/zlib) ===
    results = []
    opt_count = [0]
    pbar = tqdm(
        total=len(extracted),
        desc=f"      ğŸ¨ {desc} (æ™ºèƒ½æ··åˆ)",
        unit="img",
    )

    def on_encode_done(future):
        """ç¼–ç å®Œæˆå›è°ƒ (GIL-freeæ“ä½œå®Œæˆå)"""
        try:
            res = future.result()
        except Exception:
            res = None
        pbar.update(1)
        if res is not None:
            opt_count[0] += 1
            pbar.set_postfix(opt=opt_count[0])

    with ThreadPoolExecutor(max_workers=JP2K_WORKERS) as executor:
        futures = []
        for xref, pil_image, orig_raw_size, is_dct_cmyk in extracted:
            f = executor.submit(
                _encode_single_image, xref, pil_image, orig_raw_size,
                quality_db, mixed_page_xrefs, is_dct_cmyk
            )
            f.add_done_callback(on_encode_done)
            futures.append(f)
        # ç­‰å¾…æ‰€æœ‰ç¼–ç å®Œæˆ
        for f in futures:
            try:
                res = f.result()
            except Exception:
                res = None
            if res is not None:
                results.append(res)
    pbar.close()

    if results:
        try:
            with pikepdf.open(input_path, allow_overwriting_input=True) as pdf:
                for res in results:
                    obj = pdf.objects[res["xref"]]

                    # ç§»é™¤å·²ç¡®è®¤ä¸º trivial çš„ SMask å¼•ç”¨
                    if res["xref"] in trivial_smask_xrefs:
                        if "/SMask" in obj:
                            del obj["/SMask"]

                    if res.get("mode") == "1":
                        # äºŒå€¼åŒ–å›¾ç‰‡ (FlateDecode)
                        obj.write(res["data"], filter=pikepdf.Name("/FlateDecode"))
                        obj.Width = res["width"]
                        obj.Height = res["height"]
                        obj.ColorSpace = pikepdf.Name("/DeviceGray")
                        obj.BitsPerComponent = 1
                    else:
                        # æ™®é€šå½©è‰²/ç°åº¦å›¾ç‰‡ (JPXDecode)
                        obj.write(res["data"], filter=pikepdf.Name("/JPXDecode"))
                        obj.Width = res["width"]
                        obj.Height = res["height"]
                        if res["mode"] == "CMYK":
                            obj.ColorSpace = pikepdf.Name("/DeviceCMYK")
                        elif res["mode"] == "RGB":
                            obj.ColorSpace = pikepdf.Name("/DeviceRGB")
                        else:
                            obj.ColorSpace = pikepdf.Name("/DeviceGray")
                        obj.BitsPerComponent = 8
                        if "/Filter" in obj:
                            obj.Filter = pikepdf.Name("/JPXDecode")

                    # æ¸…ç†é€šç”¨å±æ€§
                    if "/DecodeParms" in obj:
                        del obj["/DecodeParms"]  # type: ignore
                    if "/Decode" in obj:
                        del obj["/Decode"]  # type: ignore
                    if "/ICCProfile" in obj:
                        del obj["/ICCProfile"]  # type: ignore
                pdf.remove_unreferenced_resources()
                pdf.save(output_path, compress_streams=True)
            return is_valid_pdf(output_path)
        except:
            return False
    return False


# ============================
# Phase 3: Tiling Pass
# ============================
def process_page_chunk_tiling(input_path, start_page, end_page, chunk_id, target_quality, progress_callback=None):
    # ä½¿ç”¨ç¨³å¥çš„ä¸´æ—¶æ–‡ä»¶å (é¿å…åŒ…å« .tmp ç­‰å¹²æ‰°)
    base, _ = os.path.splitext(input_path)
    # å– base çš„ hash æˆ– ç®€å•æ¸…ç†ï¼Œé˜²æ­¢è¿‡é•¿
    # ç”Ÿæˆï¼šcurrent_dir/temp_chunk_{id}.pdf
    chunk_output = os.path.join(
        os.path.dirname(input_path), f"temp_chunk_{chunk_id}.pdf"
    )

    chunk_stats = {
        "mono_tiles": 0,
        "hybrid_tiles": 0,
        "pages_modified": 0,
        "pages_skipped": 0,
    }

    doc = fitz.open(input_path)
    new_doc = fitz.open()
    new_doc.insert_pdf(doc, from_page=start_page, to_page=end_page)
    doc.close()

    modified_count = 0
    for i in range(len(new_doc)):
        page = new_doc[i]
        img_list = page.get_images(full=True)
        if not img_list:
            chunk_stats["pages_skipped"] += 1
            if progress_callback:
                progress_callback(1, False)
            continue

        # ä¸€ï¼‰å‰ç½®æ¡ä»¶ï¼šåªæœ‰ "çº¯å›¾ç‰‡é¡µé¢" (æ— æ–‡å­—ã€æ— çŸ¢é‡) æ‰è¿›å…¥ç ´åæ€§åˆ‡ç‰‡å¤„ç†
        # å¦‚æœåŒ…å«æ–‡å­—æˆ–çŸ¢é‡å›¾ï¼Œç›´æ¥è·³è¿‡ (ä¸è¿›è¡ŒåŸä½å‹ç¼©ï¼Œå› ä¸ºå®‰å…¨é˜¶æ®µå¯èƒ½å·²ç»å¤„ç†è¿‡ï¼Œæˆ–è€…ç”¨æˆ·å¸Œæœ›ä¿ç•™åŸæ ·)
        has_text = len(page.get_text("text").strip()) > 0
        has_drawings = len(page.get_drawings()) > 0
        if has_text or has_drawings:
            chunk_stats["pages_skipped"] += 1
            if progress_callback:
                progress_callback(1, False)
            continue

        page_modified = False
        for img_info in list(img_list):
            xref = img_info[0]

            # å…³é”®ä¿®å¤ï¼šå¸¦ SMask/Mask çš„å›¾å±‚å¸¸æ˜¯é€æ˜å å±‚èµ„æºï¼Œ
            # å¯¹å…¶åšäºŒå€¼åŒ–å¯èƒ½å¯¼è‡´æ•´é¡µè¢«é‡å†™ä¸ºè¿‘å…¨é»‘å›¾ï¼ˆç¬¬34é¡µé»‘å±é—®é¢˜ï¼‰ã€‚
            # fitz get_images(full=True) è¿”å› tupleï¼Œç¬¬2é¡¹é€šå¸¸ä¸º smask xrefï¼ˆ0 è¡¨ç¤ºæ— ï¼‰ã€‚
            try:
                smask_xref = int(img_info[1]) if len(img_info) > 1 else 0
            except:
                smask_xref = 0
            if smask_xref > 0:
                continue

            rects = page.get_image_rects(xref)
            if not rects:
                continue

            # è®¡ç®—å›¾ç‰‡å æ¯”ï¼Œç”¨äº Hybrid æ¨¡å¼ä¸‹çš„å†³ç­–
            is_large = (sum(r.get_area() for r in rects) / page.rect.get_area()) > 0.5

            pix = fitz.Pixmap(new_doc, xref)
            if pix.n >= 4:
                pix = fitz.Pixmap(fitz.csRGB, pix)
            if pix.n == 1:
                pil_img = Image.frombytes('L', [pix.width, pix.height], pix.samples)
                pil_img = pil_img.convert('RGB')
            elif pix.n == 2:
                # ç°åº¦+alpha â†’ å»alphaè½¬RGB
                pix_no_alpha = fitz.Pixmap(fitz.csGRAY, pix)
                pil_img = Image.frombytes('L', [pix_no_alpha.width, pix_no_alpha.height], pix_no_alpha.samples)
                pil_img = pil_img.convert('RGB')
            else:
                pil_img = Image.frombytes('RGB', [pix.width, pix.height], pix.samples)

            # 1. ä¸¥æ ¼ç°åº¦æ£€æµ‹
            is_color = detect_strict_color(
                pil_img, grid_size=GRID_SIZE, threshold=COLOR_STD_THRESHOLD
            )
            if is_color:
                continue  # å½©è‰²å›¾è·³è¿‡

            mode = detect_mono_or_hybrid(pil_img)[0]
            # åŸºäºå½“å‰ xref çš„è¦†ç›–ç‡ï¼Œé¿å…å°é¢ç§¯æ‰¿è½½å±‚è§¦å‘æ•´é¡µé‡å†™
            coverage_ratio = (sum(r.get_area() for r in rects) / page.rect.get_area()) if rects else 0.0

            # 2. ç°åº¦å›¾å¤„ç†ç­–ç•¥
            # æ­¤æ—¶å·²ç¡®å®šé¡µé¢æ— å¹²æ‰°å…ƒç´ ï¼Œå¯ä»¥å®‰å…¨åœ°é‡æ„é¡µé¢
            if mode == "MONO":
                # é˜²æŠ¤1ï¼šä»…å¤„ç†è¦†ç›–ç‡è¾ƒé«˜çš„ MONO å›¾å±‚ï¼Œé¿å…å°é¢ç§¯å›¾å±‚è¯¯è§¦å‘æ•´é¡µ clean_contents
                if coverage_ratio < 0.5:
                    continue

                # é˜²æŠ¤2ï¼šè·³è¿‡â€œçº¯é»‘æ‰¿è½½å±‚â€ï¼ˆå¸¸è§äºå¸¦ alpha/å å±‚çš„èµ„æºé¡µï¼‰ï¼Œé¿å…è¾“å‡ºæ•´é¡µé»‘å±
                gray_arr = np.asarray(pil_img.convert("L"), dtype=np.uint8)
                if int(gray_arr.max()) == 0:
                    continue

                # å…¨å•è‰² -> ç›´æ¥äºŒå€¼åŒ–é‡æ„ (ä¸åŒºåˆ†å¤§å°ï¼Œåªè¦æ˜¯çº¯å›¾é¡µé¢çš„ Mono å‡å¤„ç†)
                # type: ignore
                img_bin = pil_img.convert("L").point(
                    lambda x: 0 if x < BINARIZE_THRESHOLD else 255, "1"
                )
                buf = io.BytesIO()
                img_bin.save(buf, format="PNG", icc_profile=None)

                # é‡æ„é¡µé¢å†…å®¹
                page.clean_contents()
                page.add_redact_annot(rects[0])
                page.apply_redactions(images=PDF_REDACT_IMAGE_REMOVE)
                for rect in rects:
                    page.insert_image(rect, stream=buf.getvalue(), overlay=True)
                page_modified = True
                chunk_stats["mono_tiles"] += 1

            elif mode == "HYBRID":
                # äºŒï¼‰is_large é€»è¾‘ï¼š
                # åœ¨è¿›å…¥åˆ‡åˆ†åˆ†æ”¯åï¼Œå¦‚æœä¸æ˜¯çº¯äºŒå€¼åŒ–æƒ…å†µ (å³ Hybrid)ï¼Œ
                # åªæœ‰ "å¤§å›¾" æ‰è¿›è¡Œåˆ‡ç‰‡å¤„ç†ï¼›å°å›¾ç›´æ¥è·³è¿‡ã€‚
                if is_large:
                    processor = TileProcessor(
                        page,
                        xref,
                        pil_img,
                        page_num=(start_page + i),
                        quality_db=target_quality,
                    )
                    success, stats = processor.process_and_replace()
                    if success:
                        page_modified = True
                        chunk_stats["mono_tiles"] += stats["mono"]
                        chunk_stats["hybrid_tiles"] += stats["hybrid"]
                else:
                    # Hybrid å°å›¾ -> è·³è¿‡ (User requested: skip instead of in-place re-compress)
                    pass

        if page_modified:
            modified_count += 1
            chunk_stats["pages_modified"] += 1
        else:
            chunk_stats["pages_skipped"] += 1
        if progress_callback:
            progress_callback(1, page_modified)

    # é™ä½åƒåœ¾å›æ”¶çº§åˆ«ï¼Œé¿å…åœ¨ Chunk é˜¶æ®µç ´åå¼•ç”¨
    new_doc.save(chunk_output, garbage=0, deflate=True)
    new_doc.close()
    return chunk_output, chunk_stats


def run_tiling_pass(input_path, output_path, target_quality, desc):
    """æ‰§è¡Œç‰©ç†åˆ‡ç‰‡ Pass (é’ˆå¯¹ç°åº¦å›¾)"""
    file_mb = get_file_mb(input_path)
    safe_print(f"      [TILE] {desc} (ç°åº¦åˆ‡ç‰‡/ä¼˜åŒ– {target_quality}dB)...")
    # æŠ‘åˆ¶ MuPDF çš„ C çº§è¾“å‡º
    try:
        fitz.tools.set_stderr_file(os.devnull)  # type: ignore
    except:
        pass

    doc = fitz.open(input_path)
    total_pages = len(doc)
    doc.close()

    chunk_size = (total_pages + JP2K_WORKERS - 1) // JP2K_WORKERS
    futures = []
    modified_count = [0]  # ç”¨liståŒ…è£…ä»¥ä¾¿åœ¨é—­åŒ…ä¸­ä¿®æ”¹

    pbar = tqdm(
        total=total_pages,
        desc=f"      ğŸ§© {desc} (åˆ‡ç‰‡)",
        unit="page",
    )

    def on_page_done(n, was_modified):
        """æ¯å¤„ç†å®Œä¸€é¡µç”±workerçº¿ç¨‹å›è°ƒï¼ˆtqdm.updateçº¿ç¨‹å®‰å…¨ï¼‰"""
        pbar.update(n)
        if was_modified:
            modified_count[0] += 1
            pbar.set_postfix(mod=modified_count[0])

    with ThreadPoolExecutor(max_workers=JP2K_WORKERS) as executor:
        for i in range(JP2K_WORKERS):
            start = i * chunk_size
            end = min((i + 1) * chunk_size - 1, total_pages - 1)
            if start > end:
                break
            futures.append(
                executor.submit(
                    process_page_chunk_tiling, input_path, start, end, i, target_quality,
                    progress_callback=on_page_done
                )
            )

    chunk_files = []
    total_stats = {
        "mono_tiles": 0,
        "hybrid_tiles": 0,
        "pages_modified": 0,
        "pages_skipped": 0,
    }

    for f in futures:
        res, stats = f.result()
        if res:
            chunk_files.append(res)
            for k in total_stats:
                total_stats[k] += stats.get(k, 0)
    pbar.close()

    # åˆå¹¶ (Robust Sort)
    # æ–‡ä»¶åæ ¼å¼: temp_chunk_{id}.pdf
    def get_chunk_id(fname):
        # æ‰¾åˆ°æœ€åä¸€ä¸ª _ å’Œ .pdf ä¹‹é—´çš„æ•°å­—
        return int(fname.split("_")[-1].replace(".pdf", ""))

    chunk_files.sort(key=get_chunk_id)

    if not chunk_files:
        return False

    final_doc = fitz.open()
    for cf in chunk_files:
        c_doc = fitz.open(cf)
        final_doc.insert_pdf(c_doc)
        c_doc.close()

    final_doc.save(output_path, garbage=4, deflate=True)
    final_doc.close()

    for cf in chunk_files:
        os.remove(cf)

    safe_print(
        f"      [STAT] åˆ‡ç‰‡ç»Ÿè®¡: ä¿®æ”¹={total_stats['pages_modified']} | åˆ‡ç‰‡: äºŒå€¼={total_stats['mono_tiles']} æ··åˆ={total_stats['hybrid_tiles']}"
    )
    return is_valid_pdf(output_path)


# ============================
# ä¸»æµç¨‹
# ============================
def process_file(input_path, idx, total, unattended_mode=False):
    file_mb = get_file_mb(input_path)
    initially_under_threshold = file_mb < SIZE_THRESHOLD_MB
    base_name = os.path.basename(input_path)
    safe_print(f"\n[{idx}/{total}] Processing: {base_name} ({file_mb:.2f} MB)")

    current_file = input_path

    # Temp files
    tmp_clean = input_path + ".tmp_clean"
    tmp_gs = input_path + ".tmp_gs"
    tmp_img0 = input_path + ".tmp_img0"
    tmp_gray = input_path + ".tmp_gray"

    # --- Phase 1: Safe Phase ---
    safe_print("      [Phase 1] å®‰å…¨æ¨¡å¼ (æ¸…ç† + GS + æ— æŸ)...")

    if initially_under_threshold:
        safe_print("      [OK] æºæ–‡ä»¶å·²è¾ƒå° (<100MB)ï¼Œä»…è¿è¡Œå®‰å…¨æ¨¡å¼ã€‚")

    if surgical_clean(input_path, tmp_clean):
        current_file = tmp_clean

    # è·å–é¡µæ•°ä»¥æ˜¾ç¤º GS è¿›åº¦æ¡
    total_pages = 0
    try:
        with pikepdf.open(current_file) as pdf:
            total_pages = len(pdf.pages)
    except:
        pass

    # GS å¯èƒ½ä¼šå¯¼è‡´ä¸­æ–‡å­—ä½“ä¹±ç æˆ–ç»“æ„æŸåï¼Œæš‚æ—¶ç¦ç”¨ GS é‡æ„æ­¥éª¤
    # === ä¿®å¤é€»è¾‘ï¼šå°è¯•è¿è¡Œ GSï¼Œä½†å¦‚æœç»“æœæŸåæˆ–æ–‡å­—ä¹±ç ï¼Œè‡ªåŠ¨å›é€€ ===
    gs_success = run_gs_level0(current_file, tmp_gs, total_pages)
    if gs_success:
        prev_mb_before_gs = get_file_mb(current_file)
        # éªŒè¯ GS ç»“æœæ˜¯å¦æŸå (MuPDF check)
        try:
            # ç»“æ„å®Œæ•´æ€§ + æ–‡å­—å®Œæ•´æ€§æ£€æŸ¥ (åˆå¹¶æ‰“å¼€ä¸¤ä¸ªPDFï¼Œå‡å°‘I/Oå¼€é”€)
            check_doc = fitz.open(tmp_gs)
            orig_doc = fitz.open(current_file)
            gs_text = check_doc[0].get_text("text")
            orig_text = orig_doc[0].get_text("text")
            check_doc.close()
            orig_doc.close()

            keep_gs = True
            if len(orig_text.strip()) > 10:
                # ç®€å•è®¡ç®—å­—ç¬¦é‡åˆåº¦
                set_orig = set(orig_text)
                set_gs = set(gs_text)
                common = set_orig.intersection(set_gs)
                # å¦‚æœé‡åˆå­—ç¬¦æ•°å°‘äºåŸå­—ç¬¦ç§ç±»çš„ 50%ï¼Œè®¤ä¸ºä¹±ç 
                if len(common) < len(set_orig) * 0.5:
                    safe_print("      [WARN] GS æ–‡å­—æŸå (ä¹±ç æ£€æµ‹)ï¼Œå›é€€ GS æ­¥éª¤ã€‚")
                    keep_gs = False
            
            if keep_gs:
                # GS é¡µé¢çº§å›é€€ï¼šå…ˆå›é€€â€œç›¸å¯¹ä¸Šä¸€é˜¶æ®µå˜å¤§â€çš„é¡µé¢ï¼Œå†å†³å®šæ˜¯å¦é‡‡ç”¨
                gs_candidate = tmp_gs
                gs_guard_file = tmp_gs + ".tmp_guard_prev"
                gs_guard_ok, gs_worse_cnt = rollback_worse_pages_by_image_payload(
                    current_file, tmp_gs, gs_guard_file
                )
                if gs_guard_ok and is_valid_pdf(gs_guard_file):
                    gs_candidate = gs_guard_file
                    safe_print(f"      [GUARD] GS é¡µçº§å›é€€(è¿­ä»£): å›é€€ {gs_worse_cnt} é¡µ")

                # GS å†…å®¹æµçº§å›é€€ï¼šå®¹å¿è½»å¾®ç¼–ç å·®å¼‚ï¼Œä»…å›é€€â€œæ˜æ˜¾å˜å¤§â€çš„å†…å®¹æµ
                gs_content_guard_file = tmp_gs + ".tmp_guard_content"
                content_guard_ok, rolled_streams, affected_pages = rollback_worse_content_streams(
                    current_file, gs_candidate, gs_content_guard_file, tolerance_bytes=64
                )
                if content_guard_ok and is_valid_pdf(gs_content_guard_file):
                    gs_candidate = gs_content_guard_file
                    safe_print(
                        f"      [GUARD] GS æµçº§å†…å®¹å›é€€: å›é€€ {rolled_streams} æ¡æµ / {affected_pages} é¡µ"
                    )

                # ä¸¥æ ¼ç›¸å¯¹ä¸Šä¸€é˜¶æ®µå›é€€ï¼šå®ˆå«åä»ä¸å˜å°åˆ™ä¸é‡‡ç”¨
                gs_mb = get_file_mb(gs_candidate)
                if gs_mb > 0.01 and gs_mb < prev_mb_before_gs:
                    if current_file != input_path:
                        safe_remove(current_file)
                    current_file = gs_candidate
                    safe_print(f"      [DOWN] GS é‡æ„æ”¶ç›Š: {gs_mb:.2f} MB")
                    if gs_candidate != tmp_gs:
                        safe_remove(tmp_gs)
                    if gs_candidate != gs_guard_file:
                        safe_remove(gs_guard_file)
                    if gs_candidate != gs_content_guard_file:
                        safe_remove(gs_content_guard_file)
                else:
                    safe_print("      [SKIP] GS æ— æ”¶ç›Šï¼Œå›é€€ä¸Šä¸€é˜¶æ®µã€‚")
                    safe_remove(tmp_gs)
                    safe_remove(gs_guard_file)
                    safe_remove(gs_content_guard_file)
            else:
                safe_remove(tmp_gs)

        except:
            safe_print("      [WARN] GS ç»“æœä¼¼ä¹æŸå (MuPDF æ£€æŸ¥å¤±è´¥)ï¼Œå›é€€ GS æ­¥éª¤ã€‚")
            safe_remove(tmp_gs)
            # ä¿æŒ current_file ä¸å˜ (å³è·³è¿‡ GS)
    else:
         safe_remove(tmp_gs)

    if run_image_pass_safe(current_file, tmp_img0, None, "æ— æŸ"):
        if current_file != input_path:
            safe_remove(current_file)
        current_file = tmp_img0

    safe_mb = get_file_mb(current_file)
    safe_print(f"      [DOWN] å®‰å…¨æ¨¡å¼ç»“æœ: {safe_mb:.2f} MB")

    # è§„åˆ™ï¼šå¦‚æœæºæ–‡ä»¶ä¸€å¼€å§‹å°±å°äºé˜ˆå€¼ï¼Œå®‰å…¨é˜¶æ®µç»“æŸåå¿…é¡»ç›´æ¥è¿”å›ï¼Œä¸è¿›å…¥åç»­æœ‰æŸæµç¨‹ã€‚
    if initially_under_threshold:
        safe_print("      [OK] åŸå§‹æ–‡ä»¶å·²ä½äºé˜ˆå€¼ï¼Œå®‰å…¨é˜¶æ®µåç›´æ¥ç»“æŸã€‚")
        if safe_mb < file_mb:
            try:
                shutil.move(current_file, input_path)
                safe_print(
                    f"      [SAVE] è¦†ç›–åŸæ–‡ä»¶: {os.path.basename(input_path)} ({safe_mb:.2f} MB)"
                )
            except:
                pass
        else:
            safe_print("      [SKIP] å®‰å…¨æ¨¡å¼æ— æ”¶ç›Šã€‚")
            if current_file != input_path:
                safe_remove(current_file)

        # Cleanup temps
        safe_remove(tmp_clean)
        safe_remove(tmp_gs)
        safe_remove(tmp_img0)
        safe_remove(tmp_gray)
        return

    if safe_mb < SIZE_THRESHOLD_MB:
        safe_print("      [OK] å®‰å…¨ä¼˜åŒ–å·²è¾¾ç›®æ ‡å¤§å°ï¼Œè·³è¿‡æœ‰æŸé˜¶æ®µã€‚")
        # Finalize and return early (using current_file as result)
        if safe_mb < file_mb:
            try:
                shutil.move(current_file, input_path)
                safe_print(
                    f"      [SAVE] è¦†ç›–åŸæ–‡ä»¶: {os.path.basename(input_path)} ({safe_mb:.2f} MB)"
                )
            except:
                pass
        else:
            safe_print("      [SKIP] å®‰å…¨æ¨¡å¼æ— æ”¶ç›Šã€‚")
            if current_file != input_path:
                safe_remove(current_file)

        # Cleanup temps
        safe_remove(tmp_clean)
        safe_remove(tmp_gs)
        safe_remove(tmp_img0)
        safe_remove(tmp_gray)
        return

    # --- Phase 2: Interleaved Optimization (User Requested) ---
    # å…ˆæ£€æµ‹å•è‰²è£…é¥°é¡µé¢
    safe_print("      [SCAN] åˆ†æé¢œè‰²ç‰¹å¾...")
    has_mono_pages, pages_to_convert, mono_stats = detect_mono_decorative_pages(current_file)
    
    if has_mono_pages and pages_to_convert:
        total_pages_mono = mono_stats.get('total_pages', 0)
        convert_count = len(pages_to_convert)
        dominant_hue = mono_stats.get('dominant_hue', 'unknown')
        
        # ä¼°ç®—æ—¶é—´ (æ ¹æ®GPU/CPUåŒºåˆ†)
        ml_available = check_ml_pipeline_available()
        gpu_name = 'CPU'
        try:
            from ml_enhance import get_gpu_providers
            _, gpu_name = get_gpu_providers()
        except Exception:
            pass
        # GPU (DirectML/CUDA) ~0.5-1åˆ†é’Ÿ/é¡µ, CPU ~3-5åˆ†é’Ÿ/é¡µ
        time_per_page_ml = 1 if gpu_name != 'CPU' else 5
        time_per_page_traditional = 0.1  # åˆ†é’Ÿ
        estimated_ml_time = convert_count * time_per_page_ml
        estimated_traditional_time = convert_count * time_per_page_traditional
        
        safe_print(f"")
        safe_print(f"      â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
        safe_print(f"      â•‘  æ£€æµ‹åˆ° {convert_count}/{total_pages_mono} é¡µå•è‰²è£…é¥°é¡µé¢ (ä¸»è‰²è°ƒ: {dominant_hue})")
        safe_print(f"      â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£")
        safe_print(f"      â•‘  è¿™äº›é¡µé¢å«æœ‰è£…é¥°è‰²ï¼ˆå¦‚è“è‰²è¾¹æ¡†ï¼‰ï¼Œä½†ä¸»ä½“ä¸ºç°åº¦å†…å®¹ã€‚")
        safe_print(f"      â•‘  æ …æ ¼åŒ–åå¯å¯ç”¨äºŒå€¼åŒ–å‹ç¼©ï¼Œæ˜¾è‘—å‡å°æ–‡ä»¶ä½“ç§¯ã€‚")
        safe_print(f"      â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£")
        safe_print(f"      â•‘  [æé†’] è¯·å…ˆç¡®è®¤è¿™äº›é¡µé¢æ˜¯å¦ç¡®å®é€‚åˆç°åº¦åŒ–ï¼š")
        safe_print(f"      â•‘         - è‹¥æ–‡ä»¶å·²æ˜¯æ¸…æ™°ç°åº¦ç¨¿ï¼Œé€šå¸¸ä¸å¿…å†æ¬¡ç°åº¦åŒ–")
        safe_print(f"      â•‘         - è‹¥éœ€è¦ä¿ç•™ä¸“è‰²/å½©è‰²ä¿¡æ¯ï¼Œä¸å»ºè®®å¼ºåˆ¶ç°åº¦åŒ–")
        safe_print(f"      â•‘         - è‹¥æ–‡ä»¶å«æœ‰çŸ¢é‡å†…å®¹ï¼ˆå¦‚æ¿å†™ç¬”è®°ã€çŸ¢é‡ç»˜å›¾ï¼‰ï¼Œä¸å»ºè®®ç°åº¦åŒ–")
        safe_print(f"      â•‘           ç°åº¦åŒ–ä¼šå°†çŸ¢é‡ç”»é¢æ …æ ¼åŒ–ä¸ºä½å›¾ï¼Œç ´åçŸ¢é‡ä¿¡æ¯ä¸”æ— æ³•è¿˜åŸ")
        safe_print(f"      â•‘  [å»ºè®®] è‹¥æ–‡æ¡£ä¸ºåŒè‰²å¥—å°ã€ä»¥ç™½åº•é»‘å­—ä¸ºä¸»ï¼Œä¸”æ–‡ä»¶ä½“ç§¯è¾ƒå¤§ï¼ˆå¦‚ >100MBï¼‰çš„ä¹¦ç±æ‰«æä»¶ï¼Œ")
        safe_print(f"      â•‘         é€šå¸¸æ¨èç°åº¦åŒ–ä»¥æå‡å‹ç¼©æ”¶ç›Šã€‚")
        safe_print(f"      â•‘         å¦‚ä¸ç¡®å®šï¼Œå»ºè®®å…ˆé€‰ [4] ä¸æ‰§è¡Œç°åº¦åŒ–ï¼Œå¹¶äººå·¥æŠ½æ£€åå†å†³å®šã€‚")
        safe_print(f"      â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£")
        safe_print(f"      â•‘  å¢å¼ºæ–¹å¼é€‰æ‹©:")
        safe_print(f"      â•‘")
        safe_print(f"      â•‘  [1] MLå¢å¼º (ESRGAN + NAF-DPM + DoxaPy)")
        safe_print(f"      â•‘      - æ•ˆæœæœ€ä½³ï¼šæ–‡å­—é”åˆ©ã€èƒŒæ™¯çº¯å‡€ã€æ¶ˆé™¤æ‰«æå™ªç‚¹")
        # æ˜¾ç¤ºGPUåŠ é€ŸçŠ¶æ€
        if gpu_name != 'CPU':
            safe_print(f"      â•‘      - GPUåŠ é€Ÿ: {gpu_name} (å¤§å¹…æå‡é€Ÿåº¦)")
        else:
            safe_print(f"      â•‘      - ä½¿ç”¨CPUæ¨ç† (è¾ƒæ…¢)")
        safe_print(f"      â•‘      - é¢„è®¡è€—æ—¶: ~{estimated_ml_time} åˆ†é’Ÿ ({convert_count}é¡µ Ã— ~{time_per_page_ml}åˆ†é’Ÿ/é¡µ)")
        safe_print(f"      â•‘")
        safe_print(f"      â•‘  [2] ä¼ ç»Ÿå¢å¼º (é™å™ª + CLAHE + é”åŒ–)")
        safe_print(f"      â•‘      - æ•ˆæœä¸€èˆ¬ï¼šåŸºç¡€å»å™ªå’Œå¯¹æ¯”åº¦è°ƒæ•´")
        safe_print(f"      â•‘      - é¢„è®¡è€—æ—¶: ~{estimated_traditional_time:.1f} åˆ†é’Ÿ")
        safe_print(f"      â•‘")
        safe_print(f"      â•‘  [3] æ‰§è¡Œç°åº¦åŒ–ä½†è·³è¿‡å¢å¼º")
        safe_print(f"      â•‘      - ä»…æ …æ ¼åŒ–ä¸ºç°åº¦ï¼Œä¸åšä»»ä½•å¢å¼ºå¤„ç†")
        safe_print(f"      â•‘")
        safe_print(f"      â•‘  [4] ä¸æ‰§è¡Œç°åº¦åŒ– (æ¨èç”¨äºä¸ç¡®å®šåœºæ™¯)")
        safe_print(f"      â•‘      - ä¿æŒåŸé¡µé¢è‰²å½©ç»“æ„ï¼Œä¸è¿›è¡Œæ …æ ¼åŒ–")
        safe_print(f"      â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
        
        # ç”¨æˆ·äº¤äº’ / æ— äººå€¼å®ˆé»˜è®¤ç­–ç•¥
        if unattended_mode:
            safe_print("      [AUTO] æ— äººå€¼å®ˆæ¨¡å¼ï¼šé»˜è®¤ä¸æ‰§è¡Œç°åº¦åŒ–ï¼Œè¯·åç»­äººå·¥æŠ½æ£€æ–‡ä»¶å®é™…çŠ¶æ€ã€‚")
        else:
            try:
                user_input = input("      --> è¯·é€‰æ‹© [1/2/3/4]ï¼Œç›´æ¥å›è½¦é»˜è®¤é€‰æ‹© 2 (ä¼ ç»Ÿå¢å¼º): ").strip()

                if user_input == '1':
                    safe_print(f"      [ML] ä½¿ç”¨MLå¢å¼ºç®¡çº¿å¤„ç† {convert_count} é¡µ...")
                    if convert_pages_to_grayscale(current_file, tmp_gray, pages_to_convert, enhance=True, use_ml=True):
                        if current_file != input_path:
                            safe_remove(current_file)
                        current_file = tmp_gray
                        safe_print(f"      [OK] MLå¢å¼ºå®Œæˆ: {get_file_mb(current_file):.2f} MB")
                    else:
                        safe_print("      [WARN] MLå¢å¼ºè¿‡ç¨‹å‡ºé”™ã€‚")
                elif user_input == '3':
                    safe_print(f"      [SKIP] è·³è¿‡å¢å¼ºï¼Œä»…æ …æ ¼åŒ–...")
                    if convert_pages_to_grayscale(current_file, tmp_gray, pages_to_convert, enhance=False):
                        if current_file != input_path:
                            safe_remove(current_file)
                        current_file = tmp_gray
                        safe_print(f"      [OK] æ …æ ¼åŒ–å®Œæˆ: {get_file_mb(current_file):.2f} MB")
                elif user_input == '4':
                    safe_print("      [SKIP] æŒ‰ç”¨æˆ·é€‰æ‹©è·³è¿‡ç°åº¦åŒ–ï¼Œè¯·äººå·¥æ ¸éªŒé¡µé¢çœŸå®è‰²å½©å±æ€§ã€‚")
                else:  # é»˜è®¤é€‰æ‹© 2
                    safe_print(f"      [ä¼ ç»Ÿ] ä½¿ç”¨ä¼ ç»Ÿå¢å¼ºå¤„ç† {convert_count} é¡µ...")
                    if convert_pages_to_grayscale(current_file, tmp_gray, pages_to_convert, enhance=True, use_ml=False):
                        if current_file != input_path:
                            safe_remove(current_file)
                        current_file = tmp_gray
                        safe_print(f"      [OK] ä¼ ç»Ÿå¢å¼ºå®Œæˆ: {get_file_mb(current_file):.2f} MB")
                    else:
                        safe_print("      [WARN] ä¼ ç»Ÿå¢å¼ºè¿‡ç¨‹å‡ºé”™ã€‚")
            except EOFError:
                # éäº¤äº’æ¨¡å¼ï¼Œé»˜è®¤ä¸æ‰§è¡Œç°åº¦åŒ–
                safe_print("      [AUTO] éäº¤äº’æ¨¡å¼ï¼šé»˜è®¤ä¸æ‰§è¡Œç°åº¦åŒ–ï¼Œè¯·äººå·¥æ ¸éªŒé¡µé¢çœŸå®è‰²å½©å±æ€§ã€‚")
    else:
        safe_print("      [OK] æœªæ£€æµ‹åˆ°æ˜¾è‘—çš„å•è‰²è£…é¥°é¡µé¢ã€‚")

    # L1 -> I 50 -> T 50 -> L2 -> I 45 -> T 45 ...
    safe_print(
        "      [Phase 2] äº¤é”™å‹ç¼© (çŸ¢é‡ + å›¾ç‰‡ + ç°åº¦åˆ‡ç‰‡)..."
    )

    stages = [
        ("V", (4, False), "çŸ¢é‡ L1", ".tmp_v1"),
        ("I", (50,), "å›¾ç‰‡ 50dB", ".tmp_i50"),
        ("T", (50,), "åˆ‡ç‰‡ 50dB", ".tmp_t50"),
        ("V", (3, False), "çŸ¢é‡ L2", ".tmp_v2"),
        ("I", (45,), "å›¾ç‰‡ 45dB", ".tmp_i45"),
        ("T", (45,), "åˆ‡ç‰‡ 45dB", ".tmp_t45"),
        ("V", (2, False), "çŸ¢é‡ L3", ".tmp_v3"),
        ("I", (40,), "å›¾ç‰‡ 40dB", ".tmp_i40"),
        ("T", (40,), "åˆ‡ç‰‡ 40dB", ".tmp_t40"),
        ("V", (2, True), "çŸ¢é‡ L4", ".tmp_v4"),
        ("I", (35,), "å›¾ç‰‡ 35dB", ".tmp_i35"),
        ("T", (35,), "åˆ‡ç‰‡ 35dB", ".tmp_t35"),
    ]

    lossy_working_file = current_file

    for step_idx, (stype, args, desc, suffix) in enumerate(stages, 1):
        if get_file_mb(lossy_working_file) < SIZE_THRESHOLD_MB:
            break

        safe_print(f"      [STEP] æ­¥éª¤ {step_idx}: {desc} ...")
        next_file = input_path + suffix
        success = False

        if stype == "V":
            success = run_regex_pass(
                lossy_working_file, next_file, args[0], args[1], desc
            )
        elif stype == "I":
            success = run_image_pass_safe(lossy_working_file, next_file, args[0], desc)
        elif stype == "T":
            # Tiling pass
            success = run_tiling_pass(lossy_working_file, next_file, args[0], desc)

        if success:
            candidate_file = next_file

            # é¡µçº§å›é€€å®ˆå«ï¼šç›¸å¯¹â€œä¸Šä¸€é˜¶æ®µâ€å›é€€ï¼ˆä¸¥æ ¼è¿­ä»£ï¼‰
            if stype in ["I", "T"]:
                guard_file_prev = next_file + ".tmp_guard_prev"
                guard_ok_prev, worse_cnt_prev = rollback_worse_pages_by_image_payload(
                    lossy_working_file, next_file, guard_file_prev
                )
                if guard_ok_prev and is_valid_pdf(guard_file_prev):
                    candidate_file = guard_file_prev
                    safe_print(f"      [GUARD] ä¸Šä¸€é˜¶æ®µå›é€€(è¿­ä»£): å›é€€ {worse_cnt_prev} é¡µ")

            new_mb = get_file_mb(candidate_file)
            if new_mb > 0.01 and new_mb < get_file_mb(lossy_working_file):
                safe_print(f"      [DOWN] æˆåŠŸ: {new_mb:.2f} MB")
                if lossy_working_file != input_path:
                    safe_remove(lossy_working_file)
                lossy_working_file = candidate_file
                # next_file ä¸æ˜¯æœ€ç»ˆé‡‡ç”¨æ–‡ä»¶æ—¶æ¸…ç†ä¹‹
                if candidate_file != next_file:
                    safe_remove(next_file)
            else:
                safe_print("      [SKIP] æ— æ”¶ç›Š")
                safe_remove(next_file)
                # guard æ–‡ä»¶è‹¥å­˜åœ¨ä¹Ÿæ¸…ç†
                safe_remove(next_file + ".tmp_guard_prev")
        else:
            safe_print("      [SKIP] å·²è·³è¿‡")

    # Final Save
    final_mb = get_file_mb(lossy_working_file)
    final_report_name = base_name
    final_report_mb = file_mb
    if final_mb < file_mb:
        root, ext = os.path.splitext(input_path)
        optimized_path = f"{root}_opted{ext}"
        try:
            shutil.move(lossy_working_file, optimized_path)
            safe_print(
                f"      [DONE] å®Œæˆ: {os.path.basename(optimized_path)} ({final_mb:.2f} MB)"
            )
            # åªæœ‰æœ‰æŸå‹ç¼©æ‰æ·»åŠ åˆ°æŠ¥å‘Šåˆ—è¡¨
            lossy_report_list.append((base_name, os.path.basename(optimized_path)))
            final_report_name = os.path.basename(optimized_path)
            final_report_mb = final_mb
        except:
            pass
    else:
        safe_print("      [SKIP] æœªä¼˜åŒ–")
        if lossy_working_file != input_path:
            safe_remove(lossy_working_file)
        final_report_name = base_name
        final_report_mb = get_file_mb(input_path)

    if final_report_mb > SIZE_THRESHOLD_MB:
        large_file_report_list.append((base_name, final_report_name, final_report_mb))

    # Cleanup
    for s in stages:
        safe_remove(input_path + s[3])
    safe_remove(tmp_clean)
    safe_remove(tmp_gs)
    safe_remove(tmp_gs + ".tmp_guard_prev")
    safe_remove(tmp_gs + ".tmp_guard_content")
    safe_remove(tmp_img0)
    safe_remove(tmp_gray)


def main():
    if sys.platform.startswith("win"):
        try:
            # type: ignore
            sys.stdout.reconfigure(encoding="utf-8")
        except:
            pass
    multiprocessing.freeze_support()
    safe_print("=== Fireworks Hybrid Engine (Interleaved Compression) ===")
    
    # GPU detection for ML acceleration
    try:
        from ml_enhance import get_gpu_providers
        providers, provider_name = get_gpu_providers()
        if provider_name == 'CPU':
            safe_print(f"  [GPU] æœªæ£€æµ‹åˆ°GPUåŠ é€Ÿï¼Œå°†ä½¿ç”¨CPUæ¨ç†")
        else:
            safe_print(f"  [GPU] MLåŠ é€Ÿ: {provider_name} ({providers[0]})")
    except Exception:
        safe_print("  [GPU] MLæ¨¡å—æœªåŠ è½½ï¼ŒGPUæ£€æµ‹è·³è¿‡")

    pdf_files = [
        os.path.join(r, f)
        for r, d, fs in os.walk(".")
        for f in fs
        if f.lower().endswith(".pdf") and "_opted" not in f and ".tmp" not in f
    ]

    global lossy_report_list, large_file_report_list
    lossy_report_list = []
    large_file_report_list = []

    unattended_mode = False
    if len(pdf_files) > 5:
        safe_print("\n[INFO] æ£€æµ‹åˆ°å¾…å¤„ç†æ–‡ä»¶è¶…è¿‡ 5 ä¸ªï¼Œå¯å¼€å¯æ— äººå€¼å®ˆæ¨¡å¼ã€‚")
        safe_print("       æ— äººå€¼å®ˆæ¨¡å¼ä¸‹å°†ä¸å†è¯¢é—®ç”¨æˆ·æ„è§ï¼Œä¸”æ£€æµ‹åˆ°å•è‰²è£…é¥°æ¨¡å¼æ—¶é»˜è®¤ä¸è¿›è¡Œç°åº¦åŒ–ã€‚")
        try:
            choice = input("       --> æ˜¯å¦å¼€å¯æ— äººå€¼å®ˆæ¨¡å¼? [y/N]: ").strip().lower()
            unattended_mode = choice in ("y", "yes")
        except EOFError:
            unattended_mode = True
            safe_print("       [AUTO] éäº¤äº’ç¯å¢ƒï¼šè‡ªåŠ¨å¼€å¯æ— äººå€¼å®ˆæ¨¡å¼ã€‚")

    if unattended_mode:
        safe_print("[MODE] å½“å‰è¿è¡Œæ¨¡å¼ï¼šæ— äººå€¼å®ˆ")
    else:
        safe_print("[MODE] å½“å‰è¿è¡Œæ¨¡å¼ï¼šäº¤äº’")

    for idx, f in enumerate(pdf_files, 1):
        process_file(f, idx, len(pdf_files), unattended_mode=unattended_mode)

    if lossy_report_list:
        print("\n" + "=" * 50)
        safe_print("[WARN] ä»¥ä¸‹æ–‡ä»¶è§¦å‘äº†æœ‰æŸå‹ç¼© (é˜¶æ®µ 2/3)ï¼Œè¯·åŠ¡å¿…æ£€æŸ¥å†…å®¹å®Œæ•´æ€§ï¼š")
        for orig, opt in lossy_report_list:
            print(f" - {orig} -> {opt}")
        print("=" * 50 + "\n")

    if large_file_report_list:
        print("\n" + "=" * 50)
        safe_print(f"[WARN] ä»¥ä¸‹æ–‡ä»¶åœ¨å®Œæ•´å‹ç¼©æµç¨‹åä»å¤§äº {SIZE_THRESHOLD_MB}MBï¼Œè¯·é‡ç‚¹å¤æ ¸ï¼š")
        for orig, final_name, final_mb in large_file_report_list:
            print(f" - {orig} -> {final_name} ({final_mb:.2f} MB)")
        print("=" * 50 + "\n")


if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        safe_print(f"\n[ERROR] è¿è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
    finally:
        input("\næŒ‰å›è½¦é”®é€€å‡º...")
